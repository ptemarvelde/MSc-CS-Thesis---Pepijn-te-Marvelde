% LTeX: enabled=false
@inproceedings{2016-hamlet-sigmod,
  author    = {Arun Kumar and
               Jeffrey F. Naughton and
               Jignesh M. Patel and
               Xiaojin Zhu},
  editor    = {Fatma {\"{O}}zcan and
               Georgia Koutrika and
               Sam Madden},
  title     = {To Join or Not to Join?: Thinking Twice about Joins before Feature
               Selection},
  booktitle = {Proceedings of the 2016 International Conference on Management of
               Data, {SIGMOD} Conference 2016, San Francisco, CA, USA, June 26 -
               July 01, 2016},
  pages     = {19--34},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2882903.2882952},
  doi       = {10.1145/2882903.2882952},
  timestamp = {Wed, 14 Nov 2018 10:56:20 +0100},
  biburl    = {https://dblp.org/rec/conf/sigmod/KumarNPZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{acdc_khamis_2018,
  address    = {Houston TX USA},
  title      = {{AC}/{DC}: {In}-{Database} {Learning} {Thunderstruck}},
  shorttitle = {{AC}/{DC}},
  doi        = {10.1145/3209889.3209896},
  language   = {en},
  booktitle  = {Proceedings of the {Second} {Workshop} on {Data} {Management} for {End}-{To}-{End} {Machine} {Learning}},
  publisher  = {ACM},
  author     = {Khamis, Mahmoud Abo and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
  month      = jun,
  year       = {2018},
  keywords   = {notion},
  pages      = {1--10}
}

@inproceedings{amalur,
  author    = {Hai, Rihan and Koutras, Christos and Ionescu, Andra and Li, Ziyu and Sun, Wenbo and van Schijndel, Jessie and Kang, Yan and Katsifodimos, Asterios},
  booktitle = {2023 IEEE 39th International Conference on Data Engineering (ICDE)},
  title     = {Amalur: Data Integration Meets Machine Learning},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {3729-3739},
  keywords  = {Training;Data privacy;Federated learning;Computational modeling;Data integration;Training data;Manuals},
  doi       = {10.1109/ICDE55515.2023.00301}
}

@inproceedings{data-management-in-ML-kumar-2017,
  author    = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
  title     = {Data Management in Machine Learning: Challenges, Techniques, and Systems},
  year      = {2017},
  isbn      = {9781450341974},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3035918.3054775},
  doi       = {10.1145/3035918.3054775},
  abstract  = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
  booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
  pages     = {1717–1722},
  numpages  = {6},
  keywords  = {machine learning, data management},
  location  = {Chicago, Illinois, USA},
  series    = {SIGMOD '17}
}

@article{declaritve-data-analytics-survey,
  author   = {Makrynioti, Nantia and Vassalos, Vasilis},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {Declarative Data Analytics: A Survey},
  year     = {2021},
  volume   = {33},
  number   = {6},
  pages    = {2392-2411},
  keywords = {Task analysis;Data analysis;Programming;Optimization;Mathematical model;Analytical models;Prediction algorithms;Declarative programming;data science;machine learning;large-scale analytics},
  doi      = {10.1109/TKDE.2019.2958084}
}

@inproceedings{f_gmm_DBLP:conf/icde/ChengKZ021,
  author    = {Zhaoyue Cheng and
               Nick Koudas and
               Zhe Zhang and
               Xiaohui Yu},
  title     = {Efficient Construction of Nonlinear Models over Normalized Data},
  booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
               Chania, Greece, April 19-22, 2021},
  pages     = {1140--1151},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICDE51399.2021.00103},
  doi       = {10.1109/ICDE51399.2021.00103},
  timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
  biburl    = {https://dblp.org/rec/conf/icde/ChengKZ021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{f_schleich,
  author    = {Schleich, Maximilian and Olteanu, Dan and Ciucanu, Radu},
  title     = {Learning Linear Regression Models over Factorized Joins},
  year      = {2016},
  isbn      = {9781450335317},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2882903.2882939},
  doi       = {10.1145/2882903.2882939},
  abstract  = {We investigate the problem of building least squares regression models over training datasets defined by arbitrary join queries on database tables. Our key observation is that joins entail a high degree of redundancy in both computation and data representation, which is not required for the end-to-end solution to learning over joins. We propose a new paradigm for computing batch gradient descent that exploits the factorized computation and representation of the training datasets, a rewriting of the regression objective function that decouples the computation of cofactors of model parameters from their convergence, and the commutativity of cofactor computation with relational union and projection. We introduce three flavors of this approach: F/FDB computes the cofactors in one pass over the materialized factorized join; Favoids this materialization and intermixes cofactor and join computation; F/SQL expresses this mixture as one SQL query. Our approach has the complexity of join factorization, which can be exponentially lower than of standard joins. Experiments with commercial, public, and synthetic datasets show that it outperforms MADlib, Python StatsModels, and R, by up to three orders of magnitude.},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  pages     = {3–18},
  numpages  = {16},
  keywords  = {factorized databases, linear regression, join processing},
  location  = {San Francisco, California, USA},
  series    = {SIGMOD '16}
}

@misc{halide_examples,
  author       = {},
  title        = {What is Halide? Why should I use it? — Learning Halide by Examples documentation},
  howpublished = {\url{https://people.csail.mit.edu/tzumao/tmp/learning_halide/what_is_halide.html}},
  month        = {},
  year         = {},
  note         = {(Accessed on 06/15/2023)}
}

@article{halide_cost_model,
  author     = {Adams, Andrew and Ma, Karima and Anderson, Luke and Baghdadi, Riyadh and Li, Tzu-Mao and Gharbi, Micha\"{e}l and Steiner, Benoit and Johnson, Steven and Fatahalian, Kayvon and Durand, Fr\'{e}do and Ragan-Kelley, Jonathan},
  title      = {Learning to Optimize Halide with Tree Search and Random Programs},
  year       = {2019},
  issue_date = {August 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {38},
  number     = {4},
  issn       = {0730-0301},
  url        = {https://doi.org/10.1145/3306346.3322967},
  doi        = {10.1145/3306346.3322967},
  abstract   = {We present a new algorithm to automatically schedule Halide programs for high-performance image processing and deep learning. We significantly improve upon the performance of previous methods, which considered a limited subset of schedules. We define a parameterization of possible schedules much larger than prior methods and use a variant of beam search to search over it. The search optimizes runtime predicted by a cost model based on a combination of new derived features and machine learning. We train the cost model by generating and featurizing hundreds of thousands of random programs and schedules. We show that this approach operates effectively with or without autotuning. It produces schedules which are on average almost twice as fast as the existing Halide autoscheduler without autotuning, or more than twice as fast with, and is the first automatic scheduling algorithm to significantly outperform human experts on average.},
  journal    = {ACM Trans. Graph.},
  month      = {7},
  articleno  = {121},
  numpages   = {12},
  keywords   = {optimizing compilers, halide}
}

@inproceedings{lmfao_schleich_layered_2019,
  author    = {Schleich, Maximilian and Olteanu, Dan and Abo Khamis, Mahmoud and Ngo, Hung Q. and Nguyen, XuanLong},
  title     = {A Layered Aggregate Engine for Analytics Workloads},
  year      = {2019},
  isbn      = {9781450356435},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3299869.3324961},
  doi       = {10.1145/3299869.3324961},
  booktitle = {Proceedings of the 2019 International Conference on Management of Data},
  pages     = {1642–1659},
  numpages  = {18},
  keywords  = {mutliple aggregate optimization, structure-aware analytics},
  location  = {Amsterdam, Netherlands},
  series    = {SIGMOD '19}
}

@article{morpheus,
  title   = {Towards Linear Algebra over Normalized Data},
  author  = {Chen, L. and Kumar, A. and Naughton, J. and Patel, J. M.},
  journal = {PVLDB},
  volume  = {10},
  number  = {11},
  year    = {2017}
}

@inproceedings{MorpheusFI,
  title      = {Enabling and {{Optimizing Non-linear Feature Interactions}} in {{Factorized Linear Algebra}}},
  booktitle  = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author     = {Li, Side and Chen, Lingjiao and Kumar, Arun},
  date       = {2019-06-25},
  pages      = {1571--1588},
  publisher  = {{ACM}},
  location   = {{Amsterdam Netherlands}},
  doi        = {10.1145/3299869.3319878},
  url        = {https://dl.acm.org/doi/10.1145/3299869.3319878},
  abstract   = {Accelerating machine learning (ML) over relational data is a key focus of the database community. While many real-world datasets are multi-table, most ML tools expect single-table inputs, forcing users to materialize joins before ML, leading to data redundancy and runtime waste. Recent works on “factorized ML” address such issues by pushing ML through joins. However, they have hitherto been restricted to ML models linear in the feature space, rendering them less effective when users construct non-linear feature interactions such as pairwise products to boost ML accuracy. In this work, we take a first step towards closing this gap by introducing a new abstraction to enable pairwise feature interactions in multi-table data and present an extensive framework of algebraic rewrite rules for factorized LA operators over feature interactions. Our rewrite rules carefully exploit the interplay of the redundancy caused by both joins and interactions. We prototype our framework in Python to build a tool we call MorpheusFI. An extensive empirical evaluation with both synthetic and real datasets shows that MorpheusFI yields up to 5x speedups over materialized execution for a popular second-order gradient method and even an order of magnitude speedups over a popular stochastic gradient method.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  langid     = {english},
  keywords   = {notion},
  file       = {/Users/jessie/Zotero/storage/GYJXJ3WX/Li et al. - 2019 - Enabling and Optimizing Non-linear Feature Interac.pdf}
}

@techreport{MorpheusPy,
  title       = {MorpheusPy: Factorized Machine Learning with NumPy},
  author      = {Li, Side and Kumar, Arun},
  institution = {Technical report, 2018. Available at https://adalabucsd.github.io/papers/TR_2018_MorpheusPy.pdf}
}

@misc{nsight_compute,
  author       = {},
  title        = {Nsight Compute :: Nsight Compute Documentation},
  howpublished = {\url{https://docs.nvidia.com/nsight-compute/NsightCompute/index.html}},
  month        = {},
  year         = {},
  note         = {(Accessed on 06/20/2023)}
}

@inproceedings{orion_learning_gen_lin_models,
  author    = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M.},
  title     = {Learning Generalized Linear Models Over Normalized Data},
  year      = {2015},
  isbn      = {9781450327589},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2723372.2723713},
  doi       = {10.1145/2723372.2723713},
  abstract  = {Enterprise data analytics is a booming area in the data management industry. Many companies are racing to develop toolkits that closely integrate statistical and machine learning techniques with data management systems. Almost all such toolkits assume that the input to a learning algorithm is a single table. However, most relational datasets are not stored as single tables due to normalization. Thus, analysts often perform key-foreign key joins before learning on the join output. This strategy of learning after joins introduces redundancy avoided by normalization, which could lead to poorer end-to-end performance and maintenance overheads due to data duplication. In this work, we take a step towards enabling and optimizing learning over joins for a common class of machine learning techniques called generalized linear models that are solved using gradient descent algorithms in an RDBMS setting. We present alternative approaches to learn over a join that are easy to implement over existing RDBMSs. We introduce a new approach named factorized learning that pushes ML computations through joins and avoids redundancy in both I/O and computations. We study the tradeoff space for all our approaches both analytically and empirically. Our results show that factorized learning is often substantially faster than the alternatives, but is not always the fastest, necessitating a cost-based approach. We also discuss extensions of all our approaches to multi-table joins as well as to Hive.},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  pages     = {1969–1984},
  numpages  = {16},
  keywords  = {analytics, joins, machine learning, feature engineering},
  location  = {Melbourne, Victoria, Australia},
  series    = {SIGMOD '15}
}

@book{rel_db_glossary,
  title     = {The Relational Database Dictionary: A Comprehensive Glossary of Relational Terms and Concepts, with Illustrative Examples},
  author    = {Chris J. Date},
  publisher = {O'Reilly Media},
  year      = {2006},
  month     = {9},
  day       = {7},
  date      = {2006-09-07},
  edition   = {Paperback},
  language  = {English},
  pages     = {128},
  keywords  = {Databases & Big Data, Computer Software, English Dictionaries & Thesauruses, Dictionaries, Database Storage & Design},
  isbn      = {978-0596527983},
  isbn10    = {0596527985},
  url       = {https://lead.to/amazon/com/?op=bt&la=en&cu=usd&key=0596527985},
  url-am    = {https://www.amazon.com/dp/0596527985/},
  url-go    = {https://books.google.com/books?isbn=9780596527983},
  url-wo    = {https://www.worldcat.org/search?qt=l2a&q=isbn%3A9780596527983},
  price     = {$6.05},
  condition = {Used(Good)},
  timestamp = {2023-08-17}
}

@article{santoku_kumar_demonstration_2015,
  title      = {Demonstration of {Santoku}: optimizing machine learning over normalized data},
  volume     = {8},
  shorttitle = {Demonstration of {Santoku}},
  doi        = {10.14778/2824032.2824087},
  number     = {12},
  journal    = {Proceedings of the VLDB Endowment},
  author     = {Kumar, Arun and Jalal, Mona and Yan, Boqun and Naughton, Jeffrey and Patel, Jignesh M.},
  month      = aug,
  year       = {2015},
  keywords   = {notion},
  pages      = {1864--1867}
}

@mastersthesis{schijndel_cost_estimation,
  title       = {Cost Estimation for Factorized Machine Learning in Data Integration Scenarios},
  author      = {van Schijndel, Jessie},
  year        = {2022},
  institution = {Delft University of Technology},
  url         = {http://resolver.tudelft.nl/uuid:cb576f2e-adf4-43cd-859f-99910c197aed},
  type        = {Master's Thesis}
}

@article{tpcx-ai,
  author     = {Br\"{u}cke, Christoph and H\"{a}rtling, Philipp and Palacios, Rodrigo D Escobar and Patel, Hamesh and Rabl, Tilmann},
  title      = {TPCx-AI - An Industry Standard Benchmark for Artificial Intelligence and Machine Learning Systems},
  year       = {2023},
  issue_date = {August 2023},
  publisher  = {VLDB Endowment},
  volume     = {16},
  number     = {12},
  issn       = {2150-8097},
  url        = {https://doi.org/10.14778/3611540.3611554},
  doi        = {10.14778/3611540.3611554},
  abstract   = {Artificial intelligence (AI) and machine learning (ML) techniques have existed for years, but new hardware trends and advances in model training and inference have radically improved their performance. With an ever increasing amount of algorithms, systems, and hardware solutions, it is challenging to identify good deployments even for experts. Researchers and industry experts have observed this challenge and have created several benchmark suites for AI and ML applications and systems. While they are helpful in comparing several aspects of AI applications, none of the existing benchmarks measures end-to-end performance of ML deployments. Many have been rigorously developed in collaboration between academia and industry, but no existing benchmark is standardized. In this paper, we introduce the TPC Express Benchmark for Artificial Intelligence (TPCx-AI), the first industry standard benchmark for end-to-end machine learning deployments. TPCx-AI is the first AI benchmark that represents the pipelines typically found in common ML and AI workloads. TPCx-AI provides a full software kit, which includes data generator, driver, and two full workload implementations, one based on Python libraries and one based on Apache Spark. We describe the complete benchmark and show benchmark results for various scale factors. TPCx-AI's core contributions are a novel unified data set covering structured and unstructured data; a fully scalable data generator that can generate realistic data from GB up to PB scale; and a diverse and representative workload using different data types and algorithms, covering a wide range of aspects of real ML workloads such as data integration, data processing, training, and inference.},
  journal    = {Proc. VLDB Endow.},
  month      = {09},
  pages      = {3649–3661},
  numpages   = {13}
}

@article{TrinityPolyglotFrameworkFactorized2021,
  title        = {Towards a Polyglot Framework for Factorized {{ML}}},
  author       = {Justo, David and Yi, Shaoqing and Stadler, Lukas and Polikarpova, Nadia and Kumar, Arun},
  date         = {2021-07-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume       = {14},
  number       = {12},
  pages        = {2918--2931},
  issn         = {2150-8097},
  doi          = {10.14778/3476311.3476372},
  url          = {https://doi.org/10.14778/3476311.3476372},
  urldate      = {2022-06-15},
  abstract     = {Optimizing machine learning (ML) workloads on structured data is a key concern for data platforms. One class of optimizations called "factorized ML" helps reduce ML runtimes over multi-table datasets by pushing ML computations down through joins, avoiding the need to materialize such joins. The recent Morpheus system automated factorized ML to any ML algorithm expressible in linear algebra (LA). But all such prior factorized ML/LA stacks are restricted by their chosen programming language (PL) and runtime environment, limiting their reach in emerging industrial data science environments with many PLs (R, Python, etc.) and even cross-PL analytics workflows. Re-implementing Morpheus from scratch in each PL/environment is a massive developability overhead for implementation, testing, and maintenance. We tackle this challenge by proposing a new system architecture, Trinity, to enable factorized LA logic to be written only once and easily reused across many PLs/LA tools in one go. To do this in an extensible and efficient manner without costly data copies, Trinity leverages and extends an emerging industrial polyglot compiler and runtime, Oracle's GraalVM. Trinity enables factorized LA in multiple PLs and even cross-PL workflows. Experiments with real datasets show that Trinity is significantly faster than materialized execution ({$>$} 8x speedups in some cases), while being largely competitive to a prior single PL-focused Morpheus stack.},
  keywords     = {notion},
  file         = {/Users/jessie/Zotero/storage/7W6G64YU/Justo et al. - 2021 - Towards a polyglot framework for factorized ML.pdf}
}

@inproceedings{tvm,
  author    = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title     = {TVM: an automated end-to-end optimizing compiler for deep learning},
  year      = {2018},
  isbn      = {9781931971478},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms - such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
  booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {579–594},
  numpages  = {16},
  location  = {Carlsbad, CA, USA},
  series    = {OSDI'18}
}

@misc{nvidia-gpu-performance:online,
  author       = {},
  title        = {GPU Performance Background User's Guide — NVIDIA Docs},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html}},
  month        = {2},
  year         = {2023},
  note         = {(Accessed on 02/20/2024)}
}

@article{explainable-deep-learning,
  title    = {Explainable deep learning for efficient and robust pattern recognition: A survey of recent developments},
  journal  = {Pattern Recognition},
  volume   = {120},
  pages    = {108102},
  year     = {2021},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2021.108102},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320321002892},
  author   = {Xiao Bai and Xiang Wang and Xianglong Liu and Qiang Liu and Jingkuan Song and Nicu Sebe and Been Kim},
  keywords = {Explainable deep learning, Network compression and acceleration, Adversarial robustness, Stability in deep learning},
  abstract = {Deep learning has recently achieved great success in many visual recognition tasks. However, the deep neural networks (DNNs) are often perceived as black-boxes, making their decision less understandable to humans and prohibiting their usage in safety-critical applications. This guest editorial introduces the thirty papers accepted for the Special Issue on Explainable Deep Learning for Efficient and Robust Pattern Recognition. They are grouped into three main categories: explainable deep learning methods, efficient deep learning via model compression and acceleration, as well as robustness and stability in deep learning. For each of the three topics, a survey of the representative works and latest developments is presented, followed by the brief introduction of the accepted papers belonging to this topic. The special issue should be of high relevance to the reader interested in explainable deep learning methods for efficient and robust pattern recognition applications and it helps promoting the future research directions in this field.}
}

@inproceedings{cupy_learningsys2017,
  author    = {Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman},
  title     = {CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations},
  booktitle = {Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)},
  year      = {2017},
  url       = {http://learningsys.org/nips17/assets/papers/paper_16.pdf}
}

@inbook{intel-mkl,
  author    = {Wang, Endong
               and Zhang, Qing
               and Shen, Bo
               and Zhang, Guangyong
               and Lu, Xiaowei
               and Wu, Qing
               and Wang, Yajuan},
  title     = {Intel Math Kernel Library},
  booktitle = {High-Performance Computing on the Intel® Xeon Phi{\texttrademark}: How to Fully Exploit MIC Architectures},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {167--188},
  abstract  = {In order to achieve optimal performance on multi-core and multi-processor systems, we need to fully use the features of parallelism and manage the memory hierarchical characters efficiently. The performance of sequential codes relies on the instruction-level and register-level SIMD parallelism, and also on high-speed cache-blocking functions. Threading applications need advanced planning to achieve satisfactory load balancing.},
  isbn      = {978-3-319-06486-4},
  doi       = {10.1007/978-3-319-06486-4_7},
  url       = {https://doi.org/10.1007/978-3-319-06486-4_7}
}

@inproceedings{valentine-data-generator,
  author    = {Koutras, Christos and Siachamis, George and Ionescu, Andra and Psarakis, Kyriakos and Brons, Jerry and Fragkoulis, Marios and Lofi, Christoph and Bonifati, Angela and Katsifodimos, Asterios},
  booktitle = {2021 IEEE 37th International Conference on Data Engineering (ICDE)},
  title     = {Valentine: Evaluating Matching Techniques for Dataset Discovery},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {468-479},
  keywords  = {Fabrication;Conferences;Lakes;Data engineering;Open source software},
  doi       = {10.1109/ICDE51399.2021.00047}
}

@inbook{tgds-Fagin2009,
  author    = {Fagin, Ronald},
  editor    = {LIU, LING
               and {\"O}ZSU, M. TAMER},
  title     = {Tuple-Generating Dependencies},
  booktitle = {Encyclopedia of Database Systems},
  year      = {2009},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {3201--3202},
  isbn      = {978-0-387-39940-9},
  doi       = {10.1007/978-0-387-39940-9_1274},
  url       = {https://doi.org/10.1007/978-0-387-39940-9_1274}
}


@article{gpu-in-ml-survey,
  doi       = {10.13140/RG.2.2.20697.26725},
  url       = {http://rgdoi.net/10.13140/RG.2.2.20697.26725},
  author    = {Jangamreddy,  Nikhil},
  language  = {en},
  title     = {A Survey on Specialised Hardware for Machine Learning},
  publisher = {Unpublished},
  year      = {2019}
}

@misc{cuda-programming-guide,
  author       = {},
  title        = {CUDA C++ Programming Guide},
  howpublished = {\url{https://docs.nvidia.com/cuda/archive/11.2.0/pdf/CUDA_C_Programming_Guide.pdf}},
  month        = {},
  year         = {},
  note         = {(Accessed on 03/07/2024)}
}

@article{rfecv,
  volume    = {46},
  issn      = {0885-6125},
  url       = {http://dx.doi.org/10.1023/A:1012487302797},
  doi       = {10.1023/a:1012487302797},
  number    = {1/3},
  journal   = {Machine Learning},
  publisher = {Springer Science and Business Media LLC},
  author    = {Guyon,  Isabelle and Weston,  Jason and Barnhill,  Stephen and Vapnik,  Vladimir},
  year      = {2002},
  pages     = {389–422}
}

@inproceedings{TreeRNN,
  title     = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  url       = {http://dx.doi.org/10.3115/v1/P15-1150},
  doi       = {10.3115/v1/p15-1150},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  author    = {Tai,  Kai Sheng and Socher,  Richard and Manning,  Christopher D.},
  year      = {2015}
}

@article{roofline,
  author     = {Williams, Samuel and Waterman, Andrew and Patterson, David},
  title      = {Roofline: an insightful visual performance model for multicore architectures},
  year       = {2009},
  issue_date = {April 2009},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {52},
  number     = {4},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/1498765.1498785},
  doi        = {10.1145/1498765.1498785},
  abstract   = {The Roofline model offers insight on how to improve the performance of software and hardware.},
  journal    = {Commun. ACM},
  month      = {4},
  pages      = {65–76},
  numpages   = {12}
}

@inproceedings{deep-learning-cost-prediction,
  author    = {Justus, Daniel and Brennan, John and Bonner, Stephen and McGough, Andrew Stephen},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  title     = {Predicting the Computational Cost of Deep Learning Models},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {3873-3882},
  keywords  = {Training;Deep learning;Hardware;Predictive models;Neural networks;Computational modeling;Timing;Machine Learning;Benchmark;Performance;Prediction},
  doi       = {10.1109/BigData.2018.8622396}
}

@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

@inproceedings{xgboost,
  author    = {Chen, Tianqi and Guestrin, Carlos},
  title     = {{XGBoost}: A Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  series    = {KDD '16},
  year      = {2016},
  isbn      = {978-1-4503-4232-2},
  location  = {San Francisco, California, USA},
  pages     = {785--794},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2939672.2939785},
  doi       = {10.1145/2939672.2939785},
  acmid     = {2939785},
  publisher = {ACM},
  address   = {New York, NY, USA},
  keywords  = {large-scale machine learning}
}

@article{xgboost_imbalanced_data,
  author   = {Ping Zhang and Yiqiao Jia and Youlin Shang},
  title    = {Research and application of XGBoost in imbalanced data},
  journal  = {International Journal of Distributed Sensor Networks},
  volume   = {18},
  number   = {6},
  pages    = {15501329221106935},
  year     = {2022},
  doi      = {10.1177/15501329221106935},
  url      = {https://doi.org/10.1177/15501329221106935},
  eprint   = { https://doi.org/10.1177/15501329221106935},
  abstract = { As a new and efficient ensemble learning algorithm, XGBoost has been widely applied for its multitudinous advantages, but its classification effect in the case of data imbalance is often not ideal. Aiming at this problem, an attempt was made to optimize the regularization term of XGBoost, and a classification algorithm based on mixed sampling and ensemble learning is proposed. The main idea is to combine SVM-SMOTE over-sampling and EasyEnsemble under-sampling technologies for data processing, and then obtain the final model based on XGBoost by training and ensemble. At the same time, the optimal parameters are automatically searched and adjusted through the Bayesian optimization algorithm to realize classification prediction. In the experimental stage, the G-mean and area under the curve (AUC) values are used as evaluation indicators to compare and analyze the classification performance of different sampling methods and algorithm models. The experimental results on the public data set also verify the feasibility and effectiveness of the proposed algorithm. }
}

