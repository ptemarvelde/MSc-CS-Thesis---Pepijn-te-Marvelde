@inproceedings{orion_learning_gen_lin_models,
  author    = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M.},
  title     = {Learning Generalized Linear Models Over Normalized Data},
  year      = {2015},
  isbn      = {9781450327589},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2723372.2723713},
  doi       = {10.1145/2723372.2723713},
  abstract  = {Enterprise data analytics is a booming area in the data management industry. Many companies are racing to develop toolkits that closely integrate statistical and machine learning techniques with data management systems. Almost all such toolkits assume that the input to a learning algorithm is a single table. However, most relational datasets are not stored as single tables due to normalization. Thus, analysts often perform key-foreign key joins before learning on the join output. This strategy of learning after joins introduces redundancy avoided by normalization, which could lead to poorer end-to-end performance and maintenance overheads due to data duplication. In this work, we take a step towards enabling and optimizing learning over joins for a common class of machine learning techniques called generalized linear models that are solved using gradient descent algorithms in an RDBMS setting. We present alternative approaches to learn over a join that are easy to implement over existing RDBMSs. We introduce a new approach named factorized learning that pushes ML computations through joins and avoids redundancy in both I/O and computations. We study the tradeoff space for all our approaches both analytically and empirically. Our results show that factorized learning is often substantially faster than the alternatives, but is not always the fastest, necessitating a cost-based approach. We also discuss extensions of all our approaches to multi-table joins as well as to Hive.},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  pages     = {1969–1984},
  numpages  = {16},
  keywords  = {analytics, joins, machine learning, feature engineering},
  location  = {Melbourne, Victoria, Australia},
  series    = {SIGMOD '15}
}

@mastersthesis{schijndel_cost_estimation,
  title       = {Cost Estimation for Factorized Machine Learning in Data Integration Scenarios},
  author      = {van Schijndel, Jessie},
  year        = {2022},
  institution = {Delft University of Technology},
  url         = {http://resolver.tudelft.nl/uuid:cb576f2e-adf4-43cd-859f-99910c197aed},
  type        = {Master's Thesis}
}

@misc{halide,
  author       = {},
  title        = {What is Halide? Why should I use it? — Learning Halide by Examples documentation},
  howpublished = {\url{https://people.csail.mit.edu/tzumao/tmp/learning_halide/what_is_halide.html}},
  month        = {},
  year         = {},
  note         = {(Accessed on 06/15/2023)}
}

@article{halide_cost_model,
  author     = {Adams, Andrew and Ma, Karima and Anderson, Luke and Baghdadi, Riyadh and Li, Tzu-Mao and Gharbi, Micha\"{e}l and Steiner, Benoit and Johnson, Steven and Fatahalian, Kayvon and Durand, Fr\'{e}do and Ragan-Kelley, Jonathan},
  title      = {Learning to Optimize Halide with Tree Search and Random Programs},
  year       = {2019},
  issue_date = {August 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {38},
  number     = {4},
  issn       = {0730-0301},
  url        = {https://doi.org/10.1145/3306346.3322967},
  doi        = {10.1145/3306346.3322967},
  abstract   = {We present a new algorithm to automatically schedule Halide programs for high-performance image processing and deep learning. We significantly improve upon the performance of previous methods, which considered a limited subset of schedules. We define a parameterization of possible schedules much larger than prior methods and use a variant of beam search to search over it. The search optimizes runtime predicted by a cost model based on a combination of new derived features and machine learning. We train the cost model by generating and featurizing hundreds of thousands of random programs and schedules. We show that this approach operates effectively with or without autotuning. It produces schedules which are on average almost twice as fast as the existing Halide autoscheduler without autotuning, or more than twice as fast with, and is the first automatic scheduling algorithm to significantly outperform human experts on average.},
  journal    = {ACM Trans. Graph.},
  month      = {7},
  articleno  = {121},
  numpages   = {12},
  keywords   = {optimizing compilers, halide}
}

@misc{tvm,
  doi       = {10.48550/ARXIV.1802.04799},
  url       = {https://arxiv.org/abs/1802.04799},
  author    = {Chen,  Tianqi and Moreau,  Thierry and Jiang,  Ziheng and Zheng,  Lianmin and Yan,  Eddie and Cowan,  Meghan and Shen,  Haichen and Wang,  Leyuan and Hu,  Yuwei and Ceze,  Luis and Guestrin,  Carlos and Krishnamurthy,  Arvind},
  keywords  = {Machine Learning (cs.LG),  Artificial Intelligence (cs.AI),  Programming Languages (cs.PL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{nsight_compute,
  author       = {},
  title        = {Nsight Compute :: Nsight Compute Documentation},
  howpublished = {\url{https://docs.nvidia.com/nsight-compute/NsightCompute/index.html}},
  month        = {},
  year         = {},
  note         = {(Accessed on 06/20/2023)}
}

@inproceedings{2016hamletsigmod,
  author    = {Arun Kumar and
               Jeffrey F. Naughton and
               Jignesh M. Patel and
               Xiaojin Zhu},
  editor    = {Fatma {\"{O}}zcan and
               Georgia Koutrika and
               Sam Madden},
  title     = {To Join or Not to Join?: Thinking Twice about Joins before Feature
               Selection},
  booktitle = {Proceedings of the 2016 International Conference on Management of
               Data, {SIGMOD} Conference 2016, San Francisco, CA, USA, June 26 -
               July 01, 2016},
  pages     = {19--34},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2882903.2882952},
  doi       = {10.1145/2882903.2882952},
  timestamp = {Wed, 14 Nov 2018 10:56:20 +0100},
  biburl    = {https://dblp.org/rec/conf/sigmod/KumarNPZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{santoku_kumar_demonstration_2015,
  title      = {Demonstration of {Santoku}: optimizing machine learning over normalized data},
  volume     = {8},
  shorttitle = {Demonstration of {Santoku}},
  doi        = {10.14778/2824032.2824087},
  number     = {12},
  journal    = {Proceedings of the VLDB Endowment},
  author     = {Kumar, Arun and Jalal, Mona and Yan, Boqun and Naughton, Jeffrey and Patel, Jignesh M.},
  month      = aug,
  year       = {2015},
  keywords   = {notion},
  pages      = {1864--1867}
}

@techreport{MorpheusPy,
  title       = {MorpheusPy: Factorized Machine Learning with NumPy},
  author      = {Li, Side and Kumar, Arun},
  institution = {Technical report, 2018. Available at https://adalabucsd. github. io/papers~…}
}

@article{TrinityPolyglotFrameworkFactorized2021,
  title        = {Towards a Polyglot Framework for Factorized {{ML}}},
  author       = {Justo, David and Yi, Shaoqing and Stadler, Lukas and Polikarpova, Nadia and Kumar, Arun},
  date         = {2021-07-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume       = {14},
  number       = {12},
  pages        = {2918--2931},
  issn         = {2150-8097},
  doi          = {10.14778/3476311.3476372},
  url          = {https://doi.org/10.14778/3476311.3476372},
  urldate      = {2022-06-15},
  abstract     = {Optimizing machine learning (ML) workloads on structured data is a key concern for data platforms. One class of optimizations called "factorized ML" helps reduce ML runtimes over multi-table datasets by pushing ML computations down through joins, avoiding the need to materialize such joins. The recent Morpheus system automated factorized ML to any ML algorithm expressible in linear algebra (LA). But all such prior factorized ML/LA stacks are restricted by their chosen programming language (PL) and runtime environment, limiting their reach in emerging industrial data science environments with many PLs (R, Python, etc.) and even cross-PL analytics workflows. Re-implementing Morpheus from scratch in each PL/environment is a massive developability overhead for implementation, testing, and maintenance. We tackle this challenge by proposing a new system architecture, Trinity, to enable factorized LA logic to be written only once and easily reused across many PLs/LA tools in one go. To do this in an extensible and efficient manner without costly data copies, Trinity leverages and extends an emerging industrial polyglot compiler and runtime, Oracle's GraalVM. Trinity enables factorized LA in multiple PLs and even cross-PL workflows. Experiments with real datasets show that Trinity is significantly faster than materialized execution ({$>$} 8x speedups in some cases), while being largely competitive to a prior single PL-focused Morpheus stack.},
  keywords     = {notion},
  file         = {/Users/jessie/Zotero/storage/7W6G64YU/Justo et al. - 2021 - Towards a polyglot framework for factorized ML.pdf}
}

@inproceedings{MorpheusFIEnablingOptimizingNonlinear2019,
  title      = {Enabling and {{Optimizing Non-linear Feature Interactions}} in {{Factorized Linear Algebra}}},
  booktitle  = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author     = {Li, Side and Chen, Lingjiao and Kumar, Arun},
  date       = {2019-06-25},
  pages      = {1571--1588},
  publisher  = {{ACM}},
  location   = {{Amsterdam Netherlands}},
  doi        = {10.1145/3299869.3319878},
  url        = {https://dl.acm.org/doi/10.1145/3299869.3319878},
  abstract   = {Accelerating machine learning (ML) over relational data is a key focus of the database community. While many real-world datasets are multi-table, most ML tools expect single-table inputs, forcing users to materialize joins before ML, leading to data redundancy and runtime waste. Recent works on “factorized ML” address such issues by pushing ML through joins. However, they have hitherto been restricted to ML models linear in the feature space, rendering them less effective when users construct non-linear feature interactions such as pairwise products to boost ML accuracy. In this work, we take a first step towards closing this gap by introducing a new abstraction to enable pairwise feature interactions in multi-table data and present an extensive framework of algebraic rewrite rules for factorized LA operators over feature interactions. Our rewrite rules carefully exploit the interplay of the redundancy caused by both joins and interactions. We prototype our framework in Python to build a tool we call MorpheusFI. An extensive empirical evaluation with both synthetic and real datasets shows that MorpheusFI yields up to 5x speedups over materialized execution for a popular second-order gradient method and even an order of magnitude speedups over a popular stochastic gradient method.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  langid     = {english},
  keywords   = {notion},
  file       = {/Users/jessie/Zotero/storage/GYJXJ3WX/Li et al. - 2019 - Enabling and Optimizing Non-linear Feature Interac.pdf}
}

@article{morpheus,
  title   = {Towards Linear Algebra over Normalized Data},
  author  = {Chen, L. and Kumar, A. and Naughton, J. and Patel, J. M.},
  journal = {PVLDB},
  volume  = {10},
  number  = {11},
  year    = {2017}
}

@inproceedings{lmfao_schleich_layered_2019,
  author    = {Schleich, Maximilian and Olteanu, Dan and Abo Khamis, Mahmoud and Ngo, Hung Q. and Nguyen, XuanLong},
  title     = {A Layered Aggregate Engine for Analytics Workloads},
  year      = {2019},
  isbn      = {9781450356435},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3299869.3324961},
  doi       = {10.1145/3299869.3324961},
  booktitle = {Proceedings of the 2019 International Conference on Management of Data},
  pages     = {1642–1659},
  numpages  = {18},
  keywords  = {mutliple aggregate optimization, structure-aware analytics},
  location  = {Amsterdam, Netherlands},
  series    = {SIGMOD '19}
}

@inproceedings{acdc_khamis_2018,
  address    = {Houston TX USA},
  title      = {{AC}/{DC}: {In}-{Database} {Learning} {Thunderstruck}},
  shorttitle = {{AC}/{DC}},
  doi        = {10.1145/3209889.3209896},
  language   = {en},
  booktitle  = {Proceedings of the {Second} {Workshop} on {Data} {Management} for {End}-{To}-{End} {Machine} {Learning}},
  publisher  = {ACM},
  author     = {Khamis, Mahmoud Abo and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
  month      = jun,
  year       = {2018},
  keywords   = {notion},
  pages      = {1--10}
}

@inproceedings{f_gmm_DBLP:conf/icde/ChengKZ021,
  author    = {Zhaoyue Cheng and
               Nick Koudas and
               Zhe Zhang and
               Xiaohui Yu},
  title     = {Efficient Construction of Nonlinear Models over Normalized Data},
  booktitle = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
               Chania, Greece, April 19-22, 2021},
  pages     = {1140--1151},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICDE51399.2021.00103},
  doi       = {10.1109/ICDE51399.2021.00103},
  timestamp = {Fri, 25 Jun 2021 11:31:22 +0200},
  biburl    = {https://dblp.org/rec/conf/icde/ChengKZ021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{f_schleich,
  author    = {Schleich, Maximilian and Olteanu, Dan and Ciucanu, Radu},
  title     = {Learning Linear Regression Models over Factorized Joins},
  year      = {2016},
  isbn      = {9781450335317},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2882903.2882939},
  doi       = {10.1145/2882903.2882939},
  abstract  = {We investigate the problem of building least squares regression models over training datasets defined by arbitrary join queries on database tables. Our key observation is that joins entail a high degree of redundancy in both computation and data representation, which is not required for the end-to-end solution to learning over joins.We propose a new paradigm for computing batch gradient descent that exploits the factorized computation and representation of the training datasets, a rewriting of the regression objective function that decouples the computation of cofactors of model parameters from their convergence, and the commutativity of cofactor computation with relational union and projection. We introduce three flavors of this approach: F/FDB computes the cofactors in one pass over the materialized factorized join; Favoids this materialization and intermixes cofactor and join computation; F/SQL expresses this mixture as one SQL query.Our approach has the complexity of join factorization, which can be exponentially lower than of standard joins. Experiments with commercial, public, and synthetic datasets show that it outperforms MADlib, Python StatsModels, and R, by up to three orders of magnitude.},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  pages     = {3–18},
  numpages  = {16},
  keywords  = {factorized databases, linear regression, join processing},
  location  = {San Francisco, California, USA},
  series    = {SIGMOD '16}
}

@book{rel_db_glossary,
  title     = {The Relational Database Dictionary: A Comprehensive Glossary of Relational Terms and Concepts, with Illustrative Examples},
  author    = {Chris J. Date},
  publisher = {O'Reilly Media},
  year      = {2006},
  month     = {9},
  day       = {7},
  date      = {2006-09-07},
  edition   = {Paperback},
  language  = {English},
  pages     = {128},
  keywords  = {Databases & Big Data, Computer Software, English Dictionaries & Thesauruses, Dictionaries, Database Storage & Design},
  isbn      = {978-0596527983},
  isbn10    = {0596527985},
  url       = {https://lead.to/amazon/com/?op=bt&la=en&cu=usd&key=0596527985},
  url-am    = {https://www.amazon.com/dp/0596527985/},
  url-go    = {https://books.google.com/books?isbn=9780596527983},
  url-wo    = {https://www.worldcat.org/search?qt=l2a&q=isbn%3A9780596527983},
  price     = {$6.05},
  condition = {Used(Good)},
  timestamp = {2023-08-17}
}

@article{tpcx_ai,
  author     = {Br\"{u}cke, Christoph and H\"{a}rtling, Philipp and Palacios, Rodrigo D Escobar and Patel, Hamesh and Rabl, Tilmann},
  title      = {TPCx-AI - An Industry Standard Benchmark for Artificial Intelligence and Machine Learning Systems},
  year       = {2023},
  issue_date = {August 2023},
  publisher  = {VLDB Endowment},
  volume     = {16},
  number     = {12},
  issn       = {2150-8097},
  url        = {https://doi.org/10.14778/3611540.3611554},
  doi        = {10.14778/3611540.3611554},
  abstract   = {Artificial intelligence (AI) and machine learning (ML) techniques have existed for years, but new hardware trends and advances in model training and inference have radically improved their performance. With an ever increasing amount of algorithms, systems, and hardware solutions, it is challenging to identify good deployments even for experts. Researchers and industry experts have observed this challenge and have created several benchmark suites for AI and ML applications and systems. While they are helpful in comparing several aspects of AI applications, none of the existing benchmarks measures end-to-end performance of ML deployments. Many have been rigorously developed in collaboration between academia and industry, but no existing benchmark is standardized.In this paper, we introduce the TPC Express Benchmark for Artificial Intelligence (TPCx-AI), the first industry standard benchmark for end-to-end machine learning deployments. TPCx-AI is the first AI benchmark that represents the pipelines typically found in common ML and AI workloads. TPCx-AI provides a full software kit, which includes data generator, driver, and two full workload implementations, one based on Python libraries and one based on Apache Spark. We describe the complete benchmark and show benchmark results for various scale factors. TPCx-AI's core contributions are a novel unified data set covering structured and unstructured data; a fully scalable data generator that can generate realistic data from GB up to PB scale; and a diverse and representative workload using different data types and algorithms, covering a wide range of aspects of real ML workloads such as data integration, data processing, training, and inference.},
  journal    = {Proc. VLDB Endow.},
  month      = {09},
  pages      = {3649–3661},
  numpages   = {13}
}

@article{declaritve-data-analytics-survey,
  author   = {Makrynioti, Nantia and Vassalos, Vasilis},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {Declarative Data Analytics: A Survey},
  year     = {2021},
  volume   = {33},
  number   = {6},
  pages    = {2392-2411},
  keywords = {Task analysis;Data analysis;Programming;Optimization;Mathematical model;Analytical models;Prediction algorithms;Declarative programming;data science;machine learning;large-scale analytics},
  doi      = {10.1109/TKDE.2019.2958084}
}

@inproceedings{data-management-in-ML-kumar-2017,
  author    = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
  title     = {Data Management in Machine Learning: Challenges, Techniques, and Systems},
  year      = {2017},
  isbn      = {9781450341974},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3035918.3054775},
  doi       = {10.1145/3035918.3054775},
  abstract  = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
  booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
  pages     = {1717–1722},
  numpages  = {6},
  keywords  = {machine learning, data management},
  location  = {Chicago, Illinois, USA},
  series    = {SIGMOD '17}
}

@inproceedings{amalur,
  author    = {Hai, Rihan and Koutras, Christos and Ionescu, Andra and Li, Ziyu and Sun, Wenbo and van Schijndel, Jessie and Kang, Yan and Katsifodimos, Asterios},
  booktitle = {2023 IEEE 39th International Conference on Data Engineering (ICDE)},
  title     = {Amalur: Data Integration Meets Machine Learning},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {3729-3739},
  keywords  = {Training;Data privacy;Federated learning;Computational modeling;Data integration;Training data;Manuals},
  doi       = {10.1109/ICDE55515.2023.00301}
}