% !TEX root = ../../main.tex

\chapter{Cost Estimation}

\label{chapter:cost-estimation}
In this chapter, we present the results of our experimental work and explain their application in the formulation of four different cost models. The first section (\autoref{sec:5-motivation}) provides an overview of the results of the runtime experiment, motivating why a cost model is necessary. In \autoref{sec:5-gpu-performance-analysis} the collected profiling metrics are aggregated and analyzed, providing information on how the fundamentals of the GPU can affect the F/M trade-off. \autoref{sec:5-feature-engineering} outlines how we compiled and manipulated our independent variables to produce a suitable dataset for training models. Finally, in \autoref{sec:5-cost-models}, we explore various cost models created using the detailed results of the runtime and profiling experiments, and we assess the merits and drawbacks of our analytical, linear regression, XGBoost, and hybrid models.

\section{Motivation}
\label{sec:5-motivation}

This section illustrates the need for precise cost estimation when deciding between factorization and materialization strategies. We structure this motivation in three stages. First, we demonstrate the advantages of factorization. Next, we examine how the characteristics of the data and models affect the outcome. By visualizing the performance ratio ($\frac{\text{Time}_M}{\text{Time}_F}$) against various independent variables, we identify initial trends that affect the F/M trade-off. Finally, we highlight the significance of GPUs in this context.

\subsection{Benefit of Factorization}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{chapters/05_cost_estimation/figures/real_datasets_speedup.pdf}
  \vspace*{-5mm}
  \caption[Performance ratio with factorization on real datasets]{Average Performance ratio of ML models for positive cases ($\text{Time}_M > \text{Time}_F$), split per tested real dataset and compute type. }
  \label{fig:5-real-perf-ratio}
\end{figure}

The aim of factorized machine learning is to minimize unnecessary operations during model training, thus enhancing efficiency and speed. The comparative performance of factorization versus materialization on actual datasets is depicted in \autoref{fig:5-real-perf-ratio}. The exploration of factorization proves advantageous; it is observed that in $18\%$ of the instances tested on real datasets, factorization is faster, yielding an average acceleration $5.1\times$. In the most extreme cases, the training time is reduced by more than $20$ seconds, a $27$-fold reduction. Particularly in situations where training is recurrent, such as during hyperparameter tuning or online learning, this efficiency can translate into substantial time savings.

\subsection{Data \& Model Characteristics}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/motivation_perf_ratio_vs_data_chars.pdf}
  \caption[Performance ratio for various data characteristics]{Performance ratio ($\frac{\text{Time}_M}{\text{Time}_F}$) against independent variables. Broken down by compute type (CPU/GPU). $99\%$ confidence interval shown as shaded area. The sparsity ratio is defined as the sparsity of the source tables $S_k, k\in[1,n]$ divided by the sparsity of target table $T$. The sparsity of $S$ is defined as the total nonzero values in the base tables divided by the total number of cells in the base tables, $\frac{\sum_{k=1}^{n} nnz(S_k)}{\sum_{k=1}^{n} r_{S_k} \times c_{S_k}}$. High sparsity ratio means the target table is relatively denser than the source tables.}
  \label{fig:5-complexity-ratio-vs-data-chars}
\end{figure}
The influence of different data related independent variables on the performance ratio is discussed here, and shown in \autoref{fig:5-complexity-ratio-vs-data-chars}. The data reveals a modest inverse relationship between the performance ratio and the sparsity of the target table ($T$). Further analysis, shown in the second column, delves deeper into the connection between performance and sparsity. A higher sparsity ratio —meaning the base tables ($S_k$) have a higher count of zero-valued items relative to the target table ($T$)— generally results in factorization being more efficient than materialization. The plots on the far right suggest that a higher complexity ratio ($\frac{FLOP_M}{FLOP_F}$) tends to favor factorization as the optimal training approach. This aligns with the logical premise that factorization is advantageous when it eliminates redundant operations.

It is crucial to note that the relationship between these data characteristics and the acceleration provided by factorization becomes less distinct with GPU computations. The reason is that these computations are typically limited by memory capacity rather than processing power. This perspective is examined in detail in \autoref{sec:5-gpu-performance-analysis}, which delves into the metrics gathered during profiling experiments.

\subsection{Hardware Characteristics}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/motivation_speedup_per_operator_per_gpu.pdf}
  \caption[Box plots showing performance ratio against hardware]{Box plots showing performance ratio, of various operators on synthetic data, against independent hardware variables. The performance ratio is shown to be affected by hardware choice.}
  \label{fig:5-gpu-characteristics}
\end{figure}
The hardware used for computation not only affects a program's runtime, but also influences the F/M trade-off. Different processing units (i.e., CPU or GPU type) have unique thresholds at which factorization becomes more advantageous than materialization. This phenomenon is illustrated in \autoref{fig:5-gpu-characteristics}. The impact on the performance ratio varies depending on the hardware and the specific operation performed. For example, the average performance ratio for transposed Left Matrix Multiplication on the P100 GPU is $3.03\pm2.70$, in contrast to a marginally lower $2.32\pm2.21$. On the contrary, for left (scalar) multiplication, the V100 GPU exhibits a higher performance ratio of $0.21\pm0.04$, compared to the P100 $0.19\pm0.05$.

\begin{table}[ht]
  \centering
  \input{chapters/05_cost_estimation/figures/speedup_per_gpu.tex}
  \caption[Performance ratio of ML models for cases where factorization has positive impact.]{Mean performance ratio of ML models for cases where factorization is preferred over materialization (speedup > 1). This shows hardware choice is a large factor in when to choose factorization over materialization.}
  \label{tab:5-speedup-per-gpu}
\end{table}

In instances where factorization is favored over materialization (when $\text{Time}_M > \text{Time}_F$), the performance varies between GPU types. Both the average performance ratio and the number of instances where factorization outperforms materialization differ, as indicated in \autoref{tab:5-speedup-per-gpu}. This variation underscores the importance of hardware selection in the factorization versus materialization decision. However, the variation among GPU models is less pronounced than the disparity between CPUs and GPUs. Consequently, it may be more beneficial to consider the type of computation —CPU or GPU— as an independent variable in the cost models rather than focusing on specific GPU models.

When comparing the performance ratio with the complexity ratio for different hardware settings, an interesting pattern emerges. As described in \autoref{sec:3-cost-estimation-for-factorized-ml}, the complexity ratio is the ratio of the number of floating point operations (FLOPs) needed for the factorized case divided by the FLOPs of the materialized case. According to previous research, a higher complexity ratio typically indicates that factorization is more advantageous. However, our experiments reveal that while this is generally true for CPU operations, it does not always apply to GPU computations. This pattern is illustrated in \autoref{fig:5-complexity-ratio-vs-performance-ratio}, and the reasons behind it are explored further in the following section.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/motivation_speedup_complexity_ratio.pdf}
  \caption[Performance ratio plotted against complexity ratio]{Performance ratio, of various operators on synthetic data, against complexity ratio, broken down by CPU and GPU. 95\% Confidence interval shown as shaded area. Where a lot of operators show clear correlation between the complexity ratio and the performance ratio on CPU, this is not the case for GPU.}
  \label{fig:5-complexity-ratio-vs-performance-ratio}
\end{figure}


\section{GPU Performance Analysis}
\label{sec:5-gpu-performance-analysis}
An essential preliminary step in creating a precise cost model is understanding the performance attributes of the scenarios under examination. This section analyzes the profiling metrics collected during the experiments to understand the influence of hardware selection on the factorization versus materialization trade-off. The first analysis involves comparing the memory cost and the math cost in the profiled scenarios. According to NVIDIA, an effective method to predict the execution time of a GPU program is to calculate $max(T_{mem}, T_{math})$ \cite{nvidia-gpu-performance:online}. In this formula $T_{mem}$ is the time required to transfer data to and from the GPU memory, while $T_{math}$ denotes the time needed for actual computations. This approach is in line with the inherently parallel architecture of GPUs. Should the data transfer to the GPU prove insufficiently fast, the GPU's Streaming Multiprocessors will remain idle, awaiting data. This indicates a memory-bound program. Conversely, if $T_{math} > T_{mem}$, the program is considered compute bound. This section elaborates which scenario applies to our experiments and details how this knowledge can be harnessed to estimate the runtime for machine learning training scenarios.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.50\textwidth}
    \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/profiling-mem-vs-compute-materialized.pdf}
  \end{minipage}\hfill
  \begin{minipage}{0.50\textwidth}
    \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/profiling-mem-vs-compute-factorized.pdf}
  \end{minipage}
  \caption[Memory cost vs math cost of profiled scenarios]{Memory cost ($T_{mem}$) vs compute cost ($T_{math}$) of profiled scenarios. The memory cost is computed as the total number of bytes read and written to memory divided by the measured average memory bandwidth. The math cost is the number of cycles the Streaming Multiprocessors were active divided by the measured average SM frequency.}
  \label{fig:5-profiling-mem-vs-compute}
\end{figure}

\autoref{fig:5-profiling-mem-vs-compute} shows the relationship between memory time $T_{mem}$ and computation time $T_{math}$, revealing a strong correlation ($\rho = 0.99$). The data predominantly shows that the memory cost exceeds the computational cost, as most points lie below the $y=x$ line, indicating that the operations are memory-bound. This observation suggests that memory cost prediction should be prioritized in our cost estimation efforts. A notable distinction between factorization and materialization emerges when examining their correlation values; materialized cases exhibit a correlation of $\rho = 0.99$, while factorized cases show a significantly lower correlation of $\rho = 0.40$. This discrepancy arises because materialization typically involves handling a single matrix, or two in the case of matrix multiplication. In contrast, the factorized case on the normalized matrix involves multiple matrices ($S_k,I_k,M_k, k \in \{1 \ldots n\}$), each contributing to different computations. Although this diversity reduces both the memory and the computation costs, on average, it also results in a deviation from the $T_{mem} = T_{math}$ line due to the sequential execution of computations on these matrices within the GPU.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/roofline-plot.pdf}
  \caption[Roofline chart comparing F/M, per GPU]{Roofline chart showing where the performance of the GPUs lies in the memory-bound vs compute-bound spectrum. The subplots on top and right side of each figure show the distribution along the performance (GFLOPs) and operational intensity (FLOPS/byte) axes. Similar GPU types have similar distributions.}
  \label{fig:5-roofline-plot}
\end{figure}

\subsubsection*{Roofline Model}
More insights into the efficiency of the tested scenarios, and the differences between factorization and materialization, and GPU types, can be gained by using a roofline model. It is a “model that offers insight \ldots  on improving parallel software and hardware for floating point computations”~\cite{roofline}. The model delineates whether an operation is constrained by memory or computation. On the x-axis, it plots the arithmetic intensity of a program (in our context, this refers to an operator applied to a dataset), measured in FLOPs per byte. The y-axis represents the achievable performance in GFLOPS. The roofline itself (illustrated in gray) signifies the performance limit for a particular GPU, derived from its maximum memory bandwidth and computational capacity in FLOPs per second. The intersection point, known as the ``ridge point,'' indicates the minimum arithmetic intensity required to fully leverage the computational capabilities of the GPU. By plotting programs on this chart, one can infer whether a program is memory-bound (to the left of the ridge point) or compute-bound (to the right of the ridge point). This analysis is crucial because it highlights potential optimization possibilities by pinpointing performance bottlenecks.

The roofline charts depicted in \autoref{fig:5-roofline-plot} validate that most scenarios are constrained by memory. However, what stands out is the impact of the GPU type on performance, as well as the contrast between factorization and materialization. The difference among GPU models is particularly evident in the distribution plots to the right of each subplot. High-performance GPUs such as the A10G and A40 demonstrate a superior memory bandwidth compared to the GTX1660Ti, which often reaches a plateau in performance due to memory bandwidth limitations. Consequently, scenarios involving the A10G and A40 achieve a higher average performance.

The divergence between factorization and materialization highlighted in these graphs is informative. Materialized operators exhibit lower arithmetic complexity than their factorized counterparts, as indicated in the top density plots. This suggests that factorized operators, on average, are less constrained by memory, allowing them to better leverage the computational resources of the GPU. However, the significant variance in performance achieved by factorized operators, as shown in the right density plots, is due to the sequential execution of operations on different matrix segments. There is potential for optimization here, which could allow these operators to more effectively utilize the GPU’s computational power.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/roofline-operators.pdf}
  \caption[Roofline chart per operator]{Roofline chart comparing factorization and materialization for Left Matrix Multiplication, Left Scalar Multiplication and Row Summation Transpose. Metrics from NVIDIA A40.}
  \label{fig:5-roofline-operators}
\end{figure}

The roofline chart in \autoref{fig:5-roofline-operators} provides a detailed comparison of different operators, highlighting the distinctions between factorization and materialization. For Left Matrix Multiplication (LMM), the factorized approach exhibits a large variance in performance, aligning with the previously mentioned use of a normalized matrix. Despite this variance, factorization generally appears to be advantageous for LMM, as indicated by the profiling metrics that suggest a more complete utilization of GPU resources. In contrast, the difference in performance between factorized and materialized cases for Left Scalar Multiplication is less pronounced. This is expected since the factorized operation is simpler and involves only the multiplication of source matrices by a scalar. The rightmost plot reveals a clear underutilization of GPU capabilities for Transpose Row Summation in the factorized form, compared to the materialized form. This suggests that while factorization can be beneficial for certain operations, it may not always be the most efficient use of GPU resources. For a complete view of all operators, the full set of roofline charts is available in \autoref{appendix:analysis-additional-figures}, providing insight into the performance dynamics of each operator.

\section{Feature Engineering}
\label{sec:5-feature-engineering}
We proceed to construct a well-suited dataset for training our cost models. Building on the foundational work that has revealed the relations between speedup and various data and hardware characteristics in \autoref{sec:5-motivation}, and incorporating the insights from our profiling analysis we enrich and process the dataset.

This dataset, gathered through our comprehensive experiments, has a sample for each unique scenario, with the runtime and attributes of the dataset, the operator type, the model form (F/M), and the hardware configuration. The following section outlines the preprocessing and augmentation of this dataset with additional features. The enrichment process is designed to capture the complex interactions and patterns we have observed. The features are carefully crafted to enhance the models' ability to make predictions about when to use factorization or materialization. The complete set of features is detailed in \autoref{appendix:features}, with a focused discussion on a select subset presented in \autoref{tab:5-feature-subset}.

\subsection{Preprocessing Profiling Metrics}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/feature-engineering.pdf}
  \caption[Feature enrichment workflow]{Workflow of enriching the collected data with additional features from the profiling experiments. Items related to these profiling experiments are \textbf{bolded}, while the features from the data, model \& hardware characteristics are \textit{italicized}. Description for the aggregation process (“1”) is in \autoref{sec:5-feature-engineering}.}
  \label{fig:5-feature-enrichment}
\end{figure}
The feature engineering process begins with the integration of the profiling metrics into the dataset. This process presents two significant challenges. The first challenge arises from the fact that these metrics are collected at the kernel level, while our focus is on operator performance. To address this, we aggregate kernel-level metrics to reflect the performance of the entire scenario. The second challenge is the incomplete coverage of the metrics, as they were collected for a subset of scenarios. The strategy to overcome these obstacles is depicted in \autoref{fig:5-feature-enrichment} and is further detailed below.

The procedure to aggregate kernel-level metrics to operator-level metrics is as follows, and is also depicted in \autoref{fig:5-feature-enrichment} (marked by the “1”). For each scenario, we compute the sum of the metrics that are totals, i.e., the metrics with units of \textit{nanoseconds}, \textit{bytes}, or \textit{cycles} (refer to \autoref{tab:4-profiling-metrics} for applicable metrics). The remaining metrics are either \textit{percentages} or measures \textit{per second}. For these metrics, we calculate the weighted average, where the weight is the runtime of the kernel. This ensures that the metrics accurately represent the total runtime of the scenario. This procedure yields precise operator-level metrics for each scenario.

\begin{table}[ht]
  \centering
  \begin{tabular}{p{0.19\linewidth}p{0.37\linewidth}>{\footnotesize}p{0.35\linewidth}}
    \toprule
    Feature                                   & Formula                                                                                                 & Description                                                                                                                                       \\
    \midrule\midrule
    \texttt{dram\_bytes\_sum}                 & $\texttt{dram\_bytes\_read\_sum} + \texttt{dram\_bytes\_write\_sum}$                                    & Total number of bytes read and written to DRAM.                                                                                                   \\
    $T_{mem}$                                 & $\frac{\texttt{dram\_bytes\_sum}}{\texttt{memory\_throughput\_byte\_weighted\_mean}}$                   & Total memory bytes divided by the achieved memory throughput. Gives the cost of the involved memory operators in seconds.                         \\
    $T_{math}$                                & $\frac{\texttt{sm\_active\_cycles\_sum}}{\texttt{sm\_frequency\_weighted\_mean}}$                       & Total active cycles divided by the achieved frequency of the Streaming Multiprocessors. Gives the cost of the involved math operators in seconds. \\
    \texttt{FLOPs}                            & $\frac{\texttt{compute\_throughput\_weighted\_mean}}{100} \times \text{\small{gpu\_processing\_power}}$ & Total number of FLOPs executed in the scenario. Processing power is for double precision.                                                         \\
    \texttt{arithmetic\_-} \texttt{intensity} & $\frac{\texttt{FLOPs} \times \texttt{duration}}{\texttt{dram\_bytes\_sum}}$                             & The number of FLOPs executed per byte read or written to memory.                                                                                  \\

    \bottomrule
  \end{tabular}
  \caption[Derived features]{Overview of features computed from the profiling metrics. Features beginning with "gpu" are constants for the specific GPU used.}
  \label{tab:5-derived-features}
\end{table}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\linewidth]{chapters/05_cost_estimation/figures/analytical-regressor-fit.pdf}
  \caption[Analytical model memory cost prediction vs true values]{True $T_{mem}$ vs predicted $T_{mem}$ for the analytical model's $OperatorCost$ function. Tested on samples not used for training. MSE: $0.0295$.}
  \label{fig:5-analytical-regressor-fit}
\end{figure}
The second issue relates to the fact that metrics are not collected for all scenarios. This is addressed by fitting a statistical model to the collected metrics and employing this model to predict the missing metrics. The model, denoted as $OperatorCost$ in \autoref{fig:5-feature-enrichment}, is an ensemble of linear regressors. It uses aggregated metrics, hardware characteristics, and data characteristics as features, with memory cost $T_{mem}$ as the target variable. Each combination of operator and training type (F/M) has its own linear regression model.
To enhance the model, we engineer an additional set of features derived from the metrics collected, as shown in \autoref{tab:5-derived-features}.
The model is trained on a subset of the metrics collected and tested on the remaining metrics. Subsequently, the model is used to predict the missing metrics. This process is crucial to ensure that we have metrics for all scenarios, as the cost models require a complete dataset for training. The true versus predicted values of this model are depicted in \autoref{fig:5-analytical-regressor-fit}.



\subsection{Model-level Math- and Memory-cost}
\label{subsec:5-model-level-cost}
In this section, we describe the process of calculating the memory and mathematical costs for each scenario. Contrary to the previous section, which estimated the memory cost as perceived by the GPU, we now compute the theoretical costs. Memory cost is determined by dividing the total number of bytes read and written to memory by memory throughput. The mathematical cost is computed as the total number of FLOPs required for a computation. These definitions align with those used in the profiling experiments. However, in this context, we derive them from the available data and model characteristics, rather than predicting them using a regressor, as was explained in the preceding section.

The complexity of an operator or model, as previously detailed, is equal to the mathematical costs and is determined by examining the algorithms and summing the computations performed. This process takes into account various data characteristics, such as the number of nonzero items and dataset sizes. For memory costs, a similar approach is employed, but we sum the number of bytes read and written to memory. For each Machine Learning (ML) model, we incorporate both the total summed costs and the costs per involved operator as features. This results in a multitude of additional features that can be utilized to train the cost models.

A subset of these features is presented in \autoref{tab:5-feature-subset}. The complete set of features is displayed in \autoref{appendix:features}. These features are used to train the cost models, as will be detailed in the subsequent section.

\begin{table}[ht]
  \input{chapters/05_cost_estimation/auto-generated/feature-table-subset.tex}
  \caption[Feature table]{Table showing a subset of the base, and derived/engineered features used for training the cost models}
  \label{tab:5-feature-subset}
\end{table}

\section{Cost Models}
\label{sec:5-cost-models}
This section details the process of developing four different types of cost models, each capable of choosing when to favor factorization over materialization. These models are constructed using the enriched dataset, as described in the preceding section. We start with an introduction of the metrics used to evaluate the performance of the cost model (\autoref{sec:5-metrics}), followed by the models themselves. The first model, designed to be as interpretable as possible, is an analytical model (\autoref{subsec:5-analytical}). Next, we fit a range of linear regressors to the dataset to create a linear ML-based cost model (\autoref{subsec:5-linear-regression}). The third model is a tree-boosting model, used to demonstrate the performance of a more intricate model (\autoref{subsec:5-xgboost}). The final model is a hybrid model that merges linear regression and tree-boosting models to produce a more precise model.

\subsection{Problem Modeling and Assessing Performance}
\label{sec:5-metrics}
The decision whether to opt for factorization or materialization is a binary classification problem. However, it is more important to accurately predict scenarios that save more time. Therefore, we use regression models instead of classification models. This approach enables us to define the decision boundary, allowing greater flexibility in prioritizing scenarios with greater performance benefits.

\subsubsection{Metrics to Assess Cost Model Performance}
To assess the effectiveness of the cost models, we use a variety of metrics that provide insight into both the accuracy and efficiency of the models. Previous work~\cite{amalur_tkde24,orion_learning_gen_lin_models,MorpheusFI} predominantly uses accuracy and speedup (performance ratio) to evaluate the efficacy of their cost estimation methods. However, due to the imbalanced nature of the dataset, where most scenarios lean toward materialization, accuracy is not an appropriate measure. Similarly, the performance ratio has its limitations since averaging the ratio of multiple scenarios into a single value may lead to information being lost, such as the amount of time saved, which is a more informative metric to evaluate the performance of cost models across various scenarios.

\begin{table}[ht]
  \centering
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{p{0.2\linewidth}p{0.35\linewidth}p{0.4\linewidth}}
      \toprule
      Metric                      & Formula                                                                                                 & Description                                                                           \\
      \midrule\midrule
      Time Saved                  & $\sum\text{Time}_M - \sum \text{Time}_F$ where factorization is \textbf{predicted to be faster}         & The sum of the time saved for all positively predicted scenarios.                     \\
      Maximum Possible Time Saved & $\sum \text{Time}_M - \sum \text{Time}_F$ where factorization \textbf{is faster}                        & The maximum possible time that could be saved.                                        \\
      Weighted Performance Ratio  & $\frac{\sum \text{Time}_M}{\sum \text{Time}_F}$ where  factorization is \textbf{predicted to be faster} & The ratio of the total materialization time to the total factorization time.          \\
      Utility                     & $U = \frac{\text{Time Saved}}{\text{Max. Time Saved}}$                                                  & The fraction of time saved when using this model, relative to the maximum time saved. \\
      \bottomrule
    \end{tabular}}
  \caption{Metrics to assess cost model performance}
  \label{tab:metrics}
\end{table}


Consequently, we introduce a new metric, the time saved. It is the difference between the sum of materialized times and the sum of factorized times, for those cases where the cost models predict factorization to be faster. This metric offers a more practical assessment of the effectiveness of the model in real-world scenarios. Cases where the model predicts materialization to be faster are less relevant, as such a case will not result in a time loss compared to the normal, materialized execution. However, to evaluate whether the models exhibit adequate performance, the maximum potential time saved is also considered as a benchmark. To still allow reasoning about the achieved speedup of a set of scenarios, we adjust the performance ratio to be a weighted average according to the training time of a scenario. The final metric, used to assess generalizability as discussed in \autoref{subsec:6-generalizability}, is the utility of a model, which represents the proportion of time saved by using this model in relation to the maximum achievable time savings. These metrics are outlined in \autoref{tab:metrics}.

\subsection{Analytical}
\label{subsec:5-analytical}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/analytical-architecture.pdf}
  \caption[Analytical model Architecture]{Architecture of the Analytical model. Shows the control flow of inputs to a final decision on whether to use factorization or materialization. $OperatorCost$ is the function defined in \autoref{fig:5-feature-enrichment}.}
  \label{fig:5-analytical-architecture}
\end{figure}

As demonstrated in \autoref{sec:5-gpu-performance-analysis}, the runtime of the evaluated Machine Learning scenarios is primarily determined by the time required to read and write to memory. Consequently, to generate an accurate prediction of the execution time of a scenario, we can use the memory cost as a proxy measure, an approach adopted by this analytical model.

\subsubsection*{ANLY.1 Profiling Metrics Based model}
The procedure to calculate the factorized and materialized memory cost is shown in \autoref{fig:5-analytical-architecture}. First, we gather the operators used in the model under evaluation. Next, we employ a regression model, fitted to the collected profiling metrics (as illustrated in \autoref{fig:5-feature-enrichment}), to predict the memory cost of the factorized and materialized operators. This step is fit, as we lack profiling metrics for the scenarios under test. Finally, we sum the costs for each training type and select the scenario with the lowest predicted runtime.

A significant limitation of this model is its dependence on a pre-trained regressor to predict the memory cost. This regressor is trained on a subset of the collected metrics and is used to predict missing metrics. As it is only fit to a subset of possible operator scenarios, it is unsuitable for predicting the cost of operations that deviate from the tested scenarios. The development of a model capable of predicting the memory cost for all scenarios is a complex undertaking and is left for future work.

\subsubsection*{ANLY.2 Model-level Memory Cost Analysis Based model}
By inspecting the operations performed during ML model training, we can calculate the number of bytes read and written during computation. This is achieved by calculating the number of bytes read and written for each operation and summing these values. The memory cost is then determined by dividing the total number of bytes read and written by the GPU’s memory throughput. Compared to ANLY.1, this model is more adaptable to new ML models, as it does not require a pre-trained regressor to estimate the operator cost. However, it does not consider other factors that influence memory cost, such as cache layout and hit rates.

\subsubsection{Analytical Model Evaluation}
Both analytical models yield a predicted memory cost for the factorized and materialized cases. To arrive at the final decision, we compute the ratio between the cases and opt for factorization if this ratio ($\frac{M}{F}$) exceeds a certain threshold. This threshold is fine-tuned to minimize the number of false positives while still favoring factorization in instances where it significantly outperforms materialization in terms of time efficiency. For the first analytical model (ANLY.1), we set the threshold at $1.7$, and for ANLY.2, we set it at $10.0$. The fact that the model only performs well with such a high ratio of materialized to factorized memory cost indicates that factorization introduces considerable overhead in areas not captured by this simplistic memory estimation.

The results of the evaluation are shown in \autoref{fig:5-analytical-model-evaluation}. Both models perform similarly well. The ANLY.1 model predicts much fewer false positives, but the sum of time lost by these false positives is almost the same as that of the FPs for ANLY.2. Subtracting the time lost for the FPs from the time saved for the true positives, we see that ANLY.2 saves more time than ANLY.1. This is likely due to the fact that ANLY.2 is more conservative in predicting factorization, and thus has a higher threshold for when to choose factorization over materialization. Altogether, both models save around $250$s of the total $1,670$s of the validation set. Breaking down the positively predicted cases by compute type, we see that a lot of time (147s \& $136$s respectively, shown in \autoref{fig:5-model-cpu-gpu}) is lost for false positives of CPU scenarios. So, for GPU scenarios, the model is more accurate than for CPU scenarios, which is expected, as we only use memory cost as a model for runtime. We have shown this to be a valid strategy for GPUs, but for CPUs, the runtime is largely dominated by the number of FLOPs executed.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{chapters/05_cost_estimation/figures/analytical-models-compare.pdf}
  \caption[Analytical Model Confusion Matrix]{Confusion matrix of the analytical models' performance on the test set (CPU and GPU on synthetic data). Decision boundaries set at $1.7$ and $10.0$ respectively. Adding time difference of the false positive cases and the true positive cases gives the total time saved by the model. For ANLY.1 this is 256s, for ANLY.2 it is 257s. }
  \label{fig:5-analytical-model-evaluation}
\end{figure}

The evaluation results are presented in \autoref{fig:5-analytical-model-evaluation}. Both models, ANLY.1 and ANLY.2, exhibit comparable performance. Although ANLY.1 predicts fewer false positives, the cumulative time lost due to these false positives is nearly identical to that of ANLY.2. By subtracting the time lost due to false positives from the time saved by true positives, it is observed that ANLY.2 saves more time than ANLY.1. This is likely because ANLY.2 adopts a more conservative approach to predicting factorization, thus having a higher threshold for choosing factorization over materialization.

Both models save approximately $250$ seconds of the total $1,670$ seconds of the validation set. When we break down the positively predicted cases by compute type, we find that a significant amount of time ($147$ seconds and $136$ seconds, respectively, as shown in \autoref{fig:5-model-cpu-gpu}) is lost due to false positives in CPU scenarios. Therefore, these models are more accurate for GPU scenarios than for CPU scenarios, which aligns with our expectation, as we use memory cost as a model for runtime. We have demonstrated this to be a valid strategy for GPUs, but for CPUs, the runtime is predominantly determined by the number of Floating Point Operations Per Second (FLOPs) executed.


\subsection{Linear Regression}
\label{subsec:5-linear-regression}
In this section, we investigate the second type of cost model, the linear regression model. The objective of this model is to maintain explainability while delivering superior performance compared to the manually tuned decision rules found in related work. We employ a range of models, all of which fundamentally rely on linear regression. We start with a single regressor and, by partitioning the dataset according to the categorical variables, we eventually arrive at more intricate models comprising ensembles of linear regression models.

The architecture of each of the linear regression models is depicted in \autoref{fig:5-linear-regression-architecture}. In short, before training, the dataset is partitioned according to several categorical variables (operator type, hardware type, or F/M), or by filtering a portion of the dataset. Each final model comprises a set of linear regression models, each of which is fit to a subset of the training data. For example, for LINR.3, we train two regressors: one for the factorization scenarios and the other for the materialization scenarios. During inference, each regressor is used to predict the runtime of the scenario under test. The output is the difference between the predicted materialized time and the predicted factorized time, which is used to estimate the time saved by opting for factorization over materialization. If a categorical variable is not used to divide the dataset, it is incorporated as a feature of the model.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/statistical-architecture.pdf}
  \caption[Linear Regression model Architecture]{Architecture of the linear regression models. Shows how the data, and models, are split for each model. The final split-level belonging to each respective model is colored in the same color. For LINR.5 we show the linear regression ensemble fit to the data. For clarity, we leave this out for the other models. Each box represents a regression model, the same-colored boxes, connected via dotted lines, are combined into an ensemble to end up with the linear regression cost models.}
  \label{fig:5-linear-regression-architecture}
\end{figure}

\subsubsection*{LINR.1 Linear Regressor Fit to Full Training Set}
The first linear regressor is trained on the complete training set, which includes all operators. The rationale behind this approach is the probable existence of a correlation between the performance of individual operators and the performance of the models in which they are used. By training the regressor on the entire set of operators, we strive to capture these correlations and leverage them to enhance accuracy. This model predicts the time conserved by opting for factorization over materialization.

\subsubsection*{LINR.2 Linear Regressor Fit to Model Runtimes}
The second linear regressor is trained only on the runtimes of the models, and the individual operators are left out of the training set. This model helps determine whether the inclusion of operators contributes to utility. Similarly to LINR.1, this model forecasts the time conserved by opting for factorization over materialization.

\subsubsection*{LINR.3 Separate Regressors for F and M}
This model is a composite of two linear regressors, one for factorization and another for materialization. By training distinct regressors for factorization and materialization, we aim to more explicitly capture the correlations between independent variables and runtime than the preceding models. Each internal regressor predicts the runtime of the scenario under test, and the final prediction is the faster predicted scenario.

\subsubsection*{LINR.4 Separate Regressors for each Model Type}
Similar to LINR.3, this model is also a composite of multiple internal regressors. However, instead of having a single regressor for each factorization and materialization, we have a distinct regressor for each type of model. This is done to capture the differences in the correlations between independent variables and runtime for different model types. Like the first two models, this model forecasts the time saved by choosing factorization over materialization (by predicting whether time is conserved by choosing F).

\subsubsection*{LINR.5 Separate Regressors for CPU and GPU}
In previous sections, we have shown that the choice of hardware plays a significant role in the trade-off we investigate. Therefore, it is likely that there are differences in the correlations between the independent variables and the runtime between CPU and GPU. A singular linear regression model is probably incapable of capturing these differences. Consequently, we test the performance of a set of models, one which is only fit to CPU scenarios and another which is only fit to GPU scenarios.

\subsubsection*{LINR.6 Separate Regressors for F, M and Model Type}
The next version of the linear regression model we create is a combination of LINR.4 and LINR.3. By training separate regressors for every combination of factorization, materialization, and model type, we enable the models to more freely capture differences between groups.

\subsubsection*{LINR.7 Separate Regressors For Each Combination of Categories}
The final, most granular, ensemble is one that has a distinct regressor for each combination of factorization, materialization, model type, and hardware. This is done to capture the differences in the correlations between the independent variables and runtime for each combination of the dimensions.


\subsubsection{Linear Regression Model Evaluation}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.75\linewidth]{chapters/05_cost_estimation/figures/stat-models-compare.pdf}
  \caption[Linear Regression Model Evaluation]{Evaluation of the linear regression models on the validation set (synthetic data, only models). The plots show statistics of those scenarios where the model predicts that factorization is faster. The plot on the left shows $\sum \text{Time}_M - \sum \text{Time}_F$, the total time saved in positively predicted scenarios. The right plots show $\frac{\text{Time}_M}{\text{Time}_F}$ highlighting that conservative models can result in high performance ratios with a low amount of time saved, for example LINR.7.}
  \label{fig:5-linear-regression-model-evaluation}
\end{figure}

A performance comparison for the linear regression models is shown in \autoref{fig:5-linear-regression-model-evaluation}. We choose to highlight the cases where the model predicts factorization to be faster and evaluate the saved time in these cases. Overall, models that fit more distinct regressors perform better, showing that each of the chosen categorical variables impacts the F/M trade-off.

Models LINR.1 through LINR.3 demonstrate little time saved, but the average speedup of positively classified cases is high. This is attributable to these models producing either a substantial number of false negatives, thereby missing out on a significant amount of time saved (LINR.1), or many false positives, which reduces the total time saved (LINR.2 and LINR.3). LINR.4, which is split solely by model type, performs the worst among the models, indicating a correlation between the performance ratios of the different model types. The LINR.5 and LINR.6 models both identify a relatively large fraction of positive samples, albeit at the cost of numerous false positives and negatives. LINR.7, which is the most granular, performs the best on the \textbf{validation} set. This is likely because it can capture the differences in the relationships between the independent variables and the runtime for each combination of the dimensions. This model is the most complex, but also the most accurate, with a utility value of $0.65$, indicating that it saves $65\%$ of the maximum possible time saved.

However, when evaluating the \textbf{test} datasets (which includes real-world datasets, the train-test split is elaborated in \autoref{subsec:6-validation-strategy}), the performance of the models is considerably lower (visualized in \autoref{fig:5-model-cpu-gpu} for LINR.1 and LINR.5). This is probably due to the models overfitting to the training data (which contains only synthetic dataset scenarios).  LINR.1 and LINR.5 show the best utility, which can be explained by the fact that they do not use extensive partitioning of the training data, thereby preventing the regression from fitting to the training data. To address this, we choose to evaluate the performance of a more complex model.

\subsection{XGBoost}
\label{subsec:5-xgboost}
The third type of model we evaluate is XGBoost~\cite{xgboost}, specifically an \texttt{XGBRegressor}\footnote{\url{https://xgboost.readthedocs.io/en/stable/python/python_api.html\#xgboost.XGBRegressor}}. This model is a gradient-boosting algorithm, an ensemble learning method that utilizes a series of decision trees. We employ XGBoost due to its excellent performance demonstrated in related cost prediction scenarios~\cite{tvm}, as well as its excellent ability to handle unbalanced datasets~\cite{xgboost_imbalanced_data}. The model is trained on a dataset identical to the linear regression models and employs the same features.

\begin{table}[ht]
  \centering
  \begin{tabular}{llll}
    \toprule
    Model & Target     & Pruning       & Decision Boundary            \\
    \midrule \midrule
    XGB.1 & Runtime    & All operators & $Time_M > Time_F$            \\
    XGB.2 & Runtime    & Only models   &                              \\
    XGB.3 & Speedup    & All operators & $\texttt{speedup} > 1.0$     \\
    XGB.4 & Speedup    & Only model    &                              \\
    XGB.5 & Time saved & All operators & $\texttt{time\_saved} > 0.0$ \\
    XGB.6 & Time saved & Only models   &                              \\
    \bottomrule
  \end{tabular}
  \caption[XGBoost configurations]{Overview of the different configurations for the XGBoost models.}
  \label{tab:5-xgboost-configurations}
\end{table}

We explore a variety of configurations for this model, varying along two dimensions: their target variable, and whether all operators are included, or just the model operators, in the training dataset. The target variable is either the runtime of the scenario (with separate targets for factorized and materialized), the speedup of a scenario, or the time saved by opting for factorization over materialization. The dataset can be pruned to retain only training samples, where the operator is one of the model types (K-Means, logistic regression, linear regression, or GNMF), or all samples can be retained. By evaluating multiple models with different configurations, we aim to identify the model that best captures the relationships between independent variables and the factorization/materialization (F/M) trade-off. An overview of which model uses which configuration and how the decision to materialize or factorize is made based on the predicted value(s), is presented in \autoref{tab:5-xgboost-configurations}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{chapters/05_cost_estimation/figures/xgb-models-compare.pdf}
  \caption[XGBoost model Comparison]{Evaluation of the XGBoost models on the validation set.}
  \label{fig:5-xgboost-evaluation}
\end{figure}

A comparative evaluation of the XGB models is shown in \autoref{fig:5-xgboost-evaluation}. As expected, these models, being more complex, outperform the linear regression models. Among the XGB models, there is minimal variation in performance on the validation set. There are no significant differences in performance based on the target variable used or whether all operators are included in the training set. XGB.3, which predicts the speedup of factorization over materialization, marginally exceeds the other models, accurately predicting 98\% of the validation scenarios.

\subsection{Quantitative Comparison Using Real-world Data}
\label{subsec:5-comparing-performance}
The performance of the two most effective models for each type is illustrated in \autoref{fig:5-model-cpu-gpu}. We plot the total time saved on the test scenarios, which includes the scenarios on the Hamlet and TPCx-AI datasets. As expected, the most complex XGBoost models perform best. Specifically, XGB.3 and XGB.5 save $600$ and $700$ seconds, respectively, of the total $1670$ seconds. The linear regression model performs the worst, probably due to overfitting to the synthetic data in the training set.

Interestingly, when we dissect the performance by compute type, the XGBoost models perform significantly better on GPU scenarios than on CPU scenarios, whereas LINR.5 performs better on CPU scenarios. This can be attributed to the fact that LINR.5 comprises two separate regressors, one for the CPU and one for the GPU, enabling it to capture the differences in the relationships between independent variables and runtime for each compute type. The process of combining these models to create a more accurate hybrid model is discussed in the following section.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{chapters/05_cost_estimation/figures/compare_gpu_vs_cpu.pdf}
  \caption[Cost Model Comparison Broken Down by Compute Type]{Comparison of the best models, broken down by compute type. Evaluated on the test set.}
  \label{fig:5-model-cpu-gpu}
\end{figure}

\subsubsection{Combining Cost Models}
\label{subsec:5-hybrid}
To construct the final cost model, we combine the best performing models. We employ the XGB.3 model for GPU scenarios and the LINR.5 model for CPU scenarios. This approach is adopted to capture the differences in the relationships between independent variables and runtime for each compute type. The final model is evaluated on the test set, and the results are presented in \autoref{fig:5-model-cpu-gpu}. The model achieves $80\%$ of the performance of an ideal cost model, saving $1344$ seconds of the total $1670$ seconds. This represents a significant improvement over the best individual models, which achieved a maximum of $47\%$ of theoretical performance.

\subsubsection{Qualitative Comparison}
In the Machine Learning (ML) framework proposed in~\cite{amalur}, for which this thesis develops a cost model, the decision between factorization and materialization is made at runtime. Consequently, this should not introduce any overhead to the training process. We briefly touch upon the overhead introduced by the cost of inference of the models, as well as other factors that could influence the choice between cost models. An overview of all aspects is presented in \autoref{tab:5-meta-results}.

\begin{table}[ht]
  \centering
  \begin{tabular}{llllll}
    \toprule
    Model             & Training time & Extensibility & Inference Speed & Performance \\
    \midrule
    Analytical 1      & $-$           & $+$           & $-$             & $-$         \\
    Analytical 2      &               & $+$           & $+$             & $-$         \\
    linear regression & $+$           & $-$           & $+$             & $-$         \\
    XGBoost           & $+/-$         & $-$           & $+/-$           & $+/-$       \\
    Hybrid            & $+/-$         & $-$           & $+/-$           & $+$         \\
    \bottomrule
  \end{tabular}

  \caption{Qualitative comparison of the different models.}
  \label{tab:5-meta-results}
\end{table}


The first aspect we examine is the training time. For linear regression models, this is fast, but it is slower for the XGBoost model. The outliers in this case are the analytical models. ANLY.1 is notably slow as it requires fitting a large set of regressors, one for each combination of operator and F/M. In contrast, ANLY.2 does not have a traditional training time as it employs a handcrafted formula.

In terms of extensibility, the ANLY.1 models are the easiest to extend for new ML models, as they already possess an inherent ``understanding'' of the operators used, provided by the included profiling metrics on LA operators. For the remaining models, an updated training set is required, which includes the new ML models.

Arguably, the most crucial factor, other than performance, is inference speed, as we want to avoid overhead in the training process. ANLY.2 and the linear regression model are very quick as they are simple equations with no more than $30$ terms. XGBoost, and consequently the hybrid model, is slightly slower, but still fast. ANLY.1 is the slowest here, as computing the $OperatorCost$ of all involved operators is costly.

The final aspect is performance, which was discussed in the previous section.

\subsection{Evaluating Feature Importance}
\label{subsec:6-feature-importance}
To determine which characteristics are most influential in the factorization/materialization (F/M) trade-off, we examine the final hybrid model, with a particular focus on the gain per feature from the GPU leg of the model.

We present the gain of the ten most influential features of the XGBoost model in \autoref{fig:5-gpu-feature-importance}. The gain quantifies the impact of a feature based on its contribution to reducing training error in the underlying decision trees. It can be interpreted as a measure of the influence a feature exerts on the model's prediction, which is insightful, as we are interested in identifying which features most significantly impact the F/M trade-off.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.75\linewidth]{chapters/05_cost_estimation/figures/xgboost-feat-importance.pdf}
  \caption[Feature importances of the hybrid model]{Feature importances of the Hybrid model, XGBoost (GPU) leg.}
  \label{fig:5-gpu-feature-importance}
\end{figure}

From the figure, we can deduce that the model type is the most impactful. This aligns with the results shown in \autoref{sec:5-motivation}, where we demonstrate that the choice of model significantly influences the F/M trade-off. Due to the operators used in each model, the speedup varies considerably between models, even when applied to the same dataset. For instance, factorized Gaussian training is beneficial twice as often as factorized K-Means. Other important features include data characteristics such as the number of rows in tables ($r_{S_1}, r_T$), features related to the number of zeros in a table ($e_T, nnz(S)$), or features related to the differences between the materialized and factorized case ($\rho, \frac{FLOP_M}{FLOP_F}$). We observe that although the relationship between complexity and speedup was not as clear in \autoref{sec:5-motivation}, features related to the number of operations are important, even when considering only GPUs. This is evident from the relatively high gain of the materialized and scalar complexity. This is logical, as scalar operations have high speed-ups compared to other operators.

From these gain values we can conclude that, in our experiments, differences between GPUs are less impactful than differences in data characteristics and models. The GPU characteristic with the highest gain is a categorical feature for the GPU architecture, with a gain of $7$, followed by the L1 cache size and the number of Streaming Multiprocessors with gain values of $2.5$ and $1.4$, respectively.


