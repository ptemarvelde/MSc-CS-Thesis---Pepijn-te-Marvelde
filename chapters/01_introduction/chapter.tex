% !TEX root = ../../main.tex

\chapter{Introduction}
\label{chapter:introduction}

The field of Machine Learning (ML) has gained massive traction over the last few decades, with both industry and academia investing substantial efforts to enhance data availability and the performance of ML algorithms. This performance factor encompasses both the accuracy of a model and the computational and time costs associated with training these models.  To make training more efficient, a novel approach called factorized learning has been proposed~\cite{orion_learning_gen_lin_models}. This approach enables models to be trained on normalized data. It is applicable to a wide range of realistic ML workflow scenarios and joinable data sources, thus opening up new possibilities for more efficient model training.

In a typical machine learning scenario, the first step for a data scientist aiming to train a model is to gather the necessary data for the training process. Often, these data come from disparate sources. Therefore, they first need to join these sources to create a single dataset to use as input for an ML model. This is achieved through a process called Data Integration (DI). The act of executing the join between the tables is called materialization~\cite{rel_db_glossary}. Factorized learning eliminates this prerequisite to the training process by learning directly from the source datasets, without first joining them. \autoref{fig:running-example-fac-vs-mat} illustrates the difference between factorized learning and learning over materialized data. The reason factorized learning can be more efficient is that the values in the materialized data (orange cells in $T$ in the figure) do not lead to redundant computations during training. However, source datasets can also have redundant values, and this redundancy is not the only factor that affects the efficiency of factorized learning. Apart from the data-characteristics (which include redundancy), model parameters and hardware characteristics can also influence the choice between factorized learning and materialization.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\linewidth]{chapters/01_introduction/figures/running-example-intro.pdf}
  \caption[Running Example: Source tables $S$ and target table $T$]{\textbf{Running Example: Source tables $S$ and target table $T$}. \\ Illustration of input data used for factorized Learning vs Learning over Materialized data, schema from TPCx-AI~\cite{tpcx-ai} use case 1 (unused columns not shown). Target redundancy, that is avoided by factorization, shown in orange.}
  \label{fig:running-example-fac-vs-mat}
\end{figure}

Therefore, because factorization does not always increase efficiency, there is a need to be able to select those cases where factorization is beneficial. This decision between factorization and materialization is a multidimensional cost optimization problem. Creating a cost model capable of making this choice is an interesting and important problem because factorized learning is a very novel approach to the fundamentals of machine learning. It has potential to reduce the cost of model training without affecting performance. Factorized ML could also be easily extensible to federated learning in a scenario where computations involving a source dataset are executed in the silo in which the dataset is located~\cite{amalur}.

However, solving this problem is challenging because the optimization space is exceptionally large and may be hardware-dependent. Previous solutions, such as Morpheus~\cite{morpheus} and Amalur's cost estimation~\cite{amalur_tkde24}, have focused on theoretical cost or simple heuristics without considering the hardware dimension.

\autoref{fig:ml-pipeline} shows the applicability of the cost model we propose. For an ML practitioner aiming to optimize their training processes with the use of factorized learning, the data preparation and preprocessing steps do not change. Gathering source datasets and defining how to, e.g., join and clean them is still necessary. After finishing preprocessing, formalizing how the datasets should be joined, and having decided which model they want to train, our cost model predicts the optimal training method, factorized, or materialized. Using such a cost model can result in considerable time savings, particularly in scenarios where the total training time is high,  such as hyperparameter tuning or training complex models. This is elaborated on in \autoref{sec:5-motivation}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\linewidth]{chapters/01_introduction/figures/ML-Pipeline.pdf}
  \caption{Function of this thesis' cost model in an ML pipeline.}
  \label{fig:ml-pipeline}
\end{figure}

\section{Running Example}
Throughout this thesis we use a running example, based on TPCx-AI~\cite{tpcx-ai} use case 1, to explain various concepts. The scenario is that of a data scientist working for an e-Commerce firm, who is tasked with creating a data science pipeline to perform customer segmentation. Three tables are joined to create a single table which is then used as input in a K-Means clustering model. The schema of the tables and the joined table are shown in \autoref{fig:running-example-fac-vs-mat}.

The three tables to be joined are the \textbf{Orders} table ($S_1$), the \textbf{Lineitem} table ($S_2$), and the \textbf{Order returns} table ($S_3$). The \textbf{Orders} table contains information about orders made by customers, the schema of this table is $S_1(order\_id$, $customer\_sk$, $\ date)$. The \textbf{Lineitem} table contains information about the items in each order, its schema is $S_2(order\_id$, $product\_id$, $quantity$,  $price)$. The \textbf{Order returns} table, having schema $S_3(order\_id$, $product\_id$, $return\_quantity)$, contains information about the returns made by customers.

The joined table contains all the columns from the three source tables and is used as input for the K-Means clustering model. It is created by first left joining $S2$ with $S1$ on shared columns $order\_id$ and $product\_id$, then joining the result of this with $S1$ on $order\_id$. After this we have $T(order\_id$, $customer\_sk$, $date$, $product\_id$, $quantity$, $price$, $return\_quantity)$.

\section{Research Questions \& Contributions}
This thesis focuses on facilitating the adoption of factorized machine learning. A significant part of this work involves developing a cost model designed to accurately decide the optimal approach for training a machine learning model. The cost model, considering the data, model, and hardware dimensions, decides between factorized or materialized computation.

\subsection{Research Questions}
The research questions answered in this thesis are:
\begin{enumerate}[leftmargin=1.5cm, label=\emph{RQ.\arabic*}]
  \item How can we optimize and implement factorized Machine Learning for GPUs?
  \item How can we accurately predict the optimal choice between factorized or materialized training of a Machine Learning model, on CPU and GPU, through leveraging knowledge about model, data, and hardware characteristics?
\end{enumerate}

\subsection{Contributions}
The Research Questions are answered by providing the following contributions:
\begin{enumerate}[leftmargin=1.5cm, label=\emph{C.\arabic*}]
  \item A GPU optimized implementation of Amalur's factorized Machine Learning framework.
  \item A cost model that predicts whether factorized or materialized learning is faster, capable of accurate predictions regardless of dataset, model hyperparameters, or hardware used. This cost model is the result of a detailed study that compares multiple cost calculation strategies.
\end{enumerate}

\section{Cost Estimation for Factorized Machine Learning}
To develop a cost model capable of accurately predicting whether factorized or materialized learning is faster, we capture the runtime of training types for a set of scenarios. We vary the data used, the models trained, and the hardware settings. These collected results are then used as training data to create several types of cost models.

Evaluation on real-world datasets shows that our models outperform the state-of-the-art (SOTA), specifically our \textbf{Hybrid} model, which combines a linear regression and an XGBoost model, performs well. It outclasses the SOTA in scenarios with real-world datasets, reaching \textbf{80\%} of the maximum achievable time saved. This cost model also generalizes well to scenarios with novel hardware settings and datasets, only losing \textbf{18\%} of its predictive power.

\section{Outline}
This section provides an overview of the structure of the remainder of this thesis. We start with the theoretical concepts and principles that underpin our study in \autoref{chapter:preliminary}. The literature review (\autoref{chapter:literature}) reviews existing research relevant to our topic and identifies gaps or limitations in the existing literature. In the methodology chapter (\autoref{chapter:methodology}), we describe our overall approach to this empirical study, including the breakdown and motivation of the independent variables chosen.  In \autoref{chapter:cost-estimation}, we detail the statistical and analytical methods used to analyze the data, present the results of each experiment, and include visualizations to illustrate key findings. Next, in \autoref{chapter:evaluation-discussion}, the results are evaluated and discussed. It starts with an explanation of how the empirical results were gathered, followed by a discussion of the results of the experiments in relation to the research questions. Here, we also evaluate the validity and reliability of the results and compare our findings with those of the existing literature. This chapter ends with an in-depth interpretation and discussion of the results and their implications.  Finally, in the conclusion chapter (\autoref{chapter:conclusion}), the main contributions and findings of this thesis are summarized, and we provide an outlook for future research.

