%!TEX root = ../../main.tex

\chapter{Introduction}
\label{chapter:introduction}

% General intro
Training a Machine Learning (ML) model can be costly, both in time and in computational resources. This is the reason a lot of effort is spent ensuring models are trained in the most optimal way. A novel approach called factorized learning \cite{orion_learning_gen_lin_models}, has been proposed to allow training models on normalized data, opening new possibilities for more efficient model training. It is applicable to a large set of data realistic ML workflow scenarios and joinable data sources.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{chapters/01_introduction/figures/running-example-intro.pdf}
    \caption{Illustration of input data used for Factorized Learning vs. Learning over Materialized data, schema from TPCx-AI \cite{tpcx_ai} Use Case 1 (unused columns not shown). Target redundancy avoided by factorization shown in orange.}
    \label{fig:running-example-fac-vs-mat}
\end{figure}

When a data scientist wants to train an ML model, they first need to join disparate sources to create a single dataset (Materialization \cite{rel_db_glossary}) to use as input for an ML model. Factorized Learning eliminates this step in the training process by learning directly from the source datasets, without first joining them. \autoref{fig:running-example-fac-vs-mat} illustrates the difference between factorized learning and learning over materialized data. The reason that factorized learning can be more efficient is that values in the materialized data (orange cells in $T$ in the figure) do not lead to redundant computations during training. However, the source datasets can also have redundant values, and this redundancy is not the only factor that affects the efficiency of factorized learning. Apart from the data-characteristics (which include redundancy), model parameters and hardware characteristics can also influence the choice between factorized learning and materialization.

Deciding between factorization and materialization is a multi-dimensional cost optimization problem. This is an interesting and important problem because factorized learning is a very novel approach to the fundamentals of machine learning. It has the potential to reduce the cost of model training without affecting performance. It could also be easily extensible to federated learning in a scenario where computation involving a source dataset is executed in the silo that dataset resides in.

However, solving this problem is challenging because the optimization space is exceptionally large and may be hardware dependent. Previous solutions, such as Morpheus \cite{morpheus} and Amalur's cost estimation \cite{schijndel_cost_estimation}, have focused on theoretical cost or simple heuristics without considering the hardware dimension.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{chapters/01_introduction/figures/ML-Pipeline.pdf}
    \caption{Function of this thesis' cost estimator in an ML pipeline.}
    \label{fig:ml-pipeline}
\end{figure}

\autoref{fig:ml-pipeline} shows the applicability of the cost estimator we propose. For an ML practitioner aiming to optimize their training processes with the use of factorized learning, the data preparation and preprocessing steps do not change. They will still need to gather source datasets and define how to e.g., join and clean them. After they have finished preprocessing, formalized how the datasets should be joined, and decided what model they want to train, the cost estimator predicts the optimal training method. Utilizing such a cost estimator can result in considerable time savings in intensive training scenarios, such as hyper-parameter tuning or training complex models.


\section{Research Questions \& Contributions}
This thesis aims to aid in the adoption of factorized machine learning, by creating a model that can accurately decide whether it is optimal to train a machine learning model on normalized or joined data. To achieve this the data, model and hardware dimensions will be considered. 

\subsection{Research Questions}
The research questions answered in this thesis are:
\begin{itemize}
    \item[RQ.1] How can we optimize and implement Factorized Machine Learning for GPUs?
    \item[RQ.2] How can we accurately predict the optimal choice between factorized or materialized training, on CPU and GPU, through leveraging knowledge about model, data, and hardware characteristics?
\end{itemize}

\subsection{Contributions}
\begin{enumerate}
    \item[C.1] GPU optimized implementation of Amalur's Factorized Machine Learning framework.
    \item[C.2] A cost model that predicts whether factorized or materialized learning is faster, capable of accurate predictions regardless of dataset, model hyper-parameters, or hardware used. This cost model is the result of a detailed study comparing multiple cost calculation strategies.
\end{enumerate}

\section{Running Example}
\todo{formalization of the running example 
    \begin{itemize}
        \item Usecase
        \item schema
        \item more details
    \end{itemize}
}

\section{Cost Estimation for Factorized Machine Learning}
To develop a cost estimator that can accurately predict whether Factorized or materialized learning is faster for a given ML task we conduct experiments on synthetic data allowing for full control of the relevant data, model and hardware factors. These empirical where then used results to train several models. We compare these models with each other, as well as with multiple baselines from related works. This was done not only to show which models performs best, but to also create thorough understanding on why these models perform the way they do.

Evaluation on real-world datasets show that our models outperform the state-of-the-art, specifically the \textbf{???} model performs well. It performs \textbf{???} as well as state-of-the-art Amalur \cite{schijndel_cost_estimation} on Hamlet datasets \cite{2016hamletsigmod}. It also performed well on the TPC-AI use cases, saving \textbf{???}\% of training time over a system without factorized ML and a cost estimator to decide whether to do materialized or factorized computation. The goal of creating a generalizable model, one that still has accurate prediction capabilities even if the scenario under evaluation is not similar to one the model was trained on, was also achieved. The final estimator only loses \textbf{???} of its predictive power on use cases with hardware, and data characteristics not in its training set.
% Something more about why this model performs the best


\section{Outline}
This section provides an overview of the structure for the rest of this thesis. We start with the theoretical concepts and principles that underpin our study in \autoref{chapter:preliminary}. The literature review (\autoref{chapter:literature}) surveys existing research relevant to our topic and identifies gaps or limitations in the existing literature. In the \hyperref[chapter:methodology]{methodology chapter}, we describe our overall approach to this empirical study, including the breakdown and motivation for the chosen independent variables. The experiment setup in \autoref{chapter:experiment-setup} provides a detailed description of the experimental environment, as well as the necessary information to replicate the results shown after. In the next chapter on \hyperref[chapter:cost-estimations]{cost estimation} we detail the statistical and analytical methods used to analyze the data, present the results of each experiment, and include visualizations to illustrate key findings. The \hyperref[chapter:evaluation-discussion]{evaluation \& discussion chapter} discusses the outcomes of the experiments in relation to the research questions, evaluates the validity and reliability of the results, compares our findings with existing literature, and provides an in-depth interpretation of the results. We also discuss the practical implications of our findings and acknowledge any limitations of our study. Finally, in the conclusion chapter (\autoref{chapter:conclusion}), the main main contributions and findings of this thesis are summarized, and we provide an outlook for future research.

