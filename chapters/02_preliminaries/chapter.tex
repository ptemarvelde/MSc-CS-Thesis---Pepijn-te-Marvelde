%! TEX root = ../../main.tex

\chapter{Factorized Machine Learning: Theoretical Concepts}
\label{chapter:preliminary}

This chapter details the necessary theoretical concepts for this thesis. First we explain \hyperref[sec:2-data-integration]{Data Integration}: the process of combining data from different sources, which is crucial to any ML workflow. With these concepts in mind we explore \hyperref[sec:2-factorized-ml]{Factorized Machine Learning} in detail. Last, in \autoref{sec:2-ml-on-gpu}, we explain GPUs and how they are crucial to the ML industry. With these concepts we provide the theoretical foundation necessary for understanding the content presented in the next chapters of this thesis.


\section{Data Integration}
\label{sec:2-data-integration}
To grasp the value and intricacies of Factorized ML one first needs to understand the field of Data Integration (DI). In the broadest definition, DI details relations between data(sets), allowing data from disparate sources to be consolidated into a single dataset. This is essential to ML use cases as ML libraries (e.g., Keras\footnote{\url{https://keras.io/}}, TensorFlow\footnote{\url{https://www.tensorflow.org/}}) expect a single table as input. An example of such a DI scenario is shown in \autoref{fig:running-example-fac-vs-mat}.

% Paragraph on formalization?

However, while joining datasets to a single table is necessary for ML it can introduce major overheads \cite{data-management-in-ML-kumar-2017}:

\begin{enumerate}
    \item \textbf{Extra storage}\\ The joined dataset will take extra space to store.
    \item \textbf{Computational redundancy} \\ Joining tables can introduce duplication of values in the materialized data (shown in orange in \autoref{fig:running-example-fac-vs-mat}). These values are included in any computations done during the training of an ML model in this dataset, resulting in duplicate computations.
    \item \textbf{Join time} \\For complex scenarios, joining datasets can take a significant amount of time.
    \item \textbf{Maintenance headaches} \\Join query needs updating when changing input table schemas.
\end{enumerate}

Factorized Machine Learning aims to alleviate these problems by “learning over joins” \cite{orion_learning_gen_lin_models}, that is, pushing the computations needed for an ML model down to the separate tables.

\subsection{Schema Mappings}
\todo{Explain TGDs and how they are used in DI}

\section{Factorized Machine Learning}
\label{sec:2-factorized-ml}
As stated previously in this thesis, Factorized ML is the process of training Machine Learning models on multiple tables without the need to materialize the join between these tables. This section will go in-depth on how this can be achieved, continuing the running example from \autoref{fig:running-example-fac-vs-mat}.  We start with the definitions (\autoref{subsec:2-normalized-matrix}) followed by an in-depth example of the involved linear algebra (\autoref{subsubsec:2-fac-ml-example}).

\subsection{Normalized Matrix}
\label{subsec:2-normalized-matrix}

As Machine Learning Algorithms can be expressed in Linear Algebra (LA) we need to express the Data Integration scenario of an ML use case in terms of Linear Algebra, .i.e., we need to translate the Schema Mappings of an integration scenario to Linear Algebra to allow us to achieve the goal of “pushing down” ML to the separate source tables. This is accomplished through the \textbf{Normalized matrix}: A set of matrices that capture the necessary DI metadata telling us how source tables map to the materialized Target table \cite{amalur, morpheus}.

The \textbf{Mapping matrix} and \textbf{Indicator matrix} respectively represent how the columns and rows from each source table $S_k$ map to the Target table $T$.

\subsubsection{Mapping Matrix}
The Mapping Matrix $M$ is a set of matrices $M_k$ for each source table $S_k$ that denotes how source columns map to target columns. A value of 1 in this matrix denotes that the corresponding (via row number) column in $S_k$ maps to the corresponding (via column number) column in $T$. The formal definition is:

\begin{definition}[\textit{Mapping matrix} \cite{amalur}]
    Each source table $S_k$ has a corresponding binary Mapping matrix $M$ of shape $c_T \times c_{S_k}$, where
    \begin{align*}
        M_k[i,j] = \begin{cases}
                       1, & \text{if $j$-th column of $S_k$ is mapped to the $i$-th column of $T$} \\
                       0, & \text{otherwise}
                   \end{cases}
    \end{align*}
\end{definition}


\subsubsection{Indicator Matrix}
% TODO intro
Now that we have defined how to map the columns from source tables to target table, we need to do the same for the rows. This is done with the \textbf{Indicator} matrix.

\begin{definition}[\textit{Indicator matrix} \cite{morpheus}]
    Each source table $S_k$ has a corresponding binary Indicator matrix $I$ of shape $r_T \times r_{S_k}$, where
    \begin{align*}
        I_k[i,j] = \begin{cases}
                       1, & \text{if $i$-th row of $S_k$ is mapped to the $j$-th row of $T$} \\
                       0, & \text{otherwise}
                   \end{cases}
    \end{align*}
\end{definition}

% \subsubsection{Redundancy Matrix}


\subsubsection{Materialization}
Using the normalized matrix we can now \textbf{materialize} the join to get the Target matrix $T$:

\begin{definition}[\textit{Materializing the Normalized Matrix to obtain Target matrix $T$}]
    \begin{itemize}
        \item[]
        \item[] Given
        \item[$k$] Table id $k \in [0,n)$
        \item[$S_k$] Source tables
        \item[$M_k$] Mapping matrices
        \item[$I_k$] Indicator matrices
    \end{itemize}
    \[
        T = \sum_{k=0}^{n-1}  I_k S_k M^T_k
    \]
\end{definition}

Intuitively this process can be seen as:
\begin{algorithmic}
    \ForEach {$k \in [0,n)$} \Comment{For each source table}
    \State $rows_k \gets I_k S_k$ \Comment{Map the source table rows to the target table}
    \State $T_k \gets rows_k M^T_k$ \Comment{Map the source table columns to the target table}
    \EndFor
    \State $T \gets \sum_{k=0}^{n-1} T_k$ \Comment{Sum the results}
\end{algorithmic}

\subsubsection{Running Example: Normalized Matrix}
\label{subsubsec:2-fac-ml-example}
To make the translation to how the normalized matrix is used in ML algorithms we first show the full normalized matrix of the running example, followed by the materialized Target table $T$. The goal is to show how the matrices interact to allow computation with all information without necessarily materializing the join.

\vspace{-1cm}
\begin{alignat*}{6}
    \intertext{
        These are the corresponding Source matrices $S_{0..2}$ for the source tables shown in \autoref{fig:running-example-fac-vs-mat}. The \textcolor{BurntOrange}{orange} numbers over the columns denote in which column of $T$ they will end up. The \textcolor{RoyalBlue}{blue} numbers at the end of each row illustrate to which target table rows they are mapped.
    }
                                                            & S_0 &     & =
    \begin{bNiceMatrix}[first-row,last-col]
        0 & 1  & 2    &     \\
        1 & 11 & 2024 & 0,1 \\
        2 & 12 & 2024 & 2   \\
        3 & 11 & 2024 & 3   \\
    \end{bNiceMatrix}   \quad                 &     & S_1 &   & =
    \begin{bNiceMatrix}[first-row,last-col]
        3 & 4  & 5  &   \\
        2 & 20 & 40 & 0 \\
        1 & 25 & 25 & 1 \\
        3 & 13 & 39 & 2 \\
        1 & 10 & 10 & 3 \\
    \end{bNiceMatrix}           \quad                 &     & S_2 &   & =
    \begin{bNiceMatrix}[first-row,last-col]
        6 &   \\
        1 & 3 \\
    \end{bNiceMatrix}                                  \\
    \intertext{
        The Indicator matrices denote how rows from $S$ map to rows in $T$. The column number denotes the row in $S$, the row number denotes the row in $T$. The \textcolor{RoyalBlue}{blue} annotations show more clearly how this works in the form \textcolor{RoyalBlue}{row number in $S_k \rightarrow$ row number in $T$}.
    }
                                                            & I_0 &     & =
    \begin{bNiceMatrix}[last-col]
        1 & 0 & 0 & 0 \rightarrow 0 \\
        1 & 0 & 0 & 0 \rightarrow 1 \\
        0 & 1 & 0 & 1 \rightarrow 2 \\
        0 & 0 & 1 & 2 \rightarrow 3 \\
    \end{bNiceMatrix}   \quad                          &     & I_1 &   & =
    \begin{bNiceMatrix}[last-col]
        1 & 0 & 0 & 0 & 0 \rightarrow 0 \\
        0 & 1 & 0 & 0 & 1 \rightarrow 1 \\
        0 & 0 & 1 & 0 & 2 \rightarrow 2 \\
        0 & 0 & 0 & 1 & 3 \rightarrow 3 \\
    \end{bNiceMatrix}     \quad                      &     & I_2 &   & =
    \begin{bNiceMatrix}[last-col]
        0 & \rightarrow 0   \\
        0 & \rightarrow 1   \\
        0 & \rightarrow 2   \\
        1 & 0 \rightarrow 3 \\
    \end{bNiceMatrix}                                            \\
    \intertext{
        The Mapping matrices denote how columns from $S$ map to columns in $T$. The row number denotes the column in $T$, the column number denotes the column in $S$. The \textcolor{BurntOrange}{orange} annotations show this in the form: \textcolor{BurntOrange}{column number in $S_k \rightarrow$ column number in $T$}.
    }
                                                            & M_0 &     & =
    \begin{bNiceMatrix}[last-col]
        1 & 0 & 0 & \textcolor{BurntOrange}{0 \rightarrow 0} \\
        0 & 1 & 0 & \textcolor{BurntOrange}{1 \rightarrow 1} \\
        0 & 0 & 1 & \textcolor{BurntOrange}{2 \rightarrow 2} \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 3}   \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 4}   \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 5}   \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 6}   \\
    \end{bNiceMatrix}   \quad &     & M_1 &   & =
    \begin{bNiceMatrix}[last-col]
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 0}   \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 1}   \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 2}   \\
        1 & 0 & 0 & \textcolor{BurntOrange}{0 \rightarrow 3} \\
        0 & 1 & 0 & \textcolor{BurntOrange}{1 \rightarrow 4} \\
        0 & 0 & 1 & \textcolor{BurntOrange}{2 \rightarrow 5} \\
        0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 6}   \\
    \end{bNiceMatrix} \quad &     & M_2 &   & =
    \begin{bNiceMatrix}[last-col]
        0 & \textcolor{BurntOrange}{\rightarrow 0}   \\
        0 & \textcolor{BurntOrange}{\rightarrow 1}   \\
        0 & \textcolor{BurntOrange}{\rightarrow 2}   \\
        0 & \textcolor{BurntOrange}{\rightarrow 3}   \\
        0 & \textcolor{BurntOrange}{\rightarrow 4}   \\
        0 & \textcolor{BurntOrange}{\rightarrow 5}   \\
        1 & \textcolor{BurntOrange}{0 \rightarrow 6} \\
    \end{bNiceMatrix}
\end{alignat*}
\begin{gather*}
    \begin{alignat*}{4}
        \intertext{For conciseness we show the calculation of one of the sub-target tables $T_0$.}
                            & T_0 &       & = I_0                 &  & S_0 &  & M_0^T                   \\
                            & T_0 &       & = \begin{bNiceMatrix}
                                                  1 & 0 & 0 \\
                                                  1 & 0 & 0 \\
                                                  0 & 1 & 0 \\
                                                  0 & 0 & 1 \\
                                              \end{bNiceMatrix} &  &
        \begin{bNiceMatrix}
            1 & 11 & 2024 \\
            2 & 12 & 2024 \\
            3 & 11 & 2024 \\
        \end{bNiceMatrix} &     & M_0^T                                                                 \\
                            & T_0 &       & = \begin{bNiceMatrix}
                                                  1 & 11 & 2024 \\
                                                  1 & 11 & 2024 \\
                                                  2 & 12 & 2024 \\
                                                  3 & 11 & 2023 \\
                                              \end{bNiceMatrix} &  &     &  & \begin{bNiceMatrix}
                                                                                  1 & 0 & 0 & 0 & 0 & 0 & 0 \\
                                                                                  0 & 1 & 0 & 0 & 0 & 0 & 0 \\
                                                                                  0 & 0 & 1 & 0 & 0 & 0 & 0 \\
                                                                              \end{bNiceMatrix} \\
    \end{alignat*}\\
    \hspace{-4cm}
    T_0  = \begin{bNiceMatrix}
        \Block[fill=red!15,rounded-corners]{4-3}{}
        1 & 11 & 2024 & 0 & 0 & 0 & 0 \\
        1 & 11 & 2024 & 0 & 0 & 0 & 0 \\
        2 & 12 & 2024 & 0 & 0 & 0 & 0 \\
        3 & 11 & 2023 & 0 & 0 & 0 & 0 \\
    \end{bNiceMatrix}
\end{gather*}

\begingroup
\setlength{\arraycolsep}{4.5pt}
\begin{alignat*}{2}
    \intertext{
        The materialized Target table $T$ is the element wise sum of the dot product of each tuple of Indicator, Source, and Mapping matrices. For each source table $S_k$ the intermittent result is shown as $T_k$. For clarity the cells from each source table are colored in the same color in the intermittent result and in Target table $T$.
    }
     & T_0= I_0 S_0 M_0^T &  & = \begin{bNiceMatrix}[first-row]
                                     0 & 1  & 2    & 3 & \cdots & 6 \\
                                     \Block[fill=red!15,rounded-corners]{4-3}{}
                                     1 & 11 & 2024 & 0 & \cdots & 0 \\
                                     1 & 11 & 2024 & 0 & \cdots & 0 \\
                                     2 & 12 & 2024 & 0 & \cdots & 0 \\
                                     3 & 11 & 2023 & 0 & \cdots & 0 \\
                                 \end{bNiceMatrix}
    T_1= I_1 S_1 M_1^T= \begin{bNiceMatrix}[first-row]
                            0 & 1 & 2 & 3                                             & 4  & 5  & 6 \\
                            0 & 0 & 0 & \Block[fill=blue!15,rounded-corners]{4-3}{} 2 & 20 & 40 & 0 \\
                            0 & 0 & 0 & 1                                             & 25 & 25 & 0 \\
                            0 & 0 & 0 & 3                                             & 13 & 39 & 0 \\
                            0 & 0 & 0 & 1                                             & 10 & 10 & 0 \\
                        \end{bNiceMatrix}
    \\
     & T_2= I_2 S_2 M_2^T &  & = \begin{bNiceMatrix}[first-row]
                                     0 & \cdots & 5 & 6                                              \\
                                     0 & \cdots & 0 & 0                                              \\
                                     0 & \cdots & 0 & 0                                              \\
                                     0 & \cdots & 0 & 0                                              \\
                                     0 & \cdots & 0 & \Block[fill=orange!15,rounded-corners]{1-1}{}1 \\
                                 \end{bNiceMatrix}
    T  = \sum_{k=0}^{2} I_k S_k M^T_k =  \begin{bNiceMatrix}[first-row,last-col]
                                             0 & 1  & 2    & 3                                           & 4  & 5  & 6                                                  \\
                                             \Block[fill=red!15,rounded-corners]{4-3}{}
                                             1 & 11 & 2024 & \Block[fill=blue!15,rounded-corners]{4-3}{}
                                             2 & 20 & 40   & 0                                           & 0                                                            \\
                                             1 & 11 & 2024 & 1                                           & 25 & 25 & 0                                              & 1 \\
                                             2 & 12 & 2024 & 3                                           & 13 & 39 & 0                                              & 2 \\
                                             3 & 11 & 2024 & 1                                           & 10 & 10 & \Block[fill=orange!15,rounded-corners]{1-1}{}1 & 3 \\
                                         \end{bNiceMatrix}
\end{alignat*}
\endgroup


\subsection{Factorized Linear Algebra}
In the previous section we have shown the properties of the Normalized matrix. This section will show how commonly used Linear Algebra operators are rewritten \cite{morpheus} for the Normalized matrix for the purpose of performing factorized ML.


\subsubsection{Element-wise Scalar Operations}

\subsubsection{Aggregation}

\subsubsection{Multiplication}

\subsubsection{Running Example: Right Matrix Multiplication}

\begingroup
\setlength{\arraycolsep}{3.0pt}
\begin{alignat*}{1}
    \intertext{We right matrix multiplication (RMM) by multiplying with $X$. First for the materialized Target table $T$:}
    X T & = \begin{bNiceMatrix}
                1 & 1 & 2 & 3 \\
            \end{bNiceMatrix}
    \begin{bNiceMatrix}
        1 & 11 & 2024 & 2 & 20 & 40 & 0 \\
        1 & 11 & 2024 & 1 & 25 & 25 & 0 \\
        2 & 12 & 2024 & 3 & 13 & 39 & 0 \\
        3 & 11 & 2023 & 1 & 10 & 10 & 1 \\
    \end{bNiceMatrix}                                                         \\
        & =\begin{bNiceMatrix}
               15 & 79 & 14165 & 12 & 101 & 173 & 3 \\
           \end{bNiceMatrix}                                             \\
    \intertext{Now for the Normalized matrix, recall the rewrite rule for RMM:}
    X T & = \sum_{k=0}^{n-1} X I_k S_k M^T_k
    \intertext{For conciseness we refer back to sub results $T_{0\cdots2}$ and use them directly here. We also leave out $X$ in the sub calculations for $T_{1,2}$.}
        & = X T_0 + X T_1 + X T_2                                                           \\
        & = \begin{bNiceMatrix}
                1 & 1 & 2 & 3 \\
            \end{bNiceMatrix} \begin{bNiceMatrix}
                                  1 & 11 & 2024 & 0 & \cdots & 0 \\
                                  1 & 11 & 2024 & 0 & \cdots & 0 \\
                                  2 & 12 & 2024 & 0 & \cdots & 0 \\
                                  3 & 11 & 2023 & 0 & \cdots & 0 \\
                              \end{bNiceMatrix} + X \begin{bNiceMatrix}
                                                        0 & 0 & 0 & 2 & 20 & 40 & 0 \\
                                                        0 & 0 & 0 & 1 & 25 & 25 & 0 \\
                                                        0 & 0 & 0 & 3 & 13 & 39 & 0 \\
                                                        0 & 0 & 0 & 1 & 10 & 10 & 0 \\
                                                    \end{bNiceMatrix} + X\begin{bNiceMatrix}
                                                                             0 & \cdots & 0 & 0 \\
                                                                             0 & \cdots & 0 & 0 \\
                                                                             0 & \cdots & 0 & 0 \\
                                                                             0 & \cdots & 0 & 1 \\
                                                                         \end{bNiceMatrix} \\
        & =\begin{bNiceMatrix}
               15 & 79 & 14165 & 0 & 0 & 0 & 0 \\
           \end{bNiceMatrix} +
    \begin{bNiceMatrix}
        0 & 0 & 0 & 12 & 101 & 173 & 0 \\
    \end{bNiceMatrix}+
    \begin{bNiceMatrix}
        0 & 0 & 0 & 0 & 0 & 0 & 3 \\
    \end{bNiceMatrix}                                                               \\& =\begin{bNiceMatrix}
        15 & 79 & 14165 & 12 & 101 & 173 & 3 \\
    \end{bNiceMatrix}
\end{alignat*}
\endgroup

\section{Machine Learning on GPUs}
\label{sec:2-ml-on-gpu}
\todo{Introduce the relevant parts of GPUs \& Linear Algebra}