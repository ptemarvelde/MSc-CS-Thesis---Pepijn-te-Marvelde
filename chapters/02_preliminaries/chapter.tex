%!TEX root = ../../main.tex

\chapter{Factorized Machine Learning: Theoretical Concepts}
\label{chapter:preliminary}

This chapter details the necessary theoretical concepts for this thesis. First we explain \hyperref[sec:2-data-integration]{Data Integration}: the process of combining data from different sources, which is crucial to any ML workflow. With these concepts in mind we explore \hyperref[sec:2-factorized-ml]{Factorized Machine Learning} in detail. Data Integration and Factorized Machine learning are the theoretical foundation necessary for understanding the content presented in the next chapters of this thesis.


\section{Data Integration}
\label{sec:2-data-integration}
To grasp the value and intricacies of Factorized ML one first needs to understand the field of Data Integration (DI). In the broadest definition, DI details relations between data(sets), allowing data from disparate sources to be consolidated into a single dataset. This is essential to ML use cases as ML libraries (e.g., Keras\footnote{\url{https://keras.io/}}, TensorFlow\footnote{\url{https://www.tensorflow.org/}}) expect a single table as input. An example of such a DI scenario is shown in \autoref{fig:running-example-fac-vs-mat}.

% Paragraph on formalization?

However, while joining datasets to a single table is necessary for ML it can introduce major overheads \cite{data-management-in-ML-kumar-2017}:

\begin{enumerate}
    \item \textbf{Extra storage}\\ The joined dataset will take extra space to store.
    \item \textbf{Computational redundancy} \\ Joining tables can introduce duplication of values in the materialized data (shown in orange in \autoref{fig:running-example-fac-vs-mat}). These values are included in any computations done during the training of an ML model in this dataset, resulting in duplicate computations.
    \item \textbf{Join time} \\For complex scenarios, joining datasets can take a significant amount of time.
    \item \textbf{Maintenance headaches} \\Join query needs updating when changing input table schemas.
\end{enumerate}

Factorized Machine Learning aims to alleviate these problems by "learning over joins" \cite{orion_learning_gen_lin_models}, that is, pushing the computations needed for an ML model down to the separate tables.

\subsection{Schema Mappings}


\section{Factorized Machine Learning}
\label{sec:2-factorized-ml}
As stated previously in this thesis Factorized ML is the process of training Machine Learning models on multiple tables without the need to materialize the join between these tables. This section will go in-depth on how this can be achieved, continuing the running example from \autoref{fig:running-example-fac-vs-mat}.  We start with the definitions (\autoref{subsec:2-normalized-matrix}) followed by an in-depth example of the involved linear algebra (\autoref{subsec:2-fac-ml-example}).

\subsection{Normalized Matrix}
\label{subsec:2-normalized-matrix}

As Machine Learning Algorithms can be expressed in Linear Algebra (LA) we need to express the Data Integration scenario of an ML use case in terms of Linear Algebra, .i.e., we need to translate the Schema Mappings of an integration scenario to Linear Algebra to allow us to achieve the goal of "pushing down" ML to the separate source tables. This is accomplished through the \textbf{Normalized matrix}: A set of matrices that capture the necessary DI metadata telling us how source tables map to the materialized Target table \cite{amalur, morpheus}.

The \textbf{Mapping matrix} and \textbf{Indicator matrix} respectively represent how the columns and rows from each source table $S_k$ map to the Target table $T$.

\subsubsection{Mapping Matrix}
The Mapping Matrix $M$ is a set of matrices $M_k$ for each source table $S_k$, where a value of 1 in this matrix denotes that the corresponding (via row number) column in $S_k$ maps to the corresponding (via column number) column in $T$. The formal definition is:

\begin{definition}[\textit{Mapping matrix}] 
Each source table $S_k$ has a corresponding binary Mapping matrix $M$ of shape $c_T \times c_{S_k}$, where
\begin{align*}
    M_k[i,j] = \begin{cases}
    1, & \text{if $j$-th column of $S_k$ is mapped to the $i$-th column of $T$}\\
    0, & \text{otherwise}
\end{cases}
\end{align*}
\end{definition}


\subsubsection{Indicator Matrix}
% TODO intro
Now that we have defined how to map the columns from source tables to target table, we need to do the same for the rows. This is done with the \textbf{Indicator} matrix.

\begin{definition}[\textit{Indicator matrix}]
Each source table $S_k$ has a corresponding binary Indicator matrix $I$ of shape shape $r_T \times r_{S_k}$, where
\begin{align*}
    I_k[i,j] = \begin{cases}
    1, & \text{if $i$-th row of $S_k$ is mapped to the $j$-th row of $T$}\\
    0, & \text{otherwise}
\end{cases}
\end{align*}
\end{definition}

% \subsubsection{Redundancy Matrix}


\subsubsection{Materialization}
Using the normalized matrix we can now \textbf{materialize} the join to get the Target matrix $T$:

\begin{definition}[\textit{Materializing the Normalized Matrix to obtain Target matrix $T$}]
\begin{itemize}
    \item[]
    \item[] Given
    \item[$k$] Table id $k \in [1,n]$
    \item[$S_k$] Source tables
    \item[$M_k$] Mapping matrices
    \item[$I_k$] Indicator matrices
\end{itemize}
\[
T = \sum_{k=1}^{n}  I_k \cdot S_k \cdot M^T_k
\]

\end{definition}

\todo{transition to how these definitions are used in ML applications}

\subsection{Factorized Linear Algebra}
\todo{LA operators}

\subsection{Factorized Machine Learning Example: Linear Regression}
\label{subsec:2-fac-ml-example}
\todo{
\begin{itemize}
    \item Go through full example with Linear Regression
\end{itemize}
}