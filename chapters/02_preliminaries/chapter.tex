%! TEX root = ../../main.tex

\chapter{Preliminaries: Factorized Machine Learning}
\label{chapter:preliminary}

This chapter details the preliminary theoretical concepts for this thesis. First, we explain Data Integration: the process of combining data from different sources, which is crucial for any ML workflow, in \autoref{sec:2-data-integration}. With these concepts in mind, we explore Factorized Machine Learning in detail (\autoref{sec:2-factorized-ml}). Finally, in \autoref{sec:2-ml-on-gpu}, we explain GPUs and how they are crucial for the ML industry. With these concepts, we provide the theoretical foundation necessary for understanding the content presented in the next chapters of this thesis.


\section{Data Integration}
\label{sec:2-data-integration}
In order to comprehend the significance and complexities of Factorized Machine Learning, it is necessary to have a grasp of the field of Data Integration (DI). In its broadest sense, DI details the relationships between datasets, enabling the merging of data from diverse sources into a unified dataset. This process is crucial for ML applications, as ML frameworks (such as Keras\footnote{\url{https://keras.io/}} and TensorFlow\footnote{\url{https://www.tensorflow.org/}}) typically require a single table as input. An example of such a DI scenario is illustrated in \autoref{fig:running-example-fac-vs-mat}.

However, when merging data sets into a unified table is essential for machine learning, it may present the following significant challenges \cite{data-management-in-ML-kumar-2017}.

\begin{enumerate}
  \item \textbf{Extra storage}\\ The joined dataset will take extra space to store.
  \item \textbf{Computational redundancy} \\ Joining tables can introduce duplication of values in the materialized data (shown in orange in \autoref{fig:running-example-fac-vs-mat}). These values are included in any computations made during the training of an ML model in this dataset, resulting in duplicate computations.
  \item \textbf{Join time} \\For complex scenarios, joining datasets can take a significant amount of time.
  \item \textbf{Maintenance headaches} \\Join query needs updating when changing input table schemas.
\end{enumerate}

Factorized Machine Learning seeks to address issues one through three through the concept of ``learning over joins'' \cite{orion_learning_gen_lin_models}, which involves shifting the computations required for an ML model to the individual tables.

\subsection{Schema Mapping}
Schema mapping is an integral step in the Data Integration process and thus to Factorized ML. These mappings specify how the source datasets map to the target tables. Having a formal way to specify how these datasets relate is especially important for factorized ML. It allows us to convert these relationships between the source tables and the target table to a form that can be translated to linear algebra: the normalized matrix (detailed in \autoref{subsec:2-normalized-matrix}).

For the running example the schema mapping is as follows:
\begin{alignat*}{2}
  \intertext{Given source datasets, with abbreviated column names:}
   & o=order\_id, c=customer\_id, d=date,                           \\
   & p_{id}=product\_id, q=quantity, p=price, rq=return\_quantity   \\
   & S_1(o, c, d)                                                   \\
   & S_2(o, p_{id} , q,  p)                                         \\
   & S_3(o, p_{id}, rq)                                           & \\
  \intertext{The mapping to target table $T$ can be specified as follows. First, we left join $S_2$ with $S_3$ on $S_{2}.o=S_{3}.o$ and $S_{2}.p_{id}=S_{3}.p_{id}$ to get an intermediate table with schema:}
   & T(o, p_{id}, q, p, rq)                                         \\
  \intertext{Next, we inner join this with $S_1$ to get the final table $T$:}
   & T(o, c, d, p_{id}, q, p, rq)                                   \\
\end{alignat*}


% The language used for these mappings is \textit{source-to-target tuple generating dependencies (s-t tgd)} \cite{tgds-Fagin2009}. These are first-order logic formulas that specify, through atomic formulas over schemas $S$ and the target schema $T$, how the tuples of the source tables map to the target table. Here, we show the TGD for the running example.\begin{alignat*}{2}
%     \intertext{Given source datasets, with abbreviated column names:}
%      & o=order\_id, c=customer\_id, d=date,                                                                                                                      \\
%      & p_{id}=product\_id, q=quantity, p=price, rq=return\_quantity                                                                                              \\
%      & S_1(o, c, d)                                                                                                                                              \\
%      & S_2(o, p_{id} , q,  p)                                                                                                                                    \\
%      & S_3(o, p_{id}, rq)                                                                                                                                      & \\
%     \intertext{the mapping to target table $T$ can be specified as follows. First, we left join $S_2$ with $S_3$:}
%      & \forall o, p_{id}, q, p, rq \left( S_3(o, p_{id}, rq) \land S_2(o, p_{id}, q, p) \rightarrow \exists o, p_{id}, q, p, rq T(o, p_{id}, q, p, rq) \right)   \\
%     \intertext{Next, we inner join the result with $S_1$ to get the final schema $T(o, c, d, p_{id}, q, p, rq)$:} \begin{split}
%                                                                                                                       & \forall o, c, d, p_{id}, q, p, rq ( S_1(o, c, d) \land T(o, p_{id}, q, p, rq) \rightarrow \\
%                                                                                                                       & \exists o, c, d, p_{id}, q, p, rq T(o, c, d, p_{id}, q, p, rq) )
%                                                                                                                   \end{split}
% \end{alignat*}

Now that we have the schema mapping we can translate this to the normalized matrix, which we will do in the next section.

\section{Factorized Machine Learning}
\label{sec:2-factorized-ml}
As stated previously in this thesis, Factorized ML is the process of training Machine Learning models on multiple tables without the need to materialize the join between these tables. This section will go in-depth on how this can be achieved, continuing the running example from \autoref{fig:running-example-fac-vs-mat}.  We start with the definitions (\autoref{subsec:2-normalized-matrix}) followed by an in-depth example of the involved linear algebra (\autoref{subsubsec:2-fac-ml-example}).

\subsection{Normalized Matrix}
\label{subsec:2-normalized-matrix}
As Machine Learning algorithms can be expressed in Linear Algebra (LA), we need to express the Data Integration scenario of an ML use case in terms of Linear Algebra, .i.e., we need to translate the Schema Mappings of an integration scenario to Linear Algebra to allow us to achieve the goal of “pushing down” ML to the separate source tables. This is achieved through the \textbf{Normalized matrix}: A set of matrices that capture the necessary DI metadata telling us how the source tables map to the materialized Target table \cite{amalur, morpheus}.

The \textbf{Mapping matrix} and \textbf{Indicator matrix} respectively represent how the columns and rows from each source table $S_k$ map to the Target table $T$.

\subsubsection{Mapping Matrix}
The Mapping Matrix $M$ is a set of matrices $M_k$ for each source table $S_k$ that denotes how source columns map to target columns. A value of 1 in this matrix indicates that the corresponding column (via column number) in $S_k$ corresponds to the corresponding column (via column number) in $T$. The formal definition is as follows.

\begin{definition}[\textit{Mapping matrix} \cite{amalur}]
  Each source table $S_k$ has a corresponding binary Mapping matrix $M$ of shape $c_T \times c_{S_k}$, where
  \begin{align*}
    M_k[i,j] = \begin{cases}
                 1, & \text{if $j$-th column of $S_k$ is mapped to the $i$-th column of $T$} \\
                 0, & \text{otherwise}
               \end{cases}
  \end{align*}
\end{definition}

Note that in the case that there is no column overlap between source tables this Mapping Matrix is redundant. This affects the materialization step, as shown in \autoref{def:materialization}.


\subsubsection{Indicator Matrix}
Now that we have defined how to map the columns from the source tables to the target table, we need to do the same for the rows. This is done with the \textbf{Indicator} matrix.

\begin{definition}[\textit{Indicator matrix} \cite{morpheus}]
  Each source table $S_k$ has a corresponding binary Indicator matrix $I$ of shape $r_T \times r_{S_k}$, where
  \begin{align*}
    I_k[i,j] = \begin{cases}
                 1, & \text{if $i$-th row of $S_k$ is mapped to the $j$-th row of $T$} \\
                 0, & \text{otherwise}
               \end{cases}
  \end{align*}
\end{definition}

\subsubsection{Materialization}
Using the normalized matrix, we can now \textbf{materialize} the join to obtain the target matrix $T$:

\begin{definition}[\textit{Materializing the Normalized Matrix to obtain Target matrix $T$}]
  \begin{itemize}
    \item[]
    \item[] Given
    \item[$k$] Table id $k \in [1,n]$
    \item[$S_k$] Source tables
    \item[$M_k$] Mapping matrices
    \item[$I_k$] Indicator matrices
  \end{itemize}
  \[
    T = \begin{cases}
      \sum_{k=1}^n  I_k S_k M^T_k,  & \text{if there is column overlap between source tables} \\
      \begin{bNiceMatrix}
        \vdots  & \vdots & \vdots  \\
        I_1 S_1 & \cdots & I_n S_n \\
        \vdots  & \vdots & \vdots  \\
      \end{bNiceMatrix}, & \text{otherwise}
    \end{cases}
  \]
  \label{def:materialization}

\end{definition}

The materialization case when there is no column overlap is a horizontal concatenation of each source matrix $S_k$ multiplied by $I_k$: $I_k S_k, k \in [1,n]$. Intuitively the materialization process can be seen as:
\begin{algorithmic}
  \ForEach {$k \in [1,n]$} \Comment{For each source table}
  \State $rows_k \gets I_k S_k$ \Comment{Map the source table rows to the target table}
  \State $T_k \gets rows_k M^T_k$ \Comment{Map the source table columns to the target table}
  \EndFor
  \State $T \gets \sum_{k=1}^{n} T_k$ \Comment{Sum the results}
\end{algorithmic}

\subsubsection{Running Example: Normalized Matrix}
\label{subsubsec:2-fac-ml-example}
To translate the normalized matrix to how it is used in ML algorithms, we first show the full normalized matrix of the running example, followed by the materialized Target table $T$. The goal is to show how the matrices interact to allow computation with all information without necessarily materializing the join. For completeness, we show the calculations with the Mapping matrices $M_k$ included, but as highlighted before, this is not needed due to this scenario having no column overlap. However, it is insightful to show as it gives an idea of how this process looks when there is column overlap.
\begin{alignat*}{6}
  \intertext{
    These are the corresponding Source matrices $S_{1..3}$ for the source tables shown in \autoref{fig:running-example-fac-vs-mat}. The \textcolor{BurntOrange}{orange} numbers over the columns denote in which column of $T$ they will end up. The \textcolor{RoyalBlue}{blue} numbers at the end of each row illustrate to which target table rows they are mapped.
  }
                                                          & S_1 &     & =
  \begin{bNiceMatrix}[first-row,last-col]
    0 & 1  & 2    &     \\
    1 & 11 & 2024 & 0,1 \\
    2 & 12 & 2024 & 2   \\
    3 & 11 & 2024 & 3   \\
  \end{bNiceMatrix}   \quad                 &     & S_2 &   & =
  \begin{bNiceMatrix}[first-row,last-col]
    3 & 4  & 5  &   \\
    2 & 20 & 40 & 0 \\
    1 & 25 & 25 & 1 \\
    3 & 13 & 39 & 2 \\
    1 & 10 & 10 & 3 \\
  \end{bNiceMatrix}           \quad                 &     & S_3 &   & =
  \begin{bNiceMatrix}[first-row,last-col]
    6 &   \\
    1 & 3 \\
  \end{bNiceMatrix}                                  \\
  \intertext{
    The Indicator matrices denote how rows from $S$ map to rows in $T$. The column number denotes the row in $S$, the row number denotes the row in $T$. The \textcolor{RoyalBlue}{blue} annotations show more clearly how this works in the form \textcolor{RoyalBlue}{row number in $S_k \rightarrow$ row number in $T$}.
  }
                                                          & I_1 &     & =
  \begin{bNiceMatrix}[last-col]
    1 & 0 & 0 & 0 \rightarrow 0 \\
    1 & 0 & 0 & 0 \rightarrow 1 \\
    0 & 1 & 0 & 1 \rightarrow 2 \\
    0 & 0 & 1 & 2 \rightarrow 3 \\
  \end{bNiceMatrix}   \quad                          &     & I_2 &   & =
  \begin{bNiceMatrix}[last-col]
    1 & 0 & 0 & 0 & 0 \rightarrow 0 \\
    0 & 1 & 0 & 0 & 1 \rightarrow 1 \\
    0 & 0 & 1 & 0 & 2 \rightarrow 2 \\
    0 & 0 & 0 & 1 & 3 \rightarrow 3 \\
  \end{bNiceMatrix}     \quad                      &     & I_3 &   & =
  \begin{bNiceMatrix}[last-col]
    0 & \rightarrow 0   \\
    0 & \rightarrow 1   \\
    0 & \rightarrow 2   \\
    1 & 0 \rightarrow 3 \\
  \end{bNiceMatrix}                                            \\
  \intertext{
    The Mapping matrices denote how columns from $S$ map to columns in $T$. The row number denotes the column in $T$, the column number denotes the column in $S$. The \textcolor{BurntOrange}{orange} annotations show this in the form: \textcolor{BurntOrange}{column number in $S_k \rightarrow$ column number in $T$}.
  }
                                                          & M_1 &     & =
  \begin{bNiceMatrix}[last-col]
    1 & 0 & 0 & \textcolor{BurntOrange}{0 \rightarrow 0} \\
    0 & 1 & 0 & \textcolor{BurntOrange}{1 \rightarrow 1} \\
    0 & 0 & 1 & \textcolor{BurntOrange}{2 \rightarrow 2} \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 3}   \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 4}   \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 5}   \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 6}   \\
  \end{bNiceMatrix}   \quad &     & M_2 &   & =
  \begin{bNiceMatrix}[last-col]
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 0}   \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 1}   \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 2}   \\
    1 & 0 & 0 & \textcolor{BurntOrange}{0 \rightarrow 3} \\
    0 & 1 & 0 & \textcolor{BurntOrange}{1 \rightarrow 4} \\
    0 & 0 & 1 & \textcolor{BurntOrange}{2 \rightarrow 5} \\
    0 & 0 & 0 & \textcolor{BurntOrange}{\rightarrow 6}   \\
  \end{bNiceMatrix} \quad &     & M_3 &   & =
  \begin{bNiceMatrix}[last-col]
    0 & \textcolor{BurntOrange}{\rightarrow 0}   \\
    0 & \textcolor{BurntOrange}{\rightarrow 1}   \\
    0 & \textcolor{BurntOrange}{\rightarrow 2}   \\
    0 & \textcolor{BurntOrange}{\rightarrow 3}   \\
    0 & \textcolor{BurntOrange}{\rightarrow 4}   \\
    0 & \textcolor{BurntOrange}{\rightarrow 5}   \\
    1 & \textcolor{BurntOrange}{0 \rightarrow 6} \\
  \end{bNiceMatrix}
\end{alignat*}
\begin{gather*}
  \begin{alignat*}{4}
    \intertext{For conciseness we show the calculation of one of the sub-target tables $T_1$.}
                        & T_1 &       & = I_1                 &  & S_1 &  & M_1^T                   \\
                        & T_1 &       & = \begin{bNiceMatrix}
                                            1 & 0 & 0 \\
                                            1 & 0 & 0 \\
                                            0 & 1 & 0 \\
                                            0 & 0 & 1 \\
                                          \end{bNiceMatrix} &  &
    \begin{bNiceMatrix}
      1 & 11 & 2024 \\
      2 & 12 & 2024 \\
      3 & 11 & 2024 \\
    \end{bNiceMatrix} &     & M_1^T                                                                 \\
                        & T_1 &       & = \begin{bNiceMatrix}
                                            1 & 11 & 2024 \\
                                            1 & 11 & 2024 \\
                                            2 & 12 & 2024 \\
                                            3 & 11 & 2023 \\
                                          \end{bNiceMatrix} &  &     &  & \begin{bNiceMatrix}
                                                                            1 & 0 & 0 & 0 & 0 & 0 & 0 \\
                                                                            0 & 1 & 0 & 0 & 0 & 0 & 0 \\
                                                                            0 & 0 & 1 & 0 & 0 & 0 & 0 \\
                                                                          \end{bNiceMatrix} \\
  \end{alignat*}\\
  \hspace{-4cm}
  T_1  = \begin{bNiceMatrix}
    \Block[fill=red!15,rounded-corners]{4-3}{}
    1 & 11 & 2024 & 0 & 0 & 0 & 0 \\
    1 & 11 & 2024 & 0 & 0 & 0 & 0 \\
    2 & 12 & 2024 & 0 & 0 & 0 & 0 \\
    3 & 11 & 2023 & 0 & 0 & 0 & 0 \\
  \end{bNiceMatrix}
\end{gather*}

\begingroup
\setlength{\arraycolsep}{4.5pt}
\begin{alignat*}{2}
  \intertext{
    The materialized Target table $T$ is the element wise sum of the dot product of each tuple of Indicator, Source, and Mapping matrices. For each source table $S_k$ the intermittent result is shown as $T_k$. For clarity the cells from each source table are colored in the same color in the intermittent result and in Target table $T$.
  }
   & T_1= I_1 S_1 M_1^T &  & = \begin{bNiceMatrix}[first-row]
                                 0 & 1  & 2    & 3 & \cdots & 6 \\
                                 \Block[fill=red!15,rounded-corners]{4-3}{}
                                 1 & 11 & 2024 & 0 & \cdots & 0 \\
                                 1 & 11 & 2024 & 0 & \cdots & 0 \\
                                 2 & 12 & 2024 & 0 & \cdots & 0 \\
                                 3 & 11 & 2023 & 0 & \cdots & 0 \\
                               \end{bNiceMatrix}
  T_2= I_2 S_2 M_2^T= \begin{bNiceMatrix}[first-row]
                        0 & 1 & 2 & 3                                             & 4  & 5  & 6 \\
                        0 & 0 & 0 & \Block[fill=blue!15,rounded-corners]{4-3}{} 2 & 20 & 40 & 0 \\
                        0 & 0 & 0 & 1                                             & 25 & 25 & 0 \\
                        0 & 0 & 0 & 3                                             & 13 & 39 & 0 \\
                        0 & 0 & 0 & 1                                             & 10 & 10 & 0 \\
                      \end{bNiceMatrix}
  \\
   & T_3= I_3 S_3 M_3^T &  & = \begin{bNiceMatrix}[first-row]
                                 0 & \cdots & 5 & 6                                              \\
                                 0 & \cdots & 0 & 0                                              \\
                                 0 & \cdots & 0 & 0                                              \\
                                 0 & \cdots & 0 & 0                                              \\
                                 0 & \cdots & 0 & \Block[fill=orange!15,rounded-corners]{1-1}{}1 \\
                               \end{bNiceMatrix}
  T  = \sum_{k=1}^{3} I_k S_k M^T_k =  \begin{bNiceMatrix}[first-row,last-col]
                                         0 & 1  & 2    & 3                                           & 4  & 5  & 6                                                  \\
                                         \Block[fill=red!15,rounded-corners]{4-3}{}
                                         1 & 11 & 2024 & \Block[fill=blue!15,rounded-corners]{4-3}{}
                                         2 & 20 & 40   & 0                                           & 0                                                            \\
                                         1 & 11 & 2024 & 1                                           & 25 & 25 & 0                                              & 1 \\
                                         2 & 12 & 2024 & 3                                           & 13 & 39 & 0                                              & 2 \\
                                         3 & 11 & 2024 & 1                                           & 10 & 10 & \Block[fill=orange!15,rounded-corners]{1-1}{}1 & 3 \\
                                       \end{bNiceMatrix}
\end{alignat*}
\endgroup


\subsection{Factorized Linear Algebra}
In the previous section, we have shown the properties of the Normalized matrix. This section will show how commonly used Linear Algebra operators are rewritten for the Normalized matrix for the purpose of performing factorized ML \cite{morpheus}. We will show how to perform element-wise operations, reduction operations, dot-product operations, and a running example of right matrix multiplication (RMM) on the Normalized matrix. The goal is to show how (most of) these operations can be performed without materializing the join between the source tables, and how the Normalized matrix allows us to do so.


\subsubsection{Element-wise Scalar Operations}
This group of operators perform an operation on every element of a matrix independently of each other. The arithmetic operations are: $+$, $-$, $\times$, $\div$ and $ ^\wedge $ (these operators are denoted by $\oslash$). This can be seen as a scalar function $f$ applied to each element of a matrix $T$. The rewrite rule therefore is very simple for these arithmetic operators, as well as for any other scalar function (e.g., $log$, $round$) $f$:
\begin{alignat*}{2}
  x \oslash T & \rightarrow [x \oslash S, I, M] \\
  T \oslash x & \rightarrow [S \oslash x, I, M]
  \intertext{or more generally:}
  f(T)        & \rightarrow [f(S), I, M]
\end{alignat*}

These operations all return a normalized matrix and can thus be performed without materializing the join between the source tables. In the used implementation \cite{amalur_tkde24}, when a normalized matrix is transposed, the actual computation is not carried out, but the transpose is simply added as a flag. Then, for any downstream operators, the transpose flag is checked, and the computation is performed accordingly. For these element-wise operations the transposed rewrite is:
\begin{alignat*}{2}
  f(T^T) & \rightarrow [f(S), I, M]^T
\end{alignat*}

\subsubsection{Aggregation}
The supported aggregation operators are row-wise and column-wise summation, respectively abbreviated to rowSums and colSums. For the factorized rowSums case we sum each source table separately, then multiply with the indicator matrices and sum the results, the mapping matrix is irrelevant. This operation produces a single (column) vector of size $r_T \times 1$. For the transposed case, it is equal to a column summation. These rewrite rules are:
\begin{alignat*}{2}
  \text{rowSums}(T)   & \rightarrow \sum_{k=1}^n I_k \text{rowSums}(S_k) \\
  \text{rowSums}(T^T) & \rightarrow \text{colSums}(T)
\end{alignat*}

Summing column-wise gives a row vector of shape $1 \times c_T$. It is equal to first summing the indicator tables column-wise, then materializing with these aggregated indicator matrices.  The rewrite rule for the factorized case is:
\begin{alignat*}{2}
  \text{colSums}(T)   & \rightarrow \sum_{k=1}^n \text{colSums}(I_k) S_k M_k^T \\
  \text{colSums}(T^T) & \rightarrow \text{rowSums}(T)
\end{alignat*}
As these operations do not create normalized matrices, and in fact materialize (part of) the join the benefit of factorized computation is smaller.

\subsubsection{Multiplication}
As matrix multiplication is not commutative, there are different rewrite rules for left- and right-matrix multiplication. The rewrite rule for left matrix multiplication (LMM) with another matrix $X$ is:
\begin{alignat*}{2}
  TX   & \rightarrow \sum_{k=1}^n I_k S_k M_k^T X \\
  T^TX & \rightarrow (X^TT)^T
\end{alignat*}

For right matrix multiplication (RMM) the rule is the same, we still essentially materialize the join, but with $X$ on the left-hand side:
\begin{alignat*}{2}
  XT   & \rightarrow \sum_{k=1}^n X I_k S_k M_k^T \\
  XT^T & \rightarrow (TX^T)^T
\end{alignat*}

\subsubsection{Running Example: Right Matrix Multiplication}

\begingroup
\setlength{\arraycolsep}{3.0pt}
\begin{alignat*}{1}
  \intertext{We showcase right RMM and its rewrite rule by multiplying with $X$. First for the materialized Target table $T$:}
  X T & = \begin{bNiceMatrix}
            1 & 1 & 2 & 3 \\
          \end{bNiceMatrix}
  \begin{bNiceMatrix}
    1 & 11 & 2024 & 2 & 20 & 40 & 0 \\
    1 & 11 & 2024 & 1 & 25 & 25 & 0 \\
    2 & 12 & 2024 & 3 & 13 & 39 & 0 \\
    3 & 11 & 2023 & 1 & 10 & 10 & 1 \\
  \end{bNiceMatrix}                                                         \\
      & =\begin{bNiceMatrix}
           15 & 79 & 14165 & 12 & 101 & 173 & 3 \\
         \end{bNiceMatrix}                                             \\
  \intertext{Now for the Normalized matrix, recall the rewrite rule for RMM:}
  X T & = \sum_{k=1}^{n} X I_k S_k M^T_k
  \intertext{For conciseness we refer back to sub results $T_{0\cdots2}$ and use them directly here. We also leave out $X$ in the subcalculations for $T_{1,2}$.}
      & = X T_0 + X T_1 + X T_2                                                           \\
      & = \begin{bNiceMatrix}
            1 & 1 & 2 & 3 \\
          \end{bNiceMatrix} \begin{bNiceMatrix}
                              1 & 11 & 2024 & 0 & \cdots & 0 \\
                              1 & 11 & 2024 & 0 & \cdots & 0 \\
                              2 & 12 & 2024 & 0 & \cdots & 0 \\
                              3 & 11 & 2023 & 0 & \cdots & 0 \\
                            \end{bNiceMatrix} + X \begin{bNiceMatrix}
                                                    0 & 0 & 0 & 2 & 20 & 40 & 0 \\
                                                    0 & 0 & 0 & 1 & 25 & 25 & 0 \\
                                                    0 & 0 & 0 & 3 & 13 & 39 & 0 \\
                                                    0 & 0 & 0 & 1 & 10 & 10 & 0 \\
                                                  \end{bNiceMatrix} + X\begin{bNiceMatrix}
                                                                         0 & \cdots & 0 & 0 \\
                                                                         0 & \cdots & 0 & 0 \\
                                                                         0 & \cdots & 0 & 0 \\
                                                                         0 & \cdots & 0 & 1 \\
                                                                       \end{bNiceMatrix} \\
      & =\begin{bNiceMatrix}
           15 & 79 & 14165 & 0 & 0 & 0 & 0 \\
         \end{bNiceMatrix} +
  \begin{bNiceMatrix}
    0 & 0 & 0 & 12 & 101 & 173 & 0 \\
  \end{bNiceMatrix}+
  \begin{bNiceMatrix}
    0 & 0 & 0 & 0 & 0 & 0 & 3 \\
  \end{bNiceMatrix}                                                               \\& =\begin{bNiceMatrix}
    15 & 79 & 14165 & 12 & 101 & 173 & 3 \\
  \end{bNiceMatrix}
\end{alignat*}
\endgroup

\subsection{Machine Learning Models}
We use the same machine learning models as Morpheus \cite{morpheus} and \cite{amalur_tkde24}. These models consist of Linear Regression, Logistic Regression, K-means clustering, and Gaussian NMF. Having demonstrated the transformation rules for the relevant Linear Algebra operations, we are now able to illustrate how these models can be adapted for the Normalized matrix. This adaptation enables us to execute these machine learning models without materializing the join between the source tables. The algorithms are detailed in the next sections, with the factorized operators used highlighted in red. Within the algorithms, we use the following conventions: $X$ represents the matrix of independent variables, which in our scenario corresponds to the normalized matrix. The dependent variable is denoted as $y$, the weight vector as $w$, the learning rate as $\gamma$, and $n$ represents the number of iterations.

\subsubsection{Linear Regression}
\begin{algorithm}[ht!]
  \caption[Linear regression]{Linear regression using Gradient Descent
    \cite{morpheus}}\label{alg:linear-regression}
  \begin{algorithmic}
    \Require $X, y , w, \gamma$
    \For{$i \in 1:n$}
    \State $w = w - \gamma (\text{\red{$X^T$}}((\text{\red{$X w$}}) - y))$
    \EndFor
  \end{algorithmic}
\end{algorithm}
Linear Regression (\autoref{alg:linear-regression}) is an ML technique fit to find linear relationships between independent variables and a dependent variable. The algorithm utilizes gradient descent to iteratively converge towards the best solution. The LA operators performed on the normalized matrix $T$ are Transpose $T^T$, and Left Matrix Multiplication $TX$.

\subsubsection{Logistic Regression}
Logistic Regression (\autoref{alg:logistic-regression}) is very similar to Linear Regression, but instead of predicting a continuous value, it predicts a binary value. The rewrite rule for Logistic Regression uses the same operators as Linear Regression: Transpose and Left Matrix Multiplication.

\begin{algorithm}[ht]
  \caption[Linear regression]{Logistic regression using Gradient Descent
    \cite{morpheus}}\label{alg:logistic-regression}
  \begin{algorithmic}
    \Require $X, y , w, \gamma$
    \For{$i \in 1:n$}
    \State $w = w - \gamma \left(\text{\red{$X^T$}} \frac{y}{1+e^{\text{\red{$X w$}}}}\right)$
    \EndFor
  \end{algorithmic}
\end{algorithm}

\subsubsection{K-means Clustering}
The regression algorithms discussed above are supervised, that is, they predict a value based on a set of input features. K-means clustering is an unsupervised algorithm that groups data points into a predefined number of clusters based on their similarity. The operators used to calculate the clusters are $exp$ ($X^2$), Scalar Multiplication, Transposition, Row Summation, and Left Matrix Multiplication. The algorithm is shown in \autoref{alg:k-means}.

\begin{algorithm}[ht]
  \caption[K-Means Clustering]{K-Means Clustering
    \cite{morpheus}\\
    $\mathbf{1}_{r \times c}$ denotes a matrix of size $r \times c$ filled with ones, this is used to repeat a vector to a matrix, either row- or column-wise.}\label{alg:k-means}
  \begin{algorithmic}
    \Require $X, k$ (number of centroids)
    \State $C = \text{rand}(r_X \times k)$ \Comment{Randomly initialize centroids matrix $C$}
    \State $D_X = \text{\red{rowSums($X^2$)}} \times \mathbf{1}_{1 \times k}$ \Comment{Compute the $l^2$-norm of points for distances}
    \State $T_2 = \text{\red{$2 \times X$}}$

    \For{$i \in 1:n$}
    \State $D = D_X - T_2C + \left( \mathbf{1}_{r_X\times 1} \times \text{colSums}(C^2) \right)$ \Comment{Compute distances}
    \State $A = (D == \text{rowMin}(D) \times \mathbf{1}_{1 \times k})$ \Comment{Assign points to the closest centroid}
    \State $C =\frac{\text{\red{$X^T A$}}}{\mathbf{1}_{c_X \times 1} \times \text{colSums}(A)}$ \Comment{Update centroids}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\subsubsection{Gaussian Non-negative Matrix Factorization}
Gaussian Non-negative Matrix Factorization (Gaussian NMF) is a technique used to decompose a matrix into two smaller nonnegative matrices. It is used for feature extraction from data and is often used in image processing and text mining. The operators used in the algorithm are Transpose and Right Matrix Multiplication. The rank hyperparameter $r$ controls the size of the resulting matrices. The algorithm is shown in \autoref{alg:gaussian-nmf}.

\begin{algorithm}[ht]
  \caption[Gaussian NMF]{Gaussian Non-negative Matrix Factorization
    \cite{morpheus}}\label{alg:gaussian-nmf}
  \begin{algorithmic}
    \Require $X, r\ \text{(rank)}$
    \State $W = \text{rand}(r_X \times r)$ \Comment{Randomly initialize $W$}
    \State $H = \text{rand}(r \times c_X)$ \Comment{Randomly initialize $H$}
    \For{$i \in 1:n$}
    \State $H = H \times \left(\frac{\text{\red{$W^T X$}}}{W^T W H}\right)$
    \State $W = W \times \left(\frac{\text{\red{$X H^T$}}}{W(H H^T)}\right)$
    \EndFor
  \end{algorithmic}
\end{algorithm}


\subsection{Overview}
\label{subsec:factorized-ml-summary}
In this section we have presented the rewrite rules for the Normalized matrix, for commonly used Linear Algebra operators, and how these are used in the training of Machine Learning models without the need to explicitly compute the join between the source tables. \autoref{tab:factorized-ml-operators-overview}  provides a summary of the operators discussed, with the final column describing the specific operators employed for each ML model. This underscores the importance of a robust cost model in factorized ML, since various models leverage a range of operators in distinct ways when benefiting from factorized computation.

\begin{table}[ht]
  \small
  \resizebox{\textwidth}{!}{%
    \input{chapters/02_preliminaries/auto-generated/operators}}
  \caption{Overview of factorized ML operators.}
  \label{tab:factorized-ml-operators-overview}
\end{table}

\section{Machine Learning on GPUs}
\label{sec:2-ml-on-gpu}
Graphics Processing Units (GPUs) have emerged as the preferred processing units in the Machine Learning domain due to their significant advantages over Central Processing Units (CPUs). As CPUs are designed for diverse general-purpose applications, this is not surprising. Whereas CPUs excel in executing sequential tasks such as running an operating system, GPUs are optimized for parallel tasks. This specialization enables them to efficiently carry out identical operations on multiple pieces of data simultaneously, exactly what is needed for the prevalent linear algebra operations in ML.

\subsection{Architecture}
\label{subsec:gpu-architecture}

\begin{figure}[ht]
  \includegraphics[width=.95\linewidth]{chapters/02_preliminaries/figures/CPU-vs-GPU.pdf}
  \caption[Simplified view of the difference in architecture between a CPU and a GPU.]{ Simplified view of the difference in architecture between a CPU and a GPU. Compute cores are blue, control units green and L1 Cache is marked in yellow. Figure based on visualizations from \cite{gpu-in-ml-survey, cuda-programming-guide, tvm}. Compute Primitive shows an algorithm performing sequential matrix multiplication where a scalar operation is processed at a time, and a SIMD algorithm were a vector operation is computed in parallel.}
  \label{fig:cpu-vs-gpu}
\end{figure}
This section elaborates on the architecture of a GPU, emphasizing its effectiveness in performing Linear Algebra tasks. \autoref{fig:cpu-vs-gpu} illustrates the architectural differences between a CPU and a GPU and will serve as a point of reference throughout this section.

The heart of the GPU is the Streaming Multiprocessor (SM), which is responsible for executing thousands of parallel threads. Each SM comprises multiple CUDA cores (highlighted in blue), resembling CPU cores, that carry out arithmetic operations. These cores are optimized to manage multiple operations concurrently, enhancing their effectiveness for the matrix and vector calculations crucial in Machine Learning.

Data and code are transferred to the streaming multiprocessors (SMs) by passing through various memory layers. Initially, data is fetched from the host and stored in the GPU's DRAM, after which it is moved to the SM via the L2 cache. This facilitates quick data exchange and minimizes latency. Threads are grouped into blocks and then allocated to SMs for scheduling. This organization enables efficient resource management and parallel thread execution. Each SM is responsible for managing a block of threads that are then scheduled for execution. The cores within the SM execute these threads concurrently, resulting in a high degree of parallelism. This simultaneous processing of multiple data points is known as Single Instruction, Multiple Data (SIMD) architecture. Although CPUs also support SIMD, GPUs offer a much higher degree of parallelism, due to the higher number of cores, which proves to be advantageous in Machine Learning applications, where identical operations are applied across numerous data points.

Modern CPUs can handle a scalar operation within a single clock cycle, a GPU can perform operations on vectors concurrently. This difference in parallelism is illustrated in the lower section of \autoref{fig:cpu-vs-gpu}.

\subsection{Estimating Performance on GPU}
\label{subsec:gpu-performance}
Following the architecture overview, it is essential to grasp the performance dynamics of a GPU in the context of Machine Learning applications, specifically focusing on Linear Algebra operations. The GPU's ability to execute thousands of threads concurrently is the basis of its computational power, particularly for tasks with high arithmetic intensity. The subsequent passages provide valuable perspectives from \cite{nvidia-gpu-performance:online}.

The arithmetic intensity refers to the ratio of mathematical operations performed per memory operation. In the context of GPUs, it is a critical factor in determining performance constraints. The time it takes for a function to execute on a GPU can be constrained by memory bandwidth, mathematical bandwidth, or latency. To illustrate this, imagine a function that reads the input data, performs calculations, and writes the output. In an ideal case, the time spent on memory operations $ T_{mem} $ and math operations $ T_{math} $ can overlap (because many threads are running at once). In this ideal case, the total execution time will approach $ \max(T_{mem}, T_{math})$. As will be shown in \autoref{sec:5-gpu-performance-analysis} programs on GPUs are often limited by their memory bandwidth, due to their very high computational throughput. On the contrary, for CPU operations, the total time taken is highly correlated with $T_{math}$ \autoref{sec:5-motivation}. This discrepancy introduces difficulties in creating a singular cost model that performs well on both CPU and GPU scenarios.

When the computational time $ T_{math} $ exceeds the memory access time $ T_{mem} $, a function is classified as math-bound, indicating that the GPU's computational capabilities are the limiting factor. On the contrary, if $ T_{mem} $ is higher, it is memory-bound, which means that the memory bandwidth is the restricting element. This relationship is illustrated by the inequality $ \frac{\# ops}{BW_{math}} > \frac{\# bytes}{BW_{mem}} $, which can be rearranged as $ \frac{\# ops}{\# bytes} > \frac{BW_{math}}{BW_{mem}} $. Here, the left side denotes a function's arithmetic intensity, while the right side represents the GPU's $ops:byte$ ratio, i.e., the number of Floating Point Operations (FLOPs) per byte retrieved from memory.

In practice, many Machine Learning operations, such as linear layers or activation functions, often have low arithmetic intensities, sometimes executing only one operation for every two-byte element accessed from and stored in memory. This characteristic typically renders them memory-bound on GPUs. However, for operations with high arithmetic intensity, like large matrix multiplications, the GPU's mathematical bandwidth emerges as the constraining factor.

To fully leverage a GPU's capabilities, it is crucial to ensure sufficient parallelism. This is achieved by launching a significant number of thread blocks, ideally several times higher than the number of SMs, to minimize the tail effect, where only a few active thread blocks remain towards the end of a function's execution. By maintaining a high level of parallelism, GPUs can effectively hide instruction latency and maximize throughput, rendering them better suited for the parallel processing demands of Machine Learning compared to CPUs.