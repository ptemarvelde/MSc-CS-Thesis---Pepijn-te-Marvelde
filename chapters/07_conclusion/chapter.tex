% !TEX root = ../../main.tex

\chapter{Conclusion}

\label{chapter:conclusion}
In closing, we consolidate the key contributions and discoveries of this thesis in \autoref{sec:7-contributions}. Additionally, we acknowledge the limitations and outline potential areas for future research in \autoref{sec:7-future-work}.

\section{Cost Estimation for Factorized Machine Learning}
\label{sec:7-contributions}
Our research explores the dynamics of cost estimation for factorized machine learning, emphasizing the comparative performance of GPUs against CPUs. We find that GPU training exhibits distinct cost characteristics from CPU training, which significantly influences cost model design and optimization of factorized machine learning processes.

Previous cost estimation methodologies have predominantly centered on CPU contexts, resulting in inaccuracies when extrapolated to GPU environments. Our analysis reveals a pronounced difference in the speedup of factorized model training between CPU and GPU platforms. This discrepancy stems from the distinct architectural designs and processing capabilities of GPUs, which necessitates a tailored approach to cost estimation.

Through empirical research and extensive experimentation, we have formulated an innovative cost model that is tuned to the nuances of GPU computation. Our model diverges from existing methods by incorporating a deeper understanding of GPU architecture, and by leveraging a more comprehensive set of features to decide the training approach.

By accounting for the unique cost factors associated with GPU usage, we provide a more reliable framework for predicting whether factorization is beneficial. The results of our comparative analysis demonstrate that our cost model outperforms existing methods, both for CPU and GPU scenarios. In the tested scenarios on real-world datasets, where a perfect model would result in $1667$ seconds saved, the SOTA cost model achieves a time loss of $507$s, whereas using our hybrid cost model would save $1350$s, reaching almost $80\%$ of the maximum possible utility.

This progress in cost estimation facilitates the broader adoption of factorized machine learning within the industry, enabling considerable time savings in scenarios that use intensive training. Such scenarios include the training of large models, hyperparameter optimization, and real-time training. The impact of using our cost model in the ML workflow is minimal, but when factorization is faster, we achieve an average speedup of $3.8\times$. The largest hurdle for a Data Scientist to use this approach is the adaption of their data integration workflow so it fits into the factorized ML framework, which currently is a manual process.

Despite promising results, our model comes with certain limitations. The need to account for the unique characteristics of GPU computation introduces complexity into our model, which makes its predictions less explainable than the state-of-the-art models. Another risk associated with the added complexity is overfitting to the current implementation. How our cost model performs when training is done with another implementation of factorized learning is uncertain. Furthermore, the introduction of new machine learning models requires additional work to adapt our cost model accordingly. These challenges highlight areas for future research and improvement. Nevertheless, the benefits of our model in terms of accuracy and efficiency make it a valuable contribution to the field of factorized machine learning.

\section{Future Work}
\label{sec:7-future-work}
As we look forward, there are several promising directions for future work. This thesis has demonstrated the value of factorized machine learning in real-world settings, but to facilitate its adoption in the industry, steps need to be taken. One such step could be the integration of factorized machine learning models into widely used frameworks like TensorFlow and PyTorch. This would not only enhance the practicality and reach of factorized machine learning but also open up opportunities for investigating cost estimation. Given the maturity of these frameworks and the extensive research already conducted to optimize their training processes, this could significantly advance our understanding of cost dynamics in factorized machine learning.

Moreover, this integration could also enable the exploration of factorized machine learning in a distributed setting. This would be a significant advancement, as it would allow us to leverage the power of distributed computing to further enhance the efficiency and scalability of factorized machine learning models. This could potentially lead to breakthroughs in handling larger datasets and more complex computations, thus broadening the scope and impact of factorized machine learning.

In terms of future steps specifically for cost estimation in factorized machine learning, we propose two main areas of focus. Firstly, other types of cost models could be explored, such as those based on micro benchmarking. This involves conducting performance tests on individual operations before running a training scenario. The insights gained from these benchmarks could then be used to make informed decisions between materialization and factorization. This could improve accuracy as the actual datasets can be used for these benchmarks. However, consideration must be given to keeping the overhead of such an approach low. Another direction that could complement our proposed approach is the exploration of online training. This would involve continuously updating the model as new scenarios are tested, leading to a continuously improving model. Such an approach would be particularly valuable in a real-world factorized machine learning framework.

Second, we could expand on the profiling experiments conducted in this thesis. By conducting more extensive profiling experiments, on model training scenarios instead of individual operators, we could gain a deeper understanding of the cost dynamics of factorized machine learning. This would allow us to refine our cost model further and potentially identify new cost factors that could be incorporated into the model. However, this would require a significant investment in time and resources, as profiling experiments are time-consuming and computationally expensive.

Despite the challenges, such as higher complexity and the need for additional work with the introduction of new machine learning models, our model makes a valuable contribution to the field in terms of accuracy and efficiency. These future directions highlight the potential for continued refinement and expansion of our cost model, contributing to the ongoing advancement of factorized machine learning.