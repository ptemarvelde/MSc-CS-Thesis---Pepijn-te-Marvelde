% !TEX root = ../../main.tex

\chapter{Conclusion}

\label{chapter:conclusion}
To conclude this thesis we summarize the main contributions and findings of this work in \autoref{sec:7-contributions}. We also discuss the limitations of our work and suggest future research directions in \autoref{sec:7-future-work}.

\section{Cost Estimation for Factorized Machine Learning}
\label{sec:7-contributions}
We have explored the cost estimation landscape for factorized machine learning, with a particular focus on the performance of GPUs compared to CPUs. Our findings are that training on GPUs exhibit significantly different cost characteristics than training on CPUs, which has profound implications for the design of cost estimators, and the optimization of factorized machine learning.

Previous cost estimation methods have been CPU-centric, leading to inaccuracies when applied to GPU-based scenarios. We show that the speedup of factorized model training differs greatly between CPU and GPU. This discrepancy stems from the distinct architectural designs and processing capabilities of GPUs, which necessitate a tailored approach to cost estimation.

Through empirical research and extensive experimentation, we have developed a novel cost model that is finely tuned to the nuances of GPU computation. Our model diverges from existing methods by incorporating a deeper understanding of GPU architecture, and by leveraging a more comprehensive set of features to predict cost.

The results of our comparative analysis demonstrate that our cost model outperforms existing methods in terms of accuracy for GPUs. By accounting for the unique cost factors associated with GPU usage, we provide a more reliable framework for predicting the computational expenses of factorized machine learning.

The results of our comparative analysis demonstrate that our cost model outperforms existing methods, both for GPU and CPU scenarios. By accounting for the unique cost factors associated with GPU usage, we provide a more reliable framework for predicting whether factorized machine learning will result in a speedup over materialized learning.

This advancement in cost estimation not paves the way for the adoption of factorized machine learning in industry, by enabling significant time savings in training-intensive scenarios. These scenarios include the training of large models, hyperparameter tuning, and online training. Our cost estimator facilitates a smoother transition to factorized machine learning workflows, marking a significant stride towards a more efficient future in machine learning.

Despite the promising results, our model does come with certain limitations. The complexity of our model is higher than that of the state-of-the-art models, which can make it less explainable. This complexity arises from the need to account for the unique characteristics of GPU computation. Furthermore, the introduction of new machine learning models requires additional work to adapt our cost model accordingly. These challenges highlight areas for future research and improvement. Nevertheless, the benefits of our model in terms of accuracy and efficiency make it a valuable contribution to the field of factorized machine learning.

\section{Future Work}
\label{sec:7-future-work}
As we look forward, there are several promising directions for future work. This thesis has demonstrated the value of factorized machine learning in real-world settings, but to facilitate its adoption in the industry, certain steps need to be taken. One such step could be the integration of factorized machine learning models into widely-used industry frameworks like TensorFlow and PyTorch. This would not only enhance the practicality and reach of factorized machine learning but also open up opportunities for investigating cost estimation. Given the maturity of these frameworks and the extensive research already conducted to optimize their training processes, this could significantly advance our understanding of cost dynamics in factorized machine learning.

Moreover, this integration could also enable the exploration of factorized machine learning in a distributed setting. This would be a significant advancement, as it would allow us to leverage the power of distributed computing to further enhance the efficiency and scalability of factorized machine learning models. This could potentially lead to breakthroughs in handling larger datasets and more complex computations, thereby broadening the scope and impact of factorized machine learning.

In terms of future steps specifically for cost estimation in factorized machine learning, we propose two main areas of focus. Firstly, we could investigate other types of cost models, such as those based on micro benchmarking. This involves conducting performance tests on individual operations before running a training scenario. The insights gained from these benchmarks could then be used to make informed decisions between materialization and factorization. This could improve accuracy as the actual datasets can be used for these benchmarks. However, consideration must be given to keeping the overhead of such an approach low. Another direction that could complement our proposed approach is the exploration of online training. This would involve continuously updating the model as new scenarios are tested, leading to a continuously improving model. This would be particularly valuable in a real-world factorized machine learning framework.

Secondly, we could expand on the profiling experiments conducted in this thesis. By conducting more extensive profiling experiments on model training scenarios instead of individual operators, we could gain a deeper understanding of the cost dynamics of factorized machine learning. This would allow us to refine our cost model further and potentially identify new cost factors that could be incorporated into the model. However, this would require a significant investment in time and resources, as profiling experiments can be time-consuming and computationally expensive.

Despite the challenges such as higher complexity and the need for additional work with the introduction of new machine learning models, our model makes a valuable contribution to the field in terms of accuracy and efficiency. These future directions highlight the potential for continued refinement and expansion of our cost model, contributing to the ongoing advancement of factorized machine learning.