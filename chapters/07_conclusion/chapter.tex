% !TEX root = ../../main.tex

\chapter{Conclusion}

\label{chapter:conclusion}
In closing, we consolidate the key contributions and discoveries of this thesis in \autoref{sec:7-contributions}. Additionally, we acknowledge the limitations and outline potential areas for future research in \autoref{sec:7-future-work}.

\section{Cost Estimation for Factorized Machine Learning}
\label{sec:7-contributions}
Our research explores the dynamics of cost estimation for factorized machine learning, emphasizing the comparative performance of GPUs against CPUs. We discovered that GPU training exhibits distinct cost characteristics from CPU training, which significantly influences cost model design and optimization of factorized machine learning processes.

Previous cost estimation methodologies have predominantly centered on CPU contexts, resulting in inaccuracies when extrapolated to GPU environments. Our analysis reveals a pronounced difference in the speedup of factorized model training between CPU and GPU platforms. This discrepancy stems from the distinct architectural designs and processing capabilities of GPUs, which necessitates a tailored approach to cost estimation.

Through empirical research and extensive experimentation, we have formulated an innovative cost model that is tuned to the nuances of GPU computation. Our model diverges from existing methods by incorporating a deeper understanding of GPU architecture, and by leveraging a more comprehensive set of features to predict cost.

The results of our comparative analysis demonstrate that our cost model outperforms existing methods, both for GPU and CPU scenarios. By accounting for the unique cost factors associated with GPU usage, we provide a more reliable framework for predicting whether factorized machine learning will result in a speedup over materialized learning.

This advancement in cost estimation not paves the way for the adoption of factorized machine learning in industry, by enabling significant time savings in training-intensive scenarios. These scenarios include the training of large models, hyperparameter tuning, and online training. Our cost model facilitates a smoother transition to factorized machine learning workflows, marking a significant stride towards a more efficient future in machine learning.

This progress in cost estimation facilitates the broader adoption of factorized machine learning within the industry, enabling considerable time savings in scenarios that demand intensive training. Such scenarios include the training of large models, hyperparameter optimization, and real-time training. Our cost model facilitates a smoother transition to factorized machine learning workflows, marking a significant stride towards a more efficient future in machine learning.

Despite promising results, our model comes with certain limitations. The complexity of our model is higher than that of state-of-the-art models, which makes its predictions less explainable. This complexity arises from the need to account for the unique characteristics of GPU computation. Furthermore, the introduction of new machine learning models requires additional work to adapt our cost model accordingly. These challenges highlight areas for future research and improvement. Nevertheless, the benefits of our model in terms of accuracy and efficiency make it a valuable contribution to the field of factorized machine learning.
\todo{Unsure how it generalizes to other LA runtimes.}
\section{Future Work}
\label{sec:7-future-work}
As we look forward, there are several promising directions for future work. This thesis has demonstrated the value of factorized machine learning in real-world settings, but to facilitate its adoption in the industry, steps need to be taken. One such step could be the integration of factorized machine learning models into widely used frameworks like TensorFlow and PyTorch. This would not only enhance the practicality and reach of factorized machine learning but also open up opportunities for investigating cost estimation. Given the maturity of these frameworks and the extensive research already conducted to optimize their training processes, this could significantly advance our understanding of cost dynamics in factorized machine learning.

Moreover, this integration could also enable the exploration of factorized machine learning in a distributed setting. This would be a significant advancement, as it would allow us to leverage the power of distributed computing to further enhance the efficiency and scalability of factorized machine learning models. This could potentially lead to breakthroughs in handling larger datasets and more complex computations, thus broadening the scope and impact of factorized machine learning.

In terms of future steps specifically for cost estimation in factorized machine learning, we propose two main areas of focus. Firstly, other types of cost models could be explored, such as those based on micro benchmarking. This involves conducting performance tests on individual operations before running a training scenario. The insights gained from these benchmarks could then be used to make informed decisions between materialization and factorization. This could improve accuracy as the actual datasets can be used for these benchmarks. However, consideration must be given to keeping the overhead of such an approach low. Another direction that could complement our proposed approach is the exploration of online training. This would involve continuously updating the model as new scenarios are tested, leading to a continuously improving model. Such an approach would be particularly valuable in a real-world factorized machine learning framework.

Second, we could expand on the profiling experiments conducted in this thesis. By conducting more extensive profiling experiments, on model training scenarios instead of individual operators, we could gain a deeper understanding of the cost dynamics of factorized machine learning. This would allow us to refine our cost model further and potentially identify new cost factors that could be incorporated into the model. However, this would require a significant investment in time and resources, as profiling experiments are time-consuming and computationally expensive.

Despite the challenges, such as higher complexity and the need for additional work with the introduction of new machine learning models, our model makes a valuable contribution to the field in terms of accuracy and efficiency. These future directions highlight the potential for continued refinement and expansion of our cost model, contributing to the ongoing advancement of factorized machine learning.