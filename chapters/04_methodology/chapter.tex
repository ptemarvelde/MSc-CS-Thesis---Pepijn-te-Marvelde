% !TEX root = ../../main.tex

\chapter{Methodology}

\label{chapter:methodology}

This chapter details the methodology used to arrive at an accurate cost prediction for Factorized ML. First, we introduce the problem setting in \autoref{sec:4-problem-setting}, focussing on explaining the choices for independent variables. Then, we introduce the proposed cost estimation models in \autoref{sec:4-cost-estimation}.

\section{Problem Setting}
\label{sec:4-problem-setting}

As this is an empirical study the focus is on carried out experiments and their results. Therefore, it is extremely important to design these experiments well. This starts with a look back at the problem we are trying to solve, after which we can say precisely what is needed to solve this problem. The experiments are then designed to gather the necessary results to come to a fitting solution.

\subsection{Independent Variables}
To reiterate, the goal of this thesis is to create an accurate, generalizable, cost estimator for choosing whether Factorized or Materialized ML is optimal for a given ML scenario. For this we need knowledge on which factors influence this decision. Previous works have already identified the three dimensions that impact the cost: \emph{data characteristics} \cite{morpheus, amalur,schijndel_cost_estimation}, \emph{hardware characteristics} \cite{orion_learning_gen_lin_models}, and \emph{model-type \& -hyperparameters} \cite{amalur,schijndel_cost_estimation}. Here we detail the independent variables that are varied in this study.

\subsubsection{Data}
Literature has identified the effect of some data characteristics on factorized learning. Morpheus \cite{morpheus} Argues that the largest of these factors is the relation between the number of columns/row between the Source tables and Target table. They extend on this notion in \cite{MorpheusFI} also showing that sparsity has grave implications for the F/M trade-off. Amalur \cite{amalur} merges a larger range of data characteristics into a single metric that estimates the cost of training a model in FLOPs. We include the characteristics mentioned in this study complemented by a new set of features. We also include a larger range of variation per data characteristics allowing for more insights into the relation of these data characteristics and the training cost. All considered data characteristics are detailed in \autoref{tab:4-data_chars}.

\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}[t]
    \centering
    \begin{tabular}{p{0.16\linewidth}p{0.09\linewidth}p{0.23\linewidth}p{0.4\linewidth}}
        \toprule
        Independent Variable       & Symbol   & Explanation                                                        & Reason for choice                                                                                                           \\ \midrule \midrule
        Sparsity                   & $e$      & Fraction of zero-valued elements                                   & Impacts the number of computation needed for sparse implementations. \cite{MorpheusFI, morpheus, schijndel_cost_estimation} \\
        Table Size (rows/ columns) & $c/r$    & Dimensions of tables. Both Target and Source.                      & \cite{morpheus}                                                                                                             \\
        Tuple ratio                & $\rho$   & Ratio of rows from $S_{2\cdots k}$ in $S_1$                        & Influences the number of redundant operations when computing a model\cite{morpheus}                                         \\
        Feature ratio              & $\tau$   & Ratio of columns from $S_{2\cdots k}$ in $S_1$                     & Influences the number of redundant operations when computing a model\cite{morpheus}                                         \\
        Join Type                  & $j_t$    & The join type used to join the source tables to the target table   & \cite{schijndel_cost_estimation}                                                                                            \\
        Selectivity                & $\sigma$ & The fraction of rows from $S_{1\cdots k}$ that are included in $T$ & Can be used to estimate the computational redundancy between F/M \cite{MorpheusFI}                                          \\
        \bottomrule
    \end{tabular}
    \caption[Overview of data related features varied in this study]{Overview of data related features varied in this study. A reference in the 'Reason for choice' column denotes this feature is either used in the cost estimation rule in that publication, or the publication has a thorough analysis showing the impact of this feature on runtime.}
    \label{tab:4-data_chars}
\end{table}
\endgroup

\subsubsection{Hardware}
\label{subsubsec:4-hardware}
This section answers how this thesis fills \emph{RG.2} by addressing the hardware characteristics. The hardware characteristics are the second dimension that impacts the cost of training a model. The hardware characteristics are varied in this study to capture the effect of these characteristics on the cost of training a model. The overarching split in hardware is between CPU and GPU. This thesis puts more emphasis on GPUs as they are the most commonly used hardware for training ML models. To also allow for a comparison between the two, we include CPU as well, but with a smaller range of variation. We only test varying degrees of parallelism by varying the number of cores. Hardware characteristics linked to GPUs are varied through experiments on different GPU types and architectures. By varying the GPU types used we capture the effect of the following variables:
\begin{itemize}
    \item Number of Streaming Processors
    \item Number of compute cores, clock speeds and floating point processing power
    \item Cache characteristics (L1, L2 size \& bandwidth)
    \item Memory characteristics (bandwidth, frequency)
    \item GPU architecture
\end{itemize}
The actual values for these variables, and exact GPU types used, are shown in \autoref{appendix:gpu-characteristics}, and the \textit{GPU Architectures} are detailed more extensively in the next paragraph.

\paragraph{GPU Architectures}
We purposefully selected a range of GPU architectures to capture metrics with different characteristics. Both older (Pascal, 2016) and newer (Ampere, 2020) architectures are included in an effort to create a cost estimator not limited to a single generation of hardware. Including only GPUs from a single generation would limit the generalizability of the cost estimator as they use the same architecture, i.e., they use similar Streaming Multiprocessors and Cache layouts.


\subsubsection{Model}
The model characteristics are only varied by choosing four different models: Linear- and Logistic-Regression, Gaussian Non-negative Matrix Factorization \& K-Means Clustering. To prevent the number of combinations between independent variables from exploding we choose to not vary any hyperparameters such $k$ in K-Means or $r$ in G-NMF. However, we do include numerous features that capture changes that would also be captured by varying those hyperparameters.

The most important of those features is the complexity of the model, i.e., the number of operations needed to train a model. The previously mentioned hyperparameters are parameters of the function to compute this feature, thus we believe our cost models will still be able to accurately predict runtime for different hyperparameter settings, as the complexity (ratio) has already been shown to be a capable predictor for the F/M trade-off. Also, because a lot of emphasis is put on capturing the effect of the other independent variables on the cost of singular operators, we believe that the cost models will also be able to generalize well to totally new ML models.

\subsection{Dependent Variables}

The dependent variable in this study is the cost of training a model, expressed as \textbf{training time}. The goal of the cost estimators is to pick the fastest method for training a model. This is why we use training time as the dependent variable.

Various \textbf{profiling metrics} are also collected to capture the cost of training a model. These metrics are used to calculate the cost of each operation in the training process. Through micro-benchmarks, run with a representative (sub)range of our independent variables, we find how these variables affect how computations are carried out on the GPU. The collected metrics are shown in \autoref{tab:4-profiling-metrics}. These metrics were chosen, so we can investigate the effect of the independent variables on the cost of performing an operation. They allow us to calculate the total time taken for computation and memory ($ops:byte$) and reason about what changes in the independent variables cause the GPUs to be used more efficiently. This likely also affects the F/M trade-off.

\begin{table}[t]
    \begin{tabular}{lll}
        \toprule
        Section Name                  & Metric Name                          & Metric Unit  \\
        \midrule\midrule
        Command line profiler metrics & \underline{dram\_\_bytes\_read.sum}  & byte         \\
                                      & \underline{dram\_\_bytes\_write.sum} & byte         \\
        GPU Speed Of Light Throughput & \underline{DRAM Frequency}           & cycle/second \\
                                      & \textbf{SM Frequency}                & cycle/second \\
                                      & \textbf{Elapsed Cycles}              & cycle        \\
                                      & \underline{Memory Throughput}        & \%           \\
                                      & \underline{DRAM Throughput}          & \%           \\
                                      & Duration                             & nsecond      \\
                                      & \underline{L1 Cache Throughput}      & \%           \\
                                      & \underline{L2 Cache Throughput}      & \%           \\
                                      & \textbf{SM Active Cycles}            & cycle        \\
                                      & \textbf{Compute (SM) Throughput}     & \%           \\
        Memory Workload Analysis      & \underline{Memory Throughput}        & byte/second  \\
                                      & \underline{Mem Busy}                 & \%           \\
                                      & \underline{Max Bandwidth}            & \%           \\
                                      & \underline{L1 Hit Rate}              & \%           \\
                                      & \underline{L2 Hit Rate}              & \%           \\
                                      & \underline{Mem Pipes Busy}           & \%           \\
        \bottomrule
    \end{tabular}
    \caption[Collected profiling metrics and their explanation]{Collected profiling metrics and their explanation. Metrics related to compute cost are \textbf{bold}, those related to memory cost are \underline{underlined}.}
    \label{tab:4-profiling-metrics}
\end{table}


\section{Cost Estimation}
\label{sec:4-cost-estimation}
\todo{\Large Improve after finishing \autoref{chapter:cost-estimation}}

This section introduces the ideas behind the cost models, which are explained in more detail in \autoref{chapter:cost-estimation}. The first, analytical, model is a formula derived from the actual cost of the operations. Due to its simplicity it is highly explainable, but will likely perform worse than more complex methods. Therefore, the next models are ML-based solutions. The statistical model uses a logistic regression to make its decision. Compared to the first model it is easier to include more features causing this model to have a larger decision space. The third, deep learning, model uses a neural network to make its predictions. It is the least explainable of cost estimators, but can capture the most complex interactions between features. Finally, the hybrid model combines the knowledge gathered from the previous cost estimators and applies it in a model like was done in \cite{halide_cost_model}.

\subsection{Analytical}
The analytical model is a deterministic model constructed by examining the operations performed by the learning algorithm. This model is based on a formula derived from the actual cost of the operations. This formula consists of the crucial factors of the algorithm, e.g., the number of matrix multiplications. For instance, if an algorithm performs an addition and two multiplications, the formula for this would be $ADD + 2MULT$. The actual cost values for $ADD$ and $MULT$ are determined through micro benchmarks, and these values are then used to complete the formula and obtain the final model. In our case these operations are the linear algebra operations performed as part of the machine learning model training. So, through capturing the profiling metrics mentioned in \autoref{tab:4-profiling-metrics} we can calculate the cost of each operation in the training process. The simplified calculation for this would be:

\vspace{-0.5cm}
\begin{align*}
    \text{{Operator cost}} & =  \underbrace{\# \text{{instructions}} \times \text{{instruction latency}}}_{\text{{Processor Cost}}}                                                         \\
                           & + \underbrace{\text{{hit rate}} \times \text{{cache latency}} \times \text{{cache bandwidth}} \times \text{{amount read}}}_{\text{{Cache Memory Access Cost}}} \\
                           & + \underbrace{(1 - \text{{hit rate}}) \times \text{{RAM latency}} \times \text{{RAM bandwidth}} \times \text{{amount read}}}_{\text{{RAM Memory Access Cost}}}
\end{align*}

By profiling through a large range of the chosen independent variables we can estimate the effect of those variables on the factors in this formula, like the hit rate. By incorporating this in the analytical model we can create a highly explainable model that can be used to estimate the cost of different approaches.

\subsection{Statistical}
The statistical model uses reasoning and analysis of performance-impacting factors to estimate the optimal approach. This model is based on empirical data and uses logistic regression to make predictions. The model considers various features known to impact performance, such as the size of the input data, the algorithm's complexity, and the hardware configuration. By analysing the relationships between these features and the actual runtime of the algorithm, the statistical model can make accurate, and explainable, predictions about the cost of different approaches.

\subsection{XGBoost}
To check whether the previous models, such as the analytical and statistical models, are too simplistic we include this more complex estimator. If this model significantly outperforms the other models, it indicates that there are more complex feature interactions happening which the other models have failed to capture. This estimator can model these complex interactions from the data and make more accurate predictions about the cost of different approaches. The major drawback being that this model is less explainable than the other models.
\todo{What Neural network?}

\subsection{Hybrid}
Finally, the knowledge gathered from the previous cost estimators is combined and applied in a hybrid model. Much like the optimization model from \cite{halide_cost_model} this model aims to create a highly accurate, but explainable, model. This is achieved by combining intricate knowledge of the most impactful features and their interactions with a neural network to estimate the weights of its (combined) features.

\todo{Show architecture of hybrid model.}

% \subsection{? Cost Estimation at Training Time?}
% \todo{
%   This is a section I am not sure about. I think it would be interesting to see if we can estimate the cost of training a model at training time. This would allow for a more dynamic approach to the cost estimation. This would entail capturing some metrics (e.g., time taken for row sum/LMM) of the dataset at runtime and comparing cost between F/M.
%   Either fine-tuning a pre-trained model, or without a pretrained model.
%   Caveats:
%   \begin{itemize}
%     \item Needs to be fast, can't diminish the time won by choosing F over M
%   \end{itemize}
% }