% !TEX root = ../../main.tex

\chapter{Methodology}

\label{chapter:methodology}

This chapter outlines the methodology used to get an accurate cost prediction for factorized Machine Learning. We start by introducing the problem setting in \autoref{sec:4-problem-setting}, where we explain the choices for the independent variables. Following that, we present the proposed cost estimation models in \autoref{sec:4-cost-estimation}.

\section{Problem Setting}
\label{sec:4-problem-setting}

As this is an empirical study, the focus is on carried out experiments and their results. Therefore, it is extremely important to design these experiments well. This starts with a look back at the problem we are trying to solve, after which we can say precisely what is needed to solve this problem. The experiments are then designed to gather the necessary results to arrive at a fitting solution.

Given that this is an empirical study, the emphasis is on the experiments conducted and their respective outcomes. As such, the design of these experiments is of great importance. This process starts with a retrospective examination of the problem at hand, which allows us to define what is required to address this problem. The experiments are then structured to collect the essential data, leading us towards an appropriate solution.

\subsection{Independent Variables}
To reiterate, the objective of this thesis is to develop an accurate and generalizable cost estimator to determine whether factorized or materialized Machine Learning is the optimal choice for a given Machine Learning scenario. This requires understanding the factors that influence this decision. Prior research has already identified the three dimensions that impact cost: data characteristics \cite{morpheus, amalur,schijndel_cost_estimation}, hardware characteristics \cite{orion_learning_gen_lin_models}, and model type \& hyperparameters \cite{amalur,schijndel_cost_estimation}. In this section, we elaborate on the independent variables that are manipulated in this study.

\subsubsection{Data}
Existing literature has recognized the impact of certain data characteristics on factorized learning. Morpheus \cite{morpheus} argues that the most significant of these factors is the relationship between the number of columns/rows in the Source tables and the Target table. The authors expand on this notion in \cite{MorpheusFI}, demonstrating that sparsity has serious implications for the factorization vs. materialization (F/M) trade-off. In this study, we incorporate the characteristics mentioned in previous research, supplemented by a new set of features. We also include a wider range of variation for each data characteristic, which allows for more insight into the relationship between these data characteristics and the training cost. The data characteristics considered in this study are detailed in \autoref{tab:4-data_chars}.

\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}[t]
    \centering
    \begin{tabular}{p{0.16\linewidth}p{0.09\linewidth}p{0.23\linewidth}p{0.4\linewidth}}
        \toprule
        Independent Variable       & Symbol   & Explanation                                                        & Reason for choice                                                                                                           \\ \midrule \midrule
        Sparsity                   & $e$      & Fraction of zero-valued elements                                   & Impacts the number of computation needed for sparse implementations. \cite{MorpheusFI, morpheus, schijndel_cost_estimation} \\
        Table Size (rows/ columns) & $c/r$    & Dimensions of tables. Both Target and Source.                      & \cite{morpheus, schijndel_cost_estimation}                                                                                                             \\
        Tuple ratio                & $\rho$   & Ratio of rows from $S_{2\cdots k}$ in $S_1$                        & Influences the number of redundant operations when computing a model\cite{morpheus, schijndel_cost_estimation}                                         \\
        Feature ratio              & $\tau$   & Ratio of columns from $S_{2\cdots k}$ in $S_1$                     & Influences the number of redundant operations when computing a model\cite{morpheus, schijndel_cost_estimation}                                         \\
        Join Type                  & $j_t$    & The join type used to join the source tables to the target table   & \cite{schijndel_cost_estimation}                                                                                            \\
        Selectivity                & $\sigma$ & The fraction of rows from $S_{1\cdots k}$ that are included in $T$ & Can be used to estimate the computational redundancy between F/M \cite{MorpheusFI, schijndel_cost_estimation}                                          \\
        \bottomrule
    \end{tabular}
    \caption[Overview of data related features varied in this study]{Overview of data related features varied in this study. A reference in the column 'Reason for choice' denotes this feature is either used in the cost estimation rule in that publication, or the publication has a thorough analysis showing the impact of this feature on runtime.}
    \label{tab:4-data_chars}
\end{table}
\endgroup

\subsubsection{Hardware}
\label{subsubsec:4-hardware}
This section answers how this thesis addresses \emph{RG.2} by addressing the hardware characteristics that represent the second dimension that influences the cost of model training. In this study, we vary these characteristics to understand their impact on training cost. The primary distinction in hardware is between CPU and GPU. This thesis places a greater focus on GPUs, given their prevalent use in the training of ML models. However, to facilitate a comparison, we also include CPUs, albeit with a lesser degree of variation. We experiment with different degrees of parallelism by altering the number of cores. As for the characteristics associated with GPUs, we vary them through experiments on different GPU types and architectures. By changing the types of GPU used, we aim to understand the effect of the following variables.
\begin{itemize}
    \item Number of Streaming Processors
    \item Number of compute cores, clock speeds and floating point processing power
    \item Cache characteristics (L1, L2 size \& bandwidth)
    \item Memory characteristics (bandwidth, frequency)
    \item GPU architecture
\end{itemize}
The specific values for these variables, along with the exact types of GPU used, are provided in \autoref{appendix:gpu-characteristics}. A more comprehensive discussion of \textit{GPU Architectures} is presented in the next paragraph.

\paragraph{GPU Architectures}
We purposefully selected a range of GPU architectures to capture metrics with different characteristics. Both older (Pascal, 2016) and newer (Ampere, 2020) architectures are included in an effort to create a cost estimator not limited to a single generation of hardware. Including only GPUs from a single generation would limit the generalizability of the cost estimator as they use the same architecture, i.e., they use similar Streaming Multiprocessors and Cache layouts.

We intentionally chose a variety of GPU architectures to capture metrics of GPUs with different characteristics. Both older (Pascal, 2016) and newer (Ampere, 2020) architectures are incorporated with the aim of developing a cost estimator that is not confined to a single generation of hardware. Restricting the study to GPUs from a single generation would constrain the generalizability of the cost estimator, as they employ the same architecture, i.e., they utilize similar Streaming Multiprocessors and Cache layouts.

\subsubsection{Model}
The characteristics of the model are varied by selecting four distinct models: Linear Regression, Logistic Regression, Gaussian Non-negative Matrix Factorization, and K-Means Clustering. To avoid an exponential increase in the number of combinations of independent variables, we opt not to vary certain hyperparameters, such as k in K-Means or r in G-NMF. Nevertheless, we incorporate a multitude of features that encapsulate the variations that would otherwise be captured by altering these hyperparameters.

Among these features is the complexity of the model, which refers to the number of operations required to train a model. The hyperparameters mentioned above are arguments for the function used to compute this feature. Therefore, we hypothesize that our cost models will still be capable of accurately predicting runtime for different hyperparameter settings, given that the complexity (ratio) has been demonstrated to be an effective predictor for the F/M trade-off. Furthermore, due to the significant emphasis on capturing the impact of the other independent variables on the cost of individual operators, we anticipate that the cost models will also exhibit good generalizability to entirely new Machine Learning models.

One of those features is the complexity of the model, that is, the number of operations needed to train a model. The previously mentioned hyperparameters are parameters of the function to compute this feature; thus, we assume our cost models will still be able to accurately predict runtime for different hyperparameter settings, as the complexity (ratio) has already been shown to be a capable predictor for the F/M trade-off.

\subsection{Dependent Variables}
In this study, the dependent variable is the cost of training a model, which is represented as the training time. The objective of the cost estimators is to identify the most efficient method for training a model, which is why we utilize training time as the dependent variable.

A variety of \textbf{profiling metrics} is also gathered to quantify the cost of training a model. These metrics are used to calculate the cost associated with each operation in the training process. By conducting micro-benchmarks within a representative subrange of our independent variables, we discern how these variables influence the execution of computations on the GPU. The collected metrics are shown in \autoref{tab:4-profiling-metrics}. They allow us to calculate the total time taken for computation and memory ($ops:byte$), and allow us to infer how changes in the independent variables lead to more efficient utilization of GPUs. This likely also affects the F/M trade-off.

\begin{table}[t]
    \begin{tabular}{lll}
        \toprule
        Section Name                  & Metric Name                          & Metric Unit  \\
        \midrule\midrule
        Command line profiler metrics & \underline{dram\_\_bytes\_read.sum}  & byte         \\
                                      & \underline{dram\_\_bytes\_write.sum} & byte         \\
        GPU Speed Of Light Throughput & \underline{DRAM Frequency}           & cycle/second \\
                                      & \textbf{SM Frequency}                & cycle/second \\
                                      & \textbf{Elapsed Cycles}              & cycle        \\
                                      & \underline{Memory Throughput}        & \%           \\
                                      & \underline{DRAM Throughput}          & \%           \\
                                      & Duration                             & nsecond      \\
                                      & \underline{L1 Cache Throughput}      & \%           \\
                                      & \underline{L2 Cache Throughput}      & \%           \\
                                      & \textbf{SM Active Cycles}            & cycle        \\
                                      & \textbf{Compute (SM) Throughput}     & \%           \\
        Memory Workload Analysis      & \underline{Memory Throughput}        & byte/second  \\
                                      & \underline{Mem Busy}                 & \%           \\
                                      & \underline{Max Bandwidth}            & \%           \\
                                      & \underline{L1 Hit Rate}              & \%           \\
                                      & \underline{L2 Hit Rate}              & \%           \\
                                      & \underline{Mem Pipes Busy}           & \%           \\
        \bottomrule
    \end{tabular}
    \caption[Collected profiling metrics and their explanation]{Collected profiling metrics and their explanation. The metrics related to the compute cost are \textbf{bold}, those related to the memory cost are \underline{underlined}.}
    \label{tab:4-profiling-metrics}
\end{table}


\section{Cost Estimation}
\label{sec:4-cost-estimation}
This section provides an introduction to the concepts underlying the cost models, which are elaborated in \autoref{chapter:cost-estimation}. The first model, termed the analytical model, is a formula derived from the actual cost of the operations. Its simplicity lends itself to high explainability, but it may not perform as well as more complex methods due to the impracticality of incorporating all effects of the independent variables. Hence, the subsequent models are solutions based on Machine Learning. The statistical model employs linear regression for the prediction of training time. Compared to the first model, it can incorporate more features, thus broadening its decision space. The third model, a tree-boosting model, uses a set of regression trees for its predictions. Although it is the least explainable among the cost estimators, it can capture the most intricate interactions between features. Finally, the hybrid model integrates the insights derived from the preceding cost estimators and leverages the strengths of the most effective estimators to build a superior estimator.

\subsection{Analytical}
The analytical model is a deterministic model, constructed by examining the operations executed by the learning algorithm. This model is based on a formula derived from the actual cost of the operations. This formula encapsulates the critical factors of the algorithm, such as the number of matrix multiplications. For example, if an algorithm performs one addition and two multiplications, the formula for this would be $ADD + 2MULT$. The actual cost values for $ADD$ and $MULT$ are determined through micro benchmarks, and these values are then used to complete the formula and obtain the final model. In our context, these operations are the linear algebra operations performed as part of the machine learning model training. Therefore, by capturing the profiling metrics mentioned in \autoref{tab:4-profiling-metrics}, we can compute the cost of each operation in the training process. As will be demonstrated in \autoref{chapter:cost-estimation}, the memory cost of an operation is a reliable predictor for its total runtime. This is the reason we concentrate on the memory cost of an operation in the analytical model.

% The simplified calculation for this would be:

% \vspace{-0.5cm}
% \begin{align*}
%     \text{{Operator cost}} & =  \underbrace{\# \text{{instructions}} \times \text{{instruction latency}}}_{\text{{Processor Cost}}}                                                         \\
%                            & + \underbrace{\text{{hit rate}} \times \text{{cache latency}} \times \text{{cache bandwidth}} \times \text{{amount read}}}_{\text{{Cache Memory Access Cost}}} \\
%                            & + \underbrace{(1 - \text{{hit rate}}) \times \text{{RAM latency}} \times \text{{RAM bandwidth}} \times \text{{amount read}}}_{\text{{RAM Memory Access Cost}}}
% \end{align*}

By profiling across a broad spectrum of the selected independent variables, we can estimate their impact on memory cost. By integrating this information into the analytical model, we can construct a highly interpretable model that can be utilized to estimate the cost of various approaches.

\subsection{Statistical}
The statistical model employs an analysis of factors that impact performance to estimate the optimal approach. This model is grounded in empirical data and utilizes linear regression to predict runtime and make a decision between factorization and materialization. It takes into account various features known to influence performance, including the size of the input data, the complexity of the algorithm, and the hardware configuration. By examining the relationships between these features and the actual runtime of the algorithm, the statistical model can make accurate and interpretable predictions about the cost of different approaches.

\subsection{XGBoost}
To evaluate whether the preceding models, such as the analytical and statistical models, are overly simplistic, we incorporate this more intricate estimator. If this model markedly surpasses the performance of the other models, it suggests that more complex feature interactions occur that the other models have not been able to capture. This estimator is capable of modeling these complex interactions from the data and making more precise predictions about the cost of different approaches. The primary disadvantage is that this model is less interpretable than the other models.

\subsection{Hybrid}
Finally, the insights obtained from the preceding cost estimators are amalgamated and used in a hybrid model. By leveraging the strengths of multiple models, the hybrid model is able to make more accurate predictions about the cost of different approaches.