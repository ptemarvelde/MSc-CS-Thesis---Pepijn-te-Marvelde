% !TEX root = ../../main.tex

\chapter{Methodology}

\label{chapter:methodology}

This chapter details the methodology used to arrive at an accurate cost prediction for Factorized ML. First, we introduce the problem setting in \autoref{sec:4-problem-setting}, focussing on explaining the choices for independent variables. Then, we introduce the proposed cost estimation models in \autoref{sec:4-cost-estimation}.

\section{Problem Setting}
\label{sec:4-problem-setting}
\todo{Detail dependent \& independent variables}

As this is an empirical study the focus is on carried out experiments and their results. Therefore, it is extremely important to design these experiments well. This starts with a look back at the problem we are trying to solve, after which we can say precisely what is needed to solve this problem. The experiments are then designed to gather the necessary results to come to a fitting solution.

\subsection{Independent Variables}
To reiterate, the goal of this thesis is to create an accurate, generalizable, cost estimator for choosing whether Factorized or Materialized ML is optimal for a given ML scenario. For this we need knowledge on which factors influence this decision. Previous works have already identified the three dimensions that impact the cost: \emph{data characteristics} \cite{morpheus, amalur,schijndel_cost_estimation}, \emph{hardware characteristics} \cite{orion_learning_gen_lin_models}, and \emph{model-type \& -hyperparameters} \cite{amalur,schijndel_cost_estimation}.

\subsubsection{Data}
Literature has identified the effect of some data characteristics on factorized learning. Morpheus \cite{morpheus} Argues that the largest of these factors is the relation between the number of columns/row between the Source tables and Target table. They extend on this notion in \cite{MorpheusFIEnablingOptimizingNonlinear2019} also showing that sparsity has grave implications for the F/M trade off. Amalur \cite{amalur} merges a larger range of data characteristics into a single metric that estimates the cost of training a model in FLOPs. We include the characteristics mentioned in this study complemented by a new set of features. We also include a larger range of variation per data characteristics allowing for more insights into the relation of these data characteristics and the training cost. All considered data characteristics are detailed in \autoref{tab:4-data_chars}.

\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}[ht]
    \centering
    \begin{tabular}{p{0.15\linewidth}p{0.07\linewidth}p{0.23\linewidth}p{0.4\linewidth}}
        \toprule
        Independent Variable       & Symbol   & Explanation                                                        & Reason for choice                                                                                                                                          \\ \midrule \midrule
        Sparsity                   & $e$      & Fraction of zero-valued elements                                   & Impacts the number of computation needed for sparse implementations. \cite{MorpheusFIEnablingOptimizingNonlinear2019, morpheus, schijndel_cost_estimation} \\
        Table Size (rows/ columns) & $c/r$    & Dimensions of tables. Both Target and Source.                      & \cite{morpheus}                                                                                                                                            \\
        Tuple ratio                & $\rho$   & Ratio of rows/columns from $S_{2\cdots k}$ in $S_1$                & Influences the number of redundant operations when computing a model\cite{morpheus}                                                                        \\
        Feature ratio              & $\tau$   & Ratio of columns from $S_{2\cdots k}$ in $S_1$                     & Influences the number of redundant operations when computing a model\cite{morpheus}                                                                        \\
        Join Type                  & $j_T$    & The join type used to join the source tables to the target table   & \cite{schijndel_cost_estimation}                                                                                                                           \\
        Selectivity                & $\sigma$ & The fraction of rows from $S_{1\cdots k}$ that are included in $T$ & Can be used to estimate the computational redundancy between F/M \cite{MorpheusFIEnablingOptimizingNonlinear2019}                                          \\
        \bottomrule
    \end{tabular}
    \caption{Overview of data related features varied in this study. A reference in the 'Reason for choice' column denotes this feature is either used in the cost estimation rule in that publication, or the publication has a thorough analysis showing the impact of this feature on runtime.}
    \label{tab:4-data_chars}
\end{table}
\endgroup

\subsubsection{Hardware}
\paragraph{GPU Architectures}

\subsubsection{Model}
\todo{
    \begin{enumerate}
        \item Model type
        \item Model hyperparameters (not varied so leave out?)
        \item complexity (ratio)
    \end{enumerate}
}

\subsection{Dependent Variables}

\subsubsection{Training Cost}
\todo{Training time}

\subsubsection{Micro Benchmarking Hardware Metrics}
\todo{\begin{itemize}
        \item Why do we need to capture metrics?
        \item Why these metrics, how are they used in the cost estimation.
              \begin{itemize}
                  \item Calculate arithmetic complexity and ops:byte ratio. \cite{nvidia-gpu-performance:online}
              \end{itemize}
    \end{itemize}
}

\begin{table}[ht]
    \begin{tabular}{lll}
        \toprule
        Section Name                  & Metric Name                          & Metric Unit  \\
        \midrule\midrule
        Command line profiler metrics & \underline{dram\_\_bytes\_read.sum}  & byte         \\
                                      & \underline{dram\_\_bytes\_write.sum} & byte         \\
        GPU Speed Of Light Throughput & \underline{DRAM Frequency}           & cycle/second \\
                                      & \textbf{SM Frequency}                & cycle/second \\
                                      & \textbf{Elapsed Cycles}              & cycle        \\
                                      & \underline{Memory Throughput}        & \%           \\
                                      & \underline{DRAM Throughput}          & \%           \\
                                      & Duration                             & nsecond      \\
                                      & \underline{L1 Cache Throughput}      & \%           \\
                                      & \underline{L2 Cache Throughput}      & \%           \\
                                      & \textbf{SM Active Cycles}            & cycle        \\
                                      & \textbf{Compute (SM) Throughput}     & \%           \\
        Memory Workload Analysis      & \underline{Memory Throughput}        & byte/second  \\
                                      & \underline{Mem Busy}                 & \%           \\
                                      & \underline{Max Bandwidth}            & \%           \\
                                      & \underline{L1 Hit Rate}              & \%           \\
                                      & \underline{L2 Hit Rate}              & \%           \\
                                      & \underline{Mem Pipes Busy}           & \%           \\
        \bottomrule
    \end{tabular}
    \caption{Table showing the collected profiling metrics and their explanation. Metrics related to compute cost are \textbf{bold}, those related to memory cost are \underline{underlined}.}
    \label{tab:4-profiling-metrics}
\end{table}


\section{Cost Estimation}
\label{sec:4-cost-estimation}
\todo{Introduce different cost models (also include architecture figure, here or somewhere else)}

\subsection{Analytical}
The analytical model is a deterministic model constructed by examining the operations performed by the learning algorithm. This model is based on a formula derived from the actual cost of the operations. This formula consists of the crucial factors of the algorithm, e.g., the number of matrix multiplication. For instance, if an algorithm performs an addition and two multiplications, the formula for this would be $ADD + 2MULT$. The actual cost values for $ADD$ and $MULT$ are determined through micro benchmarks, and these values are then used to complete the formula and obtain the final model.

\subsection{Statistical}
The statistical model uses reasoning and analysis of performance-impacting factors to estimate the optimal approach. This model is based on empirical data and uses logistic regression to make predictions. The model considers various features known to impact performance, such as the size of the input data, the algorithm's complexity, and the hardware configuration. By analyzing the relationships between these features and the actual runtime of the algorithm, the statistical model can make accurate, and explainable, predictions about the cost of different approaches.

\subsection{Deep Learning}
This cost model uses a neural network, to make its predictions. This model is included to check whether the previous models, such as the analytical and statistical models, are too simplistic. If the black box model significantly outperforms the other models, it indicates that there are more complex feature interactions happening which the other models have failed to capture. This estimator can model these complex interactions from the data and make more accurate predictions about the cost of different approaches.

\subsection{Hybrid}
Finally, the knowledge gathered from the previous cost estimators is combined and applied in a hybrid model. Much like the optimization model from \cite{halide_cost_model} this model aims to create a highly accurate, but explainable, model. This is achieved by combining intricate knowledge of the most impactful features and their interactions with a neural network to estimate the weights of its (combined) features.

\todo{Show architecture of hybrid model.}

\subsection{? Cost Estimation at Training Time?}
\todo{
    This is a section I am not sure about. I think it would be interesting to see if we can estimate the cost of training a model at training time. This would allow for a more dynamic approach to the cost estimation. This would entail capturing some metrics (e.g., time taken for row sum/LMM) of the dataset at runtime and comparing cost between F/M.
    Either fine-tuning a pre-trained model, or without a pretrained model.
    Caveats:
    \begin{itemize}
        \item Needs to be fast, can't diminish the time won by choosing F over M
    \end{itemize}
}
