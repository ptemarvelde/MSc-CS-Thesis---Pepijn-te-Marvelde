% !TEX root = ../../main.tex

\chapter{Methodology}

\label{chapter:methodology}

This chapter outlines the methodology used to get an accurate cost prediction for factorized Machine Learning. We start by introducing the problem setting in \autoref{sec:4-problem-setting}, where we explain the choices for the independent variables. Next, we present the proposed cost estimation models in \autoref{sec:4-cost-estimation}.

\section{Problem Setting}
\label{sec:4-problem-setting}

As this is an empirical study, the focus is on carried out experiments and their results. Therefore, it is extremely important to design these experiments well. This starts with a look back at the problem we are trying to solve, after which we can say precisely what is needed to solve this problem. The experiments are then designed to gather the necessary results to arrive at a fitting solution.

\subsection{Independent Variables}
To reiterate, the objective of this thesis is to develop an accurate and generalizable cost model to determine whether factorized or materialized Machine Learning is the optimal choice for a given Machine Learning scenario. This requires understanding the factors that influence this decision. Prior research has already identified the three dimensions that impact cost: data characteristics~\cite{morpheus, amalur,amalur_tkde24}, hardware characteristics~\cite{orion_learning_gen_lin_models}, and model type \& hyperparameters~\cite{amalur,amalur_tkde24}. In this section, we elaborate on the independent variables that are taken into account in this study.

\subsubsection{Data}
Existing literature has recognized the impact of certain data characteristics on factorized learning. Morpheus~\cite{morpheus} argues that the most significant of these factors is the relationship between the number of columns/rows in the Source tables and the Target table. The authors expand on this notion in~\cite{MorpheusFI}, demonstrating that sparsity has serious implications for the factorization vs materialization (F/M) trade-off. In this study, we incorporate the characteristics mentioned in previous research, supplemented by a new set of features. We also include a wider range of variation for each data characteristic, which allows for more insight into the relationship between these data characteristics and the training cost. The data considered characteristics are detailed in \autoref{tab:4-data_chars}.

\begingroup
\renewcommand{\arraystretch}{1.5}
\begin{table}[t]
  \centering
  \begin{tabular}{p{0.16\linewidth}p{0.09\linewidth}p{0.23\linewidth}p{0.4\linewidth}}
    \toprule
    Independent Variable       & Symbol   & Explanation                                                        & Reason for choice                                                                                               \\ \midrule \midrule
    Sparsity                   & $e$      & Fraction of zero-valued elements                                   & Impacts the number of computation needed for sparse implementations.~\cite{MorpheusFI, morpheus, amalur_tkde24} \\
    Table Size (rows/ columns) & $c/r$    & Dimensions of tables. Both Target and Source.                      & ~\cite{morpheus, amalur_tkde24}                                                                                 \\
    Tuple ratio                & $\rho$   & Ratio of rows from $S_{2\cdots k}$ in $S_1$                        & Influences the number of redundant operations when computing a model~\cite{morpheus, amalur_tkde24}             \\
    Feature ratio              & $\tau$   & Ratio of columns from $S_{2\cdots k}$ in $S_1$                     & Influences the number of redundant operations when computing a model~\cite{morpheus, amalur_tkde24}             \\
    Join type                  & $j_t$    & The join type used to join the source tables to the target table   & ~\cite{amalur_tkde24}                                                                                           \\
    Selectivity                & $\sigma$ & The fraction of rows from $S_{1\cdots k}$ that are included in $T$ & Can be used to estimate the computational redundancy between F/M~\cite{MorpheusFI, amalur_tkde24}               \\
    \bottomrule
  \end{tabular}
  \caption[Overview of data related features varied in this study]{Overview of data related features varied in this study. A reference in the column 'Reason for choice' denotes this feature is either used in the cost estimation rule in that publication, or the publication has a thorough analysis showing the impact of this feature on runtime.}
  \label{tab:4-data_chars}
\end{table}
\endgroup

\subsubsection{Hardware}
\label{subsubsec:4-hardware}
This section answers how this thesis addresses \emph{RG.2} by addressing the hardware characteristics that represent the second dimension that influences the cost of model training. In this study, we vary these characteristics to understand their impact on training cost. The primary distinction in hardware is between CPU and GPU. This thesis places a greater focus on GPUs, given their prevalent use in the training of ML models. However, to facilitate a comparison, we also include CPUs, albeit with a lesser degree of variation. We experiment with different degrees of parallelism by altering the number of cores. As for the characteristics associated with GPUs, we vary them through experiments on different GPU types and architectures. By changing the types of GPU used, we aim to understand the effect of the following variables.
\begin{itemize}
  \item Number of Streaming Processors
  \item Number of compute cores, clock speeds and floating point processing power
  \item Cache characteristics (L1, L2 size \& bandwidth)
  \item Memory characteristics (bandwidth, frequency)
  \item GPU architecture
\end{itemize}
The specific values for these variables, along with the exact types of GPU used, are provided in \autoref{appendix:gpu-characteristics}. A more comprehensive discussion of \textit{GPU Architectures} is presented in the next paragraph.

\paragraph{GPU Architectures}
We intentionally chose a variety of GPU architectures to capture metrics of GPUs with different characteristics. Both older (Pascal, 2016) and newer (Ampere, 2020) architectures are incorporated with the aim of developing a cost model that is not confined to a single generation of hardware. Restricting the study to GPUs from a single generation would constrain the generalizability of the cost model, as they employ the same architecture, i.e., they utilize similar Streaming Multiprocessors and Cache layouts.

\subsubsection{Model}
The characteristics of the model are varied by selecting four distinct models: linear regression, logistic regression, Gaussian Non-negative Matrix Factorization, and K-Means Clustering. To avoid an exponential increase in the number of combinations of independent variables, we opt not to vary certain hyperparameters, such as k in K-Means or r in G-NMF. Nevertheless, we incorporate a multitude of features that encapsulate the variations that would otherwise be captured by altering these hyperparameters.

One of those features is the complexity of the model, that is, the number of operations needed to train a model. The hyperparameters mentioned above are parameters of the function to compute this feature; thus, we assume that our cost models will still be able to accurately predict runtime for different hyperparameter settings, as the complexity (ratio) has already been shown to be a capable predictor for the F/M trade-off.

\subsection{Dependent Variables}
In this study, the dependent variable is the cost of training a model, which is represented as the training time. The objective of the cost models is to identify the most efficient method for training a model, which is why we utilize training time as the dependent variable.

A variety of \textbf{profiling metrics} is also gathered to quantify the cost of training a model. These metrics are used to calculate the cost associated with each operation in the training process. By conducting micro-benchmarks within a representative sub-range of our dependent variables, we discern how these variables influence the execution of computations on the GPU. The collected metrics are shown in \autoref{tab:4-profiling-metrics}. They allow us to calculate the total time taken for computation and memory ($ops:byte$), and allow us to infer how changes in the independent variables affect the utilization of GPUs and the F/M trade-off.


\begin{table}[t]
  \begin{tabular}{lll}
    \toprule
    Section name                  & Metric name                                & Metric unit  \\
    \midrule\midrule
    Command line profiler metrics & \textit{\texttt{dram\_\_bytes\_read.sum}}  & byte         \\
                                  & \textit{\texttt{dram\_\_bytes\_write.sum}} & byte         \\
    GPU speed of light throughput & Duration                                   & nsecond      \\
                                  & \textbf{SM frequency}                      & cycle/second \\
                                  & \textbf{Elapsed cycles}                    & cycle        \\
                                  & \textbf{SM active cycles}                  & cycle        \\
                                  & \textbf{Compute (SM) throughput}           & \%           \\
                                  & \textit{DRAM frequency}                    & cycle/second \\
                                  & \textit{Memory throughput}                 & \%           \\
                                  & \textit{DRAM throughput}                   & \%           \\
                                  & \textit{L1 cache throughput}               & \%           \\
                                  & \textit{L2 cache throughput}               & \%           \\
    Memory workload analysis      & \textit{Memory throughput}                 & byte/second  \\
                                  & \textit{Mem busy}                          & \%           \\
                                  & \textit{Max bandwidth}                     & \%           \\
                                  & \textit{L1 hit rate}                       & \%           \\
                                  & \textit{L2 hit rate}                       & \%           \\
                                  & \textit{Mem pipes busy}                    & \%           \\
    \bottomrule
  \end{tabular}
  \caption[Collected profiling metrics and their explanation]{Collected profiling metrics and their explanation. The metrics related to the compute cost are \textbf{bold}, those related to the memory cost are \textit{italicized}.}
  \label{tab:4-profiling-metrics}
\end{table}

\section{Proposed Cost Models}
\label{sec:4-cost-estimation}
This section provides an introduction to the concepts underlying our proposed cost models, which are elaborated in \autoref{chapter:cost-estimation}. We first give a short introduction to the models and the reasoning behind choosing these models after which we go slightly more in-depth into each separate cost model.

\subsection{Overview}
The first model, termed the analytical model, is a formula derived from the actual cost of the operations. By using this formula to deterministically calculate the cost of training a model, for both the factorized and materialized case, we infer which is optimal. Its simplicity lends itself to high explainability, but it may not perform as well as more complex methods due to the impracticality of incorporating the effects of all independent variables.

Including more independent variables as  features is straight-forward for ML-based cost models, which is why the next types are ML-based. With our second model, a linear regression model, we can incorporate more features, broadening the decision space. The third model, a tree-boosting model, uses a set of regression trees for its predictions. It is chosen as it is still explainable, but can capture the more intricate interactions between features than the linear models. It is interesting to compare these cost-based models to reason about the complexity of the problem and the generalizability of the models. If the tree-boosting model outperforms the other models, it suggests that the problem is more complex as there are non-linear feature interactions that cannot be captured in a linear model. However, if the linear models perform sufficiently well, they are likely a better choice as there is less risk of overfitting to training data. Finally, we propose a hybrid model, which integrates the insights derived from the preceding cost models and leverages the strengths of the most effective models to build a superior model.

Our cost modelling approach differs from previous research in two key areas. Firstly, we hypothesize that hardware choice affects the F/M trade-off, and thus we include hardware characteristics in our cost models. Secondly, we investigate the use of ML-based cost models as they allow for more complex feature interactions to be captured. This is in contrast to previous research, which has primarily used analytical decision rules decide whether to factorize or materialize.

\subsection{Analytical}
The analytical model is a deterministic model, constructed by examining the operations executed by the learning algorithm. This model is based on a formula derived from the actual cost of the operations. It encapsulates the critical factors of the algorithm, such as the number of matrix multiplications. For example, if an algorithm performs one addition and two multiplications, the formula for this would be $ADD + 2MULT$. The actual cost values for $ADD$ and $MULT$ are determined through micro benchmarks, and these values are then used to complete the formula and obtain the final model. In our context, these operations are the linear algebra operations performed as part of the machine learning model training. Therefore, by capturing the profiling metrics mentioned in \autoref{tab:4-profiling-metrics}, we can compute the cost of each operation in the training process. As will be demonstrated in \autoref{chapter:cost-estimation}, the memory cost of an operation is a reliable predictor for its total runtime. This is the reason we concentrate on the memory cost of an operation in the analytical model. When compared to related research, this cost model is the most like Orion's cost model~\cite{orion_learning_gen_lin_models}, which also explicitly incorporates multiple cost factors like I/O and Cache costs. However, Orion is highly specific to CPU training, whereas the analytical model in this study is GPU-focused.

By profiling across a broad spectrum of the selected independent variables, we can estimate their impact on memory cost. Integrating this information into the analytical model, allows us to construct a highly interpretable model that can be utilized to estimate the cost of various approaches.

\subsection{Linear Regression}
This model is grounded in empirical data and utilizes linear regression to predict runtime and make a decision between factorization and materialization. It takes into account various features known to influence performance, including the size of the input data, the complexity of the algorithm, and the hardware configuration. By examining the relationships between these features and the actual runtime of the algorithm, the linear regression model can make accurate and interpretable predictions about the cost of different approaches.

Previous research has shown that modelling linear relations between features is sufficient for accurate predictions, e.g., Morpheus' decision rule that only takes into account tuple- and feature-ratios ~\cite{morpheus}, and Amalur's complexity ratio~\cite{amalur}. If the cost characteristics of factorized training on GPU do not differ significantly from those of training on CPU, we expect this linear cost model to perform well. However, if the cost characteristics are more complex, a more intricate model will result in more accurate predictions.

\subsection{XGBoost}
To evaluate whether the preceding models, such as the analytical and linear regression models, are overly simplistic, we incorporate a more intricate XGBoost regressor model. If this model surpasses the performance of the other models, it suggests that more complex feature interactions occur that the other models have not been able to capture. This model is capable of modeling these complex interactions from the data and making more precise predictions about the cost of different approaches. The primary disadvantage is that this model is more expensive to train and is prone to overfitting on small training sets~\cite{xgboost}.

\subsection{Hybrid}
Finally, the insights obtained from the preceding cost models are combined and used in a hybrid model. By leveraging the strengths of multiple models, the hybrid model is able to make more accurate predictions about the cost of different approaches. Which models are chosen, and how they are combined is determined by the performance of the individual models. Through an analysis of the performance on specific scenarios, i.e., CPU and GPU scenarios, we pick the best models for the hybrid model. This model is expected to outperform the individual models, as it can leverage the strengths of each model to make more accurate predictions.